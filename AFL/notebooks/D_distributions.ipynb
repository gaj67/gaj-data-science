{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813c8914",
   "metadata": {},
   "source": [
    "# Appendix D: Modelling Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce153bf",
   "metadata": {},
   "source": [
    "The purpose of this appendix is to discuss some of the stochastic distributions that might be useful for modelling\n",
    "the outcomes of sporting matches, particularly Australian Rules football."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf2167",
   "metadata": {},
   "source": [
    "## Beta-Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf5932",
   "metadata": {},
   "source": [
    "Consider the situation where the outcome $X$ (i.e. a win or loss) of a match between team A and team B is stochastically decided,\n",
    "given some (unknown) probability $\\theta$ that team A wins. Essentially, the outcome may be considered as a biased coin toss.\n",
    "However, prior to the match we might consider the relative strengths of teams A and B, and this knowledge might inform us\n",
    "as to likely values of the probability $\\theta$. Consequently, we may model $X$ by a continuous mixture distribution over \n",
    "$\\theta$, and then use this to derive a posterior distribution for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7cab1",
   "metadata": {},
   "source": [
    "### Bernoulli data likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef826044",
   "metadata": {},
   "source": [
    "Let the variate $X\\in\\{0,1\\}$ represent the outcome of a single match between some arbitrary team A and team B, with $X=1$ indicating a win by team A, and $X=0$ indicating a win by team B.\n",
    "For convenience, we consider the Bernoulli distribution\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\theta & ~\\sim~ & \\texttt{Bern}(\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\theta$ is the assumed probability that team A wins, which is decided before the match commences.\n",
    "Thus, we see that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & ~=~ & \\theta^x\\,(1-\\theta)^{1-x}\\,.\n",
    "\\end{eqnarray}\n",
    "If we further wish to model the outcomes of an ordered sequence of $n$ *conditionally independent* matches between the same teams A and B, \n",
    "with the same parameter\n",
    "$\\theta$, then we will obtain the joint data likelihood\n",
    "\\begin{eqnarray}\n",
    "p(X=w\\mid n,\\theta) & ~=~ & \\theta^w\\,(1-\\theta)^{n-w}\\,,\n",
    "\\end{eqnarray}\n",
    "where $w\\in\\{0,1,\\ldots,n\\}$ represents the total number of wins for team A, and $\\ell=n-w$ represents the total number of\n",
    "losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1d12d",
   "metadata": {},
   "source": [
    "Note that the use of the Bernoulli distribution explicitly presumes that only wins and losses are possible.\n",
    "How then might we deal with games for which matches may end in a draw?\n",
    "It turns out that there are a number of good reasons to treat a draw as being half-a-win and half-a-loss for each team.\n",
    "Thus, we might define the weighted log-likelihood for a draw as being given by\n",
    "\\begin{eqnarray}\n",
    "L_\\texttt{draw}(\\theta) & ~\\doteq~ & \\frac{1}{2}\\ln p(X=1\\mid\\theta)+\\frac{1}{2}\\ln p(X=0\\mid\\theta)\n",
    "\\\\& =  &\n",
    "\\frac{1}{2}\\ln\\theta+\\frac{1}{2}\\ln\\,(1-\\theta)\n",
    "~=~\n",
    "\\ln\\left[\\theta^{\\frac{1}{2}}\\,(1-\\theta)^{\\frac{1}{2}}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "We observe that functionally this takes the same form as\n",
    "\\begin{eqnarray}\n",
    "L_\\texttt{draw}(\\theta) & ~=~ & \\ln p(X=0.5\\mid\\theta)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we may numerically treat observed draws as having the value $X=\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37488b8d",
   "metadata": {},
   "source": [
    "### Beta prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53eec6",
   "metadata": {},
   "source": [
    "We seek a non-informative prior distribution for $\\theta$. \n",
    "Following Box and Tiao [[1]](#Citations \"Citation [1]: Bayesian Inference in Statistical Analysis\"), \n",
    "we desire a transformation $\\phi(\\theta)$ such that the likelihood $p(X=w\\mid n,\\theta)$, plotted as a function of $\\phi$, remains\n",
    "approximately invariant in shape and size for some fixed $n$ as $w$ varies (but varies in its mean location). \n",
    "A uniform prior for $\\phi$ then induces a\n",
    "non-informative prior for $\\theta$. In general, it turns out that the relevant prior is usually inversely\n",
    "proportional to the square-root of the variance. For the Bernoulli distribution, we have\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}[X\\mid\\theta] & ~=~ \\theta\\,(1-\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "whereupon the non-informative prior is therefore\n",
    "\\begin{eqnarray}\n",
    "p(\\theta) ~\\propto~ \\frac{1}{\\sqrt{\\theta\\,(1-\\theta)}}\n",
    "& ~~~\\Rightarrow~~~ & \\theta~\\sim~\\mathtt{Beta}\\left(\\frac{1}{2},\\frac{1}{2}\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that this is just \n",
    "[Jeffreys' prior](https://en.wikipedia.org/wiki/Jeffreys_prior \"Wikipeida: Jeffreys' prior\"), \n",
    "which notionally corresponds to a single prior pseudo-match\n",
    "with half-a-win and half-a-loss, i.e. a draw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910e26c",
   "metadata": {},
   "source": [
    "More generally, we might choose an arbitrary beta prior distribution\n",
    "\\begin{eqnarray}\n",
    "\\theta & ~\\sim & ~\\mathtt{Beta}(\\alpha,\\beta)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for some hyper-parameters $\\alpha$ and $\\beta$, where\n",
    "$\\alpha$ represents the prior number of pseudo-wins and $\\beta$ represents the prior number of pseudo-losses.\n",
    "Note that technically the parameter dependence is explicitly $\\theta\\mid\\alpha,\\beta$. However, by convention\n",
    "the hyper-parameters are often left implicit to indicate that they are held constant. Despite this useful convention,\n",
    "it is best to always explicitly condition on all relevant parameters when dealing with the actual probability functions,\n",
    "in order to avoid accidental ambiguity, e.g. by hiding important modelling assumptions.\n",
    "\n",
    "The special case of $\\alpha=\\beta$ corresponds to having no other prior information with which to\n",
    "preference a win/loss over a loss/win.\n",
    "We could, for example, choose the \n",
    "[Haldane prior](https://en.wikipedia.org/wiki/Beta_distribution \"Wikipedia: Beta distribution\") \n",
    "with $\\alpha=\\beta=0$ to indicate that we have no\n",
    "prior pseudo-matches.\n",
    "Similarly, the uniform prior with $\\alpha=\\beta=1$ corresponds to one previous pseudo-win and one pseudo-loss.\n",
    "Alternatively, \n",
    "Kerman [[2]](#Citations \"Citation [2]: Neutral noninformative and informative conjugate beta and gamma prior distributions\") \n",
    "derives $\\alpha=\\beta=\\frac{1}{3}$ as being a noninformative prior that is flatter than Jeffreys' prior, and has the property that a sampled prior value is equally likely to be greater than or less than the *ex ante* maximum likelihood estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee6ab9",
   "metadata": {},
   "source": [
    "Note that the beta distribution is also a good choice due to its complementary symmetry in $\\alpha$ and $\\beta$.\n",
    "Suppose that $\\alpha$ denotes our pre-match knowledge about the strength of team A, and similarly let\n",
    "$\\beta$ represent our prior knowlegde about team B. Now, $\\theta=\\theta_A$ is the prior probability of team A winning,\n",
    "such that $\\theta_A\\sim\\texttt{Beta}(\\alpha,\\beta)$.\n",
    "However, the ordering of teams A and B is arbitrary. If we swapped the order of the teams and their respective hyper-parameters, \n",
    "then we would instead have $\\theta_B\\sim\\texttt{Beta}(\\beta,\\alpha)$.\n",
    "The beta distribution correctly satisfies the constraint that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\theta\\mid\\alpha,\\beta] & ~=~ \\frac{\\alpha}{\\alpha+\\beta}~=~1-\\mathbb{E}[\\theta\\mid\\beta,\\alpha]\\,. \n",
    "\\end{eqnarray}\n",
    "This is the aforementioned complementary symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55306b5",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli mixture distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a0edb",
   "metadata": {},
   "source": [
    "The mixture distribution for a single match is now derived as\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\int_0^1 p(x\\mid\\theta)\\,p(\\theta\\mid\\alpha,\\beta)\\,d\\theta\n",
    "\\\\& = &\n",
    "\\int_0^1 \\theta^x(1-\\theta)^{1-x}\\,\n",
    "\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\,d\\theta\n",
    "\\\\& = &\n",
    "\\frac{1}{B(\\alpha,\\beta)}\\int_0^1 \\theta^{\\alpha+x-1}(1-\\theta)^{\\beta-x}\\,d\\theta\n",
    "\\\\& =  &\n",
    "\\frac{B(\\alpha+x,\\beta-x+1)}{B(\\alpha,\\beta)}\\int_0^1 \n",
    "\\frac{\\theta^{\\alpha+x-1}(1-\\theta)^{\\beta-x}}{B(\\alpha+x,\\beta-x+1)}\\,d\\theta\\,.\n",
    "\\end{eqnarray}\n",
    "Recognising the final integrand as the $\\texttt{Beta}(\\alpha+x,\\beta-x+1)$ distribution, we obtain\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ &\n",
    "\\frac{B(\\alpha+x,\\beta-x+1)}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we expand $B(\\cdot,\\cdot)$ in terms of the *gamma* function $\\Gamma(\\cdot)$ to obtain\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ &\n",
    "\\frac{\\Gamma(\\alpha+x)\\,\\Gamma(\\beta+1-x)}{\\Gamma(\\alpha+\\beta+1)}\\,\n",
    "\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, using the recurrence relation that $\\Gamma(z+1)=z\\,\\Gamma(z)$, we deduce that\n",
    "\\begin{eqnarray}\n",
    "p(X=1\\mid\\alpha,\\beta)~=~\\frac{\\alpha}{\\alpha+\\beta}\\,, & \\;\\;\\;\\; &\n",
    "p(X=0\\mid\\alpha,\\beta)~=~\\frac{\\beta}{\\alpha+\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{\\alpha^x\\,\\beta^{1-x}}{\\alpha+\\beta}\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that this is just\n",
    "\\begin{eqnarray}\n",
    "X & ~\\sim~ & \\texttt{Bern}(\\bar{\\theta})\\,,\n",
    "\\end{eqnarray}\n",
    "with $\\bar{\\theta}=\\frac{\\alpha}{\\alpha+\\beta}$ being the mean of the beta distribution.\n",
    "\n",
    "Subsequently, for the case of $n$ *unconditionally independent* matches between teams A and B, we would obtain the joint distribution\n",
    "\\begin{eqnarray}\n",
    "p_u(X=w\\mid n,\\alpha,\\beta) & ~=~ & \\frac{\\alpha^w\\,\\beta^{n-w}}{(\\alpha+\\beta)^n}~=~\\bar{\\theta}^w\\,(1-\\bar{\\theta})^{n-w}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this is notionally different from the *conditionally independent* case assumed\n",
    "[previously](#Bernoulli-data-likelihood \"Section: Bernoulli data likelihood\"),\n",
    "where the $n$ matches all shared the same value of the parameter $\\theta$. In the unconditional form, the parameter\n",
    "$\\theta$ is assumed to be resampled before every match.\n",
    "\n",
    "Consequently, for the conditionally independent case, we would instead derive that\n",
    "\\begin{eqnarray}\n",
    "p_c(X=w\\mid n,\\alpha,\\beta) & ~=~ & \\int_0^1 p(X=w\\mid n,\\theta)\\,p(\\theta\\mid\\alpha,\\beta)\\,d\\theta\n",
    "\\\\& = &\n",
    "\\int_0^1 \\theta^w\\,(1-\\theta)^{n-w}\\,\n",
    "\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\,d\\theta\n",
    "\\\\& = &\n",
    "\\frac{1}{B(\\alpha,\\beta)}\\int_0^1 \\theta^{\\alpha+w-1}(1-\\theta)^{\\beta+n-w-1}\\,d\\theta\n",
    "\\\\& =  &\n",
    "\\frac{B(\\alpha+w,\\beta+n-w)}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529307b",
   "metadata": {},
   "source": [
    "### Beta posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e948f73",
   "metadata": {},
   "source": [
    "For the case of observing a sequence of $n$ conditionally independent matches, the posterior distribution is given by\n",
    "\\begin{eqnarray}\n",
    "p(\\theta\\mid n,w,\\alpha,\\beta) & ~=~ &\n",
    "\\frac{p(X=w\\mid n,\\theta)\\,p(\\theta\\mid\\alpha,\\beta)}{p_c(X=w\\mid n,\\alpha,\\beta)}\n",
    "~=~\n",
    "\\frac{\\theta^{\\alpha+w-1}(1-\\theta)^{\\beta+n-w-1}}{B(\\alpha+w,\\beta+n-w)}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\theta\\mid n,w & ~\\sim~ & \\texttt{Beta}(\\alpha+w,\\beta+n-w)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691c541",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66d9b1",
   "metadata": {},
   "source": [
    "Suppose the result of the $(n+1)$-th match is now $X$, conditionally independent of previous matches for the same\n",
    "parameter $\\theta$. Then the likelihood of this result is\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & ~=~ & \\theta^{x}\\,(1-\\theta)^{1-x}\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the predictive probability of this result is given by\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \\int_0^1 p(x\\mid\\theta)\\,p(\\theta\\mid n,w,\\alpha,\\beta)\\,d\\theta\n",
    "\\\\&~=~&\n",
    "\\int_0^1\\frac{\\theta^{\\alpha+w+x-1}\\,(1-\\theta)^{\\beta+n-w-x}}\n",
    "{B(\\alpha+w,\\beta+n-w)}\\,d\\theta\n",
    "\\\\ & = &\n",
    "\\frac{B(\\alpha+w+x,\\beta+n-w+1-x)}\n",
    "{B(\\alpha+w,\\beta+n-w)}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9d1c4",
   "metadata": {},
   "source": [
    "In terms of the gamma function, $\\Gamma(\\cdot)$, this becomes\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \n",
    "\\frac{\\Gamma(\\alpha+w+x)\\,\\Gamma(\\beta+n-w+1-x)}\n",
    "     {\\Gamma(\\alpha+\\beta+n+1)}\\,\n",
    "\\frac{\\Gamma(\\alpha+\\beta+n)}\n",
    "     {\\Gamma(\\alpha+w)\\,\\Gamma(\\beta+n-w)}\\,.\n",
    "\\end{eqnarray}\n",
    "For a loss, i.e. $X=0$, the respective probability reduces to\n",
    "\\begin{eqnarray}\n",
    "p(X=0\\mid n,w,\\alpha,\\beta) & ~=~ & \\frac{\\beta+n-w}{\\alpha+\\beta+n}\\,,\n",
    "\\end{eqnarray}\n",
    "again using the recurrence relation that $\\Gamma(z+1)=z\\,\\Gamma(z)$.\n",
    "The corresponding probability of a win is therefore\n",
    "\\begin{eqnarray}\n",
    "p(X=1\\mid n,w,\\alpha,\\beta) & ~=~ & \\frac{\\alpha+w}{\\alpha+\\beta+n}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \\frac{(\\alpha+w)^x\\,(\\beta+n-w)^{1-x}}{\\alpha+\\beta+n}\\,.\n",
    "\\end{eqnarray}\n",
    "Defining $\\hat{\\theta}\\doteq\\frac{\\alpha+w}{\\alpha+\\beta+n}$, we observe that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \\left(\\hat{\\theta}\\right)^x\\,\\left(1-\\hat{\\theta}\\right)^{1-x}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\hat{\\theta} & ~\\sim~ & \\texttt{Bern}(\\hat{\\theta})\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb62c5",
   "metadata": {},
   "source": [
    "Note that the denominator of $\\hat{\\theta}$ corresponds to assuming $n$ observed matches plus $\\alpha+\\beta$ prior pseudo-matches, and the numerator corresponds to $w$ observed wins plus $\\alpha$ prior pseudo-wins.\n",
    "Thus, $\\hat{\\theta}$ is just a *smoothed* estimate of the empirical probability of a win."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5648033",
   "metadata": {},
   "source": [
    "## Gamma-Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d56fc1",
   "metadata": {},
   "source": [
    "Consider a Poisson-like count $X\\in\\mathbb{Z}^{\\ge 0}$ with over-dispersion, namely \n",
    "$\\mathbb{V}[X]>\\mathbb{E}[X]$. This suggests the need for an additonal parameter, beyond the usual Poisson rate $\\lambda$, \n",
    "to control the extra variance. \n",
    "One way in which over-dispersion can arise is when $\\lambda$ is only held constant for a single trial \n",
    "or a single sequence of trials, but exhibits inter-trial or inter-sequence variation.\n",
    "This variation may be captured by a prior distribution, giving rise to a continuous mixture distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7598d",
   "metadata": {},
   "source": [
    "### Poisson data likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6875901",
   "metadata": {},
   "source": [
    "For some fixed rate $\\lambda$, let the count $X$ be distributed as\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\lambda & ~\\sim~ & \\mathtt{Poisson}(\\lambda)\\,,\n",
    "\\end{eqnarray}\n",
    "with mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X\\mid\\lambda]~=~\\lambda\\,, & \\;\\;\\mbox{and}\\;\\; &\n",
    "\\mathbb{V}[X\\mid\\lambda]~=~\\lambda\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Then we may consider a \n",
    "single sequence of $n$ conditionally independent counts with $\\lambda$ held constant just for that sequence.\n",
    "If the average count of the sequence is $\\langle X\\rangle=\\bar{X}$, then the joint likelihood is given by\n",
    "\\begin{eqnarray}\n",
    "p(X_1,\\ldots,X_n\\mid\\lambda) & ~=~ & \\frac{e^{-n\\lambda}\\,\\lambda^{n\\bar{X}}}{\\prod_{i=1}^n X_i!}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf9c24",
   "metadata": {},
   "source": [
    "### Gamma prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2482218",
   "metadata": {},
   "source": [
    "Consider, in general, some arbitrary prior distribution, say $\\lambda\\sim D(\\boldsymbol{\\theta})$, governed by one or more \n",
    "(constant) hyper-parameters denoted by $\\boldsymbol{\\theta}$.\n",
    "Then it follows that  \n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X] & ~\\doteq~ & \\mathbb{E}\\left[\\mathbb{E}[X\\mid\\lambda]\\mid\\boldsymbol{\\theta}\\right]\n",
    " ~=~ \\mathbb{E}[\\lambda\\mid\\boldsymbol{\\theta}]\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}[X] & ~\\doteq~ & \\mathbb{E}\\left[\\mathbb{V}[X\\mid\\lambda]\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\\mathbb{V}\\left[\\mathbb{E}[X\\mid\\lambda]\\mid\\boldsymbol{\\theta}\\right]\n",
    " ~=~ \\mathbb{E}[\\lambda\\mid\\boldsymbol{\\theta}]+\\mathbb{V}[\\lambda\\mid\\boldsymbol{\\theta}]\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we observe that $\\mathbb{V}[X]>\\mathbb{E}[X]$, such that the overall process is over-dispersed compared\n",
    "to a simple Poisson process.\n",
    "Clearly we require a *proper* prior distribution with finite mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf44a07",
   "metadata": {},
   "source": [
    "Following the rationale discussed [previously](#Beta-prior-distribution \"Section: Beta prior distribution\"),\n",
    "we first seek a non-informative prior for the Possion rate $\\lambda\\in (0,\\infty)$ via a variance-stabilising transformation. However, this results in the improper prior\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda) & ~\\propto~ & \\frac{1}{\\sqrt{\\mathbb{V}[X\\mid\\lambda]}}~=~\\frac{1}{\\sqrt{\\lambda}}\\,.\n",
    "\\end{eqnarray}\n",
    "If we truncate the domain to $\\lambda\\in(0,L]$, then the truncated prior becomes\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda\\mid L) & ~=~ & \\frac{1}{2\\sqrt{L\\lambda}}\\,,\n",
    "\\end{eqnarray}\n",
    "with mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\lambda\\mid L]~=~\\frac{1}{3}L\\,, & \\;\\;\\mbox{and}\\;\\; &\n",
    "\\mathbb{V}[\\lambda\\mid L]~=~\\frac{4}{45}L^2\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. The overall mean and variance are then given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X]~=~\\frac{1}{3}L\\,, & \\;\\;\\mbox{and}\\;\\; &\n",
    "\\mathbb{V}[X]~=~\\frac{1}{3}L+\\frac{4}{45}L^2\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Clearly, the hyper-parameter $L$ may be estimated (using the method of moments) from the sample mean as $\\hat{L}=3\\bar{X}$.\n",
    "However, if the sample variance does not match $\\frac{1}{3}\\hat{L}+\\frac{4}{45}\\hat{L}^2$, then \n",
    "having only a single prior hyper-parameter does not provide enough modelling flexibility.\n",
    "Consequently, we require a prior distribution having at least two hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c7178",
   "metadata": {},
   "source": [
    "To deduce a suitable form for such a prior, [recall](#Poisson-data-likelihood \"Sectrion: Poisson data likelihood\")\n",
    "that the joint likelihood of a sequence of $n$ trials, considered as a function of $\\lambda$ with fixed\n",
    "observed counts, is proportional to $\\lambda^{n\\bar{X}}\\,e^{-n\\lambda}$. This has the same proportional form as\n",
    "the $\\texttt{Gamma}(n\\bar{X}+1,n)$ distribution.\n",
    "Consequently, we consider a prior distribution in the general form\n",
    "\\begin{eqnarray}\n",
    "\\lambda & ~\\sim~ & \\mathtt{Gamma}(\\alpha,\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\alpha$ is the shape parameter and $\\beta$ is the rate parameter.\n",
    "It now follows that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X] & ~=~ & \\mathbb{E}[\\lambda\\mid\\alpha,\\beta]~=~\\frac{\\alpha}{\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}[X] & ~=~ & \\mathbb{E}[\\lambda\\mid\\alpha,\\beta]+\\mathbb{V}[\\lambda\\mid\\alpha,\\beta]\n",
    "\\\\ & = & \\frac{\\alpha}{\\beta}+\\frac{\\alpha}{\\beta^2}~=~\\frac{\\alpha\\,(\\beta+1)}{\\beta^2}\\,.\n",
    "\\end{eqnarray}\n",
    "Not only is this over-dispersed for $\\alpha>0$, but the two hyper-parameters $\\alpha$ and $\\beta$\n",
    "provide sufficient freedom to flexibly model both the sample mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f69cf",
   "metadata": {},
   "source": [
    "### Negative binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43524b7",
   "metadata": {},
   "source": [
    "For a single trial, the explicit form of the mixture distribution is derived as\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid\\alpha,\\beta) & ~=~ & \\int_0^\\infty p(X=k\\mid\\lambda)\\,p(\\lambda\\mid\\alpha,\\beta)\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\int_0^\\infty e^{-\\lambda}\\frac{\\lambda^k}{k!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\lambda^{\\alpha-1}\\,e^{-\\beta\\lambda}\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\frac{\\beta^\\alpha}{k!\\,\\Gamma(\\alpha)}\n",
    "\\int_0^\\infty \\lambda^{\\alpha+k-1}\\,e^{-(\\beta+1)\\lambda}\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,\\frac{\\beta^\\alpha}{(\\beta+1)^{\\alpha+k}}\n",
    "\\int_0^\\infty \\frac{(\\beta+1)^{\\alpha+k}}{\\Gamma(\\alpha+k)}\\,\\lambda^{\\alpha+k-1}\\,e^{-(\\beta+1)\\lambda}\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,\\frac{\\beta^\\alpha}{(\\beta+1)^{\\alpha+k}}\\,,\n",
    "\\end{eqnarray}\n",
    "since the final integrand is just the distribution $\\mathtt{Gamma}(\\alpha+k,\\beta+1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95d767",
   "metadata": {},
   "source": [
    "In order to show that this is just the real-valued (*Polya* distribution) version of the \n",
    "[*negative binomial*](https://en.wikipedia.org/wiki/Negative_binomial_distribution \"Wikipedia: Negative binomial distribution\") \n",
    "distribution,\n",
    "we define $p$ to be the probability of a *stopping* event (i.e. one out of the required number $\\alpha$ of such events) and $q$ to be the probability of a *non-stopping* event,\n",
    "namely\n",
    "\\begin{eqnarray}\n",
    "p ~\\doteq~ \\frac{\\beta}{\\beta+1}\\,, & ~~~\\mbox{and}~~~ & q ~\\doteq~ 1-p~=~\\frac{1}{\\beta+1}\\,,\n",
    "\\end{eqnarray}\n",
    "whereupon\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid\\alpha,p) & ~=~ & \n",
    "\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,q^k\\,p^\\alpha\\,.\n",
    "\\end{eqnarray}\n",
    "This represents the overall probability of observing a sequence of $k+\\alpha$ Bernoulli trials,\n",
    "where the sequence was terminated as soon as $\\alpha$ stopping events occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57bec1",
   "metadata": {},
   "source": [
    "In the traditional integer-valued (*Pascal* distribution) form, $\\alpha>0$ is replaced by $r\\in\\mathbb{Z}^{>0}$, giving\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid r,p) & ~=~ & \n",
    "\\frac{(r+k-1)!}{k!\\,(r-1)!}\\,q^k\\,p^r~=~\\binom{k+r-1}{k}\\,q^k\\,p^r\\,.\n",
    "\\end{eqnarray}\n",
    "If we terminate the sequence of trials after just $r=1$ stopping events, then this reduces to the\n",
    "*geometric* distribution, i.e. $\\mathtt{Geom}(p)\\equiv \\mathtt{NegBinom}(1,p)$.\n",
    "Furthermore, $X\\sim\\mathtt{NegBinom}(r,p)$ corresponds to the sum of $r$ independent geometric counts, i.e.\n",
    "$X=X_1+X_2+\\ldots+X_r$ with $X_i\\sim\\mathtt{Geom}(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5dfb6",
   "metadata": {},
   "source": [
    "Finally, for a sequence of $n$ conditionally independent counts, the joint mixture distribution is\n",
    "\\begin{eqnarray}\n",
    "p_c(X_1,\\ldots,X_n\\mid\\alpha,\\beta) & ~=~ & \\int_0^\\infty p(X_1,\\ldots,X_n\\mid\\lambda)\\,p(\\lambda\\mid\\alpha,\\beta)\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\int_0^\\infty \\frac{e^{-n\\lambda}\\,\\lambda^{n\\bar{X}}}{\\prod_{i=1}^n X_i!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\lambda^{\\alpha-1}\\,e^{-\\beta\\lambda}\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\frac{\\Gamma(\\alpha+n\\bar{X})}{\\Gamma(\\alpha)\\,\\prod_{i=1}^n X_i!}\\,\\frac{\\beta^\\alpha}{(\\beta+n)^{\\alpha+n\\bar{X}}}\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, for *unconditionally* independent counts sampled with *different* values of $\\lambda$, the joint mixture distribution is just\n",
    "\\begin{eqnarray}\n",
    "p_u(X_1,\\ldots,X_n\\mid\\alpha,\\beta) & ~=~ &\n",
    "\\prod_{i=1}^n\\frac{\\Gamma(\\alpha+X_i)}{X_i!}\\cdot\\frac{q^{n\\bar{X}}\\,p^{n\\alpha}}{\\Gamma(\\alpha)^n}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18518500",
   "metadata": {},
   "source": [
    "### Gamma posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f140a",
   "metadata": {},
   "source": [
    "For a sequence of $n$ conditionally independent counts, the posterior distribution for the Poisson rate $\\lambda$ is\n",
    "given by\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda\\mid X_1,\\ldots,X_n,\\alpha,\\beta) & ~=~ &\n",
    "\\frac{p(X_1,\\ldots,X_n\\mid\\lambda)\\,p(\\lambda\\mid\\alpha,\\beta)}{p_c(X_1,\\ldots,X_n\\mid\\alpha,\\beta)}\n",
    "\\\\& = &\n",
    "\\left.\n",
    "\\frac{e^{-n\\lambda}\\,\\lambda^{n\\bar{X}}}{\\prod_{i=1}^n X_i!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\lambda^{\\alpha-1}\\,e^{-\\beta\\lambda}\n",
    "\\right/\n",
    "\\frac{\\Gamma(\\alpha+n\\bar{X})}{\\Gamma(\\alpha)\\,\\prod_{i=1}^n X_i!}\\,\\frac{\\beta^\\alpha}{(\\beta+n)^{\\alpha+n\\bar{X}}}\n",
    "\\\\& = &\n",
    "\\frac{(\\beta+n)^{\\alpha+n\\bar{X}}}{\\Gamma(\\alpha+n\\bar{X})}\\,\\lambda^{\\alpha+n\\bar{X}-1}\\,e^{-(\\beta+n)\\lambda}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\lambda\\mid X_1,\\ldots,X_n & ~\\sim~ & \\texttt{Gamma}(\\alpha+n\\bar{X},\\beta+n)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f3cd8",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24764d9b",
   "metadata": {},
   "source": [
    "[1] George E. P. Box and George C. Tiao (1973), \"*Bayesian Inference in Statistical Analysis*\", John Wiley & Sons.\n",
    "\n",
    "[2] Jouni Kerman (2011), \"*Neutral noninformative and informative conjugate beta and gamma prior distributions*\", Electron. J. Statist. 5: 1450-1470."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
