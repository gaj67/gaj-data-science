{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e68c05",
   "metadata": {},
   "source": [
    "# Appendix A: Additively Weighted Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969fe18",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69aef6",
   "metadata": {},
   "source": [
    "The task is to predict the class $c\\in\\mathcal{C}$ that best matches a sequence of observations $\\vec{\\mathbf{x}}\\doteq (\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_K)$.\n",
    "We assume some probabilistic model $P(c\\mid\\vec{\\mathbf{x}},\\Theta)$ with unknown parameters $\\Theta$.\n",
    "The parameters are to be estimated from the training data $\\mathbf{C}\\doteq[c^{(d)}]_{d=1}^N$\n",
    "and $\\mathbf{X}\\doteq[\\vec{\\mathbf{x}}^{(d)}]_{d=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318025b",
   "metadata": {},
   "source": [
    "## Expectation-Maxmisation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5e2a7",
   "metadata": {},
   "source": [
    "### Additive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad0272",
   "metadata": {},
   "source": [
    "For a single sequence $\\vec{\\mathbf{x}}=(\\mathbf{x}_k)_{k=1}^K$ of observations, we assume *a priori* that some particular observation, say $\\mathbf{x}_{k^*}$, is the best predictor of the class, such that\n",
    "\\begin{eqnarray}\n",
    "P(k^*,c\\mid\\vec{\\mathbf{x}},\\Theta) & \\doteq & P(k^*\\mid\\Theta)\\,P(c\\mid\\mathbf{x}_{k^*},\\Theta)\\,.\n",
    "\\end{eqnarray}\n",
    "It then follows that the desired predictive model is given by\n",
    "\\begin{eqnarray}\n",
    "P(c\\mid\\vec{\\mathbf{x}},\\Theta) & = & \n",
    "\\sum_{k^*=1}^K P(k^*,c\\mid\\vec{\\mathbf{x}},\\Theta)\n",
    "~=~\\sum_{k=1}^K w_k\\,P(c\\mid\\mathbf{x}_k,\\Theta)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for prior observation weights $w_k\\doteq P(k\\mid\\Theta)$. This model therefore takes the form of an additively weighted *mixture of experts*.\n",
    "We leave the sub-model (or expert) $P(c\\mid\\mathbf{x}_k,\\Theta)$ undefined, except to stipulate that its\n",
    "parameters (nominally $\\Theta$) do not depend on the prior weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5051a808",
   "metadata": {},
   "source": [
    "Finally, we may invert the model to obtain the posterior observation weights, given by\n",
    "\\begin{eqnarray}\n",
    "\\bar{w}_k & \\doteq & P(k\\mid c,\\vec{\\mathbf{x}},\\Theta)\n",
    "~=~\\frac{P(k, c\\mid\\vec{\\mathbf{x}},\\Theta)}{P(c\\mid\\vec{\\mathbf{x}},\\Theta)}\n",
    "\\nonumber\\\\& = &\n",
    "\\frac{w_k\\,P(c\\mid\\mathbf{x}_k,\\Theta)}\n",
    "{\\sum_{\\tilde{k}=1}^K w_\\tilde{k}\\,P(c\\mid\\mathbf{x}_\\tilde{k},\\Theta)}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590a3ed",
   "metadata": {},
   "source": [
    "### Expected log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1991f",
   "metadata": {},
   "source": [
    "The index $k^*$ of the optimal observation $\\mathbf{x}_{k^*}$ is, in practice, unknown. \n",
    "Hence, we introduce the binary indicators $z_k=\\delta(k=k^*)$, such that the predictive model now takes the\n",
    "form\n",
    "\\begin{eqnarray}\n",
    "P(k^*,c\\mid\\vec{\\mathbf{x}},\\Theta) & = & \n",
    "\\prod_{k=1}^K\\left[w_k\\,P(c\\mid\\mathbf{x}_{k},\\Theta)\\right]^{z_k}\\,,\n",
    "\\end{eqnarray}\n",
    "with log-likelihood\n",
    "\\begin{eqnarray}\n",
    "\\ln P(k^*,c\\mid\\vec{\\mathbf{x}},\\Theta) & = & \\sum_{k=1}^K z_k\\ln\\left[w_k\\,P(c\\mid\\mathbf{x}_{k},\\Theta)\n",
    "\\right]\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ae453",
   "metadata": {},
   "source": [
    "Given the training data $\\mathbf{C}$ and $\\mathbf{X}$, the predictive log-likelihood is now given by\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta,\\mathbf{Z}) & \\doteq &\n",
    "\\sum_{d=1}^N\\ln P(k^{*(d)},c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta)\n",
    "~=~\\sum_{d=1}^N\\sum_{k=1}^K z_k^{(d)}\\ln\\left[w_k\\,P(c^{(d)}\\mid\\mathbf{x}^{(d)}_k,\\Theta)\n",
    "\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{Z}\\doteq[\\mathbf{z}^{(d)}]_{d=1}^N$ and $\\mathbf{z}^{(d)}\\doteq[z^{(d)}_k]_{k=1}^K$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5478e4",
   "metadata": {},
   "source": [
    "Now, the indicators $\\mathbf{Z}$ are actually hidden variables, since they are defined in terms of the\n",
    "unknown indices $k^{*(d)}$.\n",
    "We therefore take expectations over $\\mathbf{Z}$, resulting in the expected log-likelihood\n",
    "\\begin{eqnarray}\n",
    "Q(\\Theta,\\Theta') & \\doteq &\n",
    "\\mathbb{E}[L(\\Theta,\\mathbf{Z})\\mid\\Theta']\n",
    "\\nonumber\\\\\n",
    "& = & \n",
    "\\sum_{d=1}^N\\sum_{k=1}^K \\mathbb{E}[z_k^{(d)}\\mid\\Theta']\\,\n",
    "\\ln\\left[w_k\\,P(c^{(d)}\\mid\\mathbf{x}^{(d)}_k,\\Theta)\\right]\n",
    "\\nonumber\\\\\n",
    "& = & \n",
    "\\sum_{d=1}^N\\sum_{k=1}^K \\bar{w}_k^{(d)}\\,\n",
    "\\ln\\left[w_k\\,P(c^{(d)}\\mid\\mathbf{x}^{(d)}_k,\\Theta)\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\bar{w}_k^{(d)}=P(k^{*(d)}=k\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta')$. These are just\n",
    "the posterior weights from the\n",
    "[previous](#Additive-model \"Section: Additive model\") section, evaluated using the parameter \n",
    "estimate $\\Theta'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4c430",
   "metadata": {},
   "source": [
    "### Maximising the log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47464d8d",
   "metadata": {},
   "source": [
    "We now assume that the model parameters $\\Theta$ include the vector\n",
    "$\\mathbf{w}\\doteq(w_1,\\ldots,w_K)$ of prior observation weights.\n",
    "We further assume that the sub-model $P(c\\mid\\mathbf{x}_k,\\Theta)$ does not depend on $\\mathbf{w}$.\n",
    "Given that the prior weights sum to unity, we add this constraint to the expected log-likelihood with a Lagrange multiplier, to form the objective function\n",
    "\\begin{eqnarray}\n",
    "F(\\mathbf{w}) & \\doteq & Q(\\Theta,\\Theta')-\\lambda (\\mathbf{1}^T\\mathbf{w}-1)\\,.\n",
    "\\end{eqnarray}\n",
    "We now choose the weights to maximise this objective function.\n",
    "\n",
    "The required gradient with respect to the $k$-th component is given by\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w_k} & = &\n",
    "\\sum_{d=1}^{N}\\frac{\\bar{w}_k^{(d)}}{w_k}-\\lambda\\,,\n",
    "\\end{eqnarray}\n",
    "which vanishes (i.e. becomes zero) exactly when\n",
    "\\begin{eqnarray}\n",
    "w_k & = & \\frac{1}{N}\\sum_{d=1}^{N}\\bar{w}_k^{(d)}\\,,\n",
    "\\end{eqnarray}\n",
    "with $\\lambda=N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989b87c",
   "metadata": {},
   "source": [
    "The expectation-maximisation (EM) algorithm now proceeds by iteratively updating the estimates of the 'prior' observation weights $w_k$ (as part of $\\Theta$) using the posterior weights computed from the previous estimate\n",
    "$\\Theta'$.\n",
    "From the [additive model](#Additive-model \"Section: Additive model\"), we obtain the update\n",
    "\\begin{eqnarray}\n",
    "w_k & = & \\frac{1}{N}\\sum_{d=1}^{N}\n",
    "\\frac{w_k'\\,P(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Theta')}\n",
    "{\\sum_{\\tilde{k}=1}^K w_\\tilde{k}'\\,P(c^{(d)}\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Theta')}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c1e95",
   "metadata": {},
   "source": [
    "## Direct Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51330ca0",
   "metadata": {},
   "source": [
    "### Additive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfb847",
   "metadata": {},
   "source": [
    "In the direct approach, we simply assume a mixture model of the form\n",
    "\\begin{eqnarray}\n",
    "P(c\\mid\\vec{\\mathbf{x}},\\Theta) & \\doteq & \\sum_{k=1}^K w_k\\,P(c\\mid\\mathbf{x}_k,\\Theta)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with mixture weights $w_k\\ge 0$ that satisfy $\\sum_{k=1}^{K}w_k=1$.\n",
    "There is no need to interpret these weights as component probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6acf8b",
   "metadata": {},
   "source": [
    "### Expected log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de095c24",
   "metadata": {},
   "source": [
    "The discriminative log-likelihood of the training data $\\mathbf{C}$ and $\\mathbf{X}$ is now given by\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta) & \\doteq & \\ln P(\\mathbf{C}\\mid\\mathbf{X},\\Theta)\n",
    "\\nonumber\\\\& = &\n",
    "\\sum_{d=1}^{N}\\ln P(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta)\n",
    "\\nonumber\\\\& = &\n",
    "\\sum_{d=1}^{N}\\ln \\sum_{k=1}^{K}w_k\\,P(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Theta)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that other forms of likelihood could also be used, such as the joint likelihood\n",
    "$P(\\mathbf{C},\\mathbf{X}\\mid\\Theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cde4eb",
   "metadata": {},
   "source": [
    "### Maximising the log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c643e",
   "metadata": {},
   "source": [
    "Given the log-likelihood and the constraints on the mixture weights, we obtain the objective function\n",
    "\\begin{eqnarray}\n",
    "F(\\mathbf{w}) & \\doteq & L(\\Theta)-\\lambda (\\mathbf{1}^T\\mathbf{w}-1)\\,,\n",
    "\\end{eqnarray}\n",
    "which is to be maximised. The gradient with respect to the $k$-th mixture component is then\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w_k} & = & \n",
    "\\sum_{d=1}^{N}\\frac{P(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Theta)}\n",
    "{\\sum_{\\tilde{k}=1}^K w_\\tilde{k}\\,P(c^{(d)}\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Theta)}\n",
    "-\\lambda\\,.\n",
    "\\end{eqnarray}\n",
    "Now, since $w_k\\ge 0$, we take the modified gradient\n",
    "\\begin{eqnarray}\n",
    "\\delta w_k & \\doteq & w_k\\frac{\\partial F}{\\partial w_k} ~=~ \n",
    "\\sum_{d=1}^{N}\\frac{w_k\\,P(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Theta)}\n",
    "{\\sum_{\\tilde{k}=1}^K w_\\tilde{k}\\,P(c^{(d)}\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Theta)}\n",
    "-\\lambda w_k\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\sum_{k=1}^{K}\\delta w_k & = & \n",
    "\\sum_{d=1}^{N}\\frac{\\sum_{k=1}^{K}w_k\\,P(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Theta)}\n",
    "{\\sum_{\\tilde{k}=1}^K w_\\tilde{k}\\,P(c^{(d)}\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Theta)}\n",
    "-\\lambda\\sum_{k=1}^{K}w_k~=~N-\\lambda\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The modified gradient vanishes when the gradient vanishes, at which point\n",
    "$\\lambda=N$. Consequently, the optimal mixture weights $w_k^*$ satisfy the relation\n",
    "\\begin{eqnarray}\n",
    "w_k^* & = & \\frac{1}{N}\\sum_{d=1}^{N}\n",
    "\\frac{w_k^*\\,P(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Theta)}\n",
    "{\\sum_{\\tilde{k}=1}^K w_\\tilde{k}^*\\,P(c^{(d)}\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Theta)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, the mixture weights may be found by iteration (assuming the iterative scheme is stable\n",
    "and convergent). Alternatively, any other gradient ascent scheme may be used (which might be faster)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
