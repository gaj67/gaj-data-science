{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d711e65",
   "metadata": {},
   "source": [
    "# Modelling Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ef4b3",
   "metadata": {},
   "source": [
    "## Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c40a3f",
   "metadata": {},
   "source": [
    "A Boltzmann Machine has an input layer with vector ${\\bf x}=(x_1,x_2,\\ldots,x_N)$ and an output layer with vector ${\\bf y}=(y_1,y_2,\\ldots,y_M)$. The relationships between input elements $x_i$ and output elements $y_j$\n",
    "take the form of an undirected graph (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2539d",
   "metadata": {},
   "source": [
    "<img src=\"Boltzmann.png\" width=\"40%\" title=\"Boltzmann Machine graph\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60cf89",
   "metadata": {},
   "source": [
    "Taking inspiration from random Markov fields and the Boltzmann distribution, we specify an arbitrary energy function\n",
    "\\begin{eqnarray}\n",
    "E({\\bf x},{\\bf y}) & = & -\\left[\n",
    "  f({\\bf x})+g({\\bf y})+h({\\bf x},{\\bf y})\n",
    "\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "such that the joint probability (or density) of ${\\bf x}$ and ${\\bf y}$ is defined as\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \\frac{e^{-E({\\bf x},{\\bf y})}}{Z_{X,Y}}\n",
    "= \\frac{e^{f({\\bf x})+g({\\bf y})+h({\\bf x},{\\bf y})}}{Z_{X,Y}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $Z_{X,Y}$ is the appropriate partition function obtained by summing (or integrating) the numerator over all\n",
    "possible values of ${\\bf x}$ and ${\\bf y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52749695",
   "metadata": {},
   "source": [
    "It follows that the conditional distributions are given by\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}\\mid{\\bf y}) & = & \\frac{e^{f({\\bf x})+h({\\bf x},{\\bf y})}}{Z_{X}({\\bf y})}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}) & = & \\frac{e^{g({\\bf y})+h({\\bf x},{\\bf y})}}{Z_{Y}({\\bf x})}\\,,\n",
    "\\end{eqnarray}\n",
    "with respective partition functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a611a40",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b551ccf",
   "metadata": {},
   "source": [
    "A *Restricted* Boltzmann Machine (RBM) further restricts the relationships between ${\\bf x}$ and ${\\bf y}$ to take the form of an undirected *bipartite* graph, such that the elements of ${\\bf x}=(x_1,x_2,\\ldots,x_N)$ form disconnected nodes in one partition, the elements of ${\\bf y}=(y_1,y_2,\\ldots,y_M)$ form disconnected nodes in the other partition, and edges exist only between nodes in different partitions, i.e. $x_i -\\!\\!\\!- y_j$ (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf819e",
   "metadata": {},
   "source": [
    "![Restricted Boltzmann Machine graph](RBM.png \"Restricted Boltzmann Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4203d97",
   "metadata": {},
   "source": [
    "Consequently, by design, the elements $x_i$ of ${\\bf x}$ are conditionally independent given ${\\bf y}$, viz.\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}\\mid{\\bf y}) & = & \\prod_{i=1}^{N}p(x_i\\mid{\\bf y})\\,,\n",
    "\\end{eqnarray}\n",
    "and the elements $y_j$ of ${\\bf y}$ are likewise conditionally independent given ${\\bf x}$, viz.\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}) & = & \\prod_{j=1}^{M}p(y_i\\mid{\\bf x})\\,.\n",
    "\\end{eqnarray}\n",
    "Therefore, the functions $f(\\cdot)$, $g(\\cdot)$ and $h(\\cdot,\\cdot)$ must be linearly separable in their\n",
    "arguments, namely\n",
    "\\begin{eqnarray}\n",
    "f({\\bf x})=\\sum_{i=1}^{N}f_{i}(x_i)\\,,\\; &\n",
    "g({\\bf y})=\\sum_{j=1}^{M}g_{j}(y_j)\\,, &\n",
    "h({\\bf x},{\\bf y})=\\sum_{i=1}^{N}\\sum_{j=1}^{M}h_{ij}(x_i,y_j)\\,,\n",
    "\\end{eqnarray}\n",
    "leading to\n",
    "\\begin{eqnarray}\n",
    "p(x_i\\mid{\\bf y}) = \\frac{e^{f_i(x_i)+\\sum_{j=1}^{M}h_{ij}(x_i,y_j)}}{Z_{X_i}({\\bf y})}\\,,\n",
    "&\\;&\n",
    "p(y_j\\mid{\\bf x}) = \\frac{e^{g_j(y_j)+\\sum_{i=1}^{N}h_{ij}(x_i,y_j)}}{Z_{Y_j}({\\bf x})}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and thus\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) &=& \n",
    "\\frac{e^{\\sum_{i=1}^{N}f_i(x_i)+\\sum_{j=1}^{M}g_{j}(y_j)+\\sum_{i=1}^{N}\\sum_{j=1}^{M}h_{ij}(x_i,y_j)}}{Z_{X,Y}}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f22f52",
   "metadata": {},
   "source": [
    "## Hidden outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589e64d",
   "metadata": {},
   "source": [
    "Traditionally, the RBM output is considered to be a hidden or latent layer, for which the values of ${\\bf y}$ are never observed in practice, and hence must be summed (or integrated) out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a68e973",
   "metadata": {},
   "source": [
    "### Bernoulli outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc10b3",
   "metadata": {},
   "source": [
    "For tractability, ${\\bf y}$ is usually taken to be a vector of binary values, i.e. ${\\bf y}\\in\\{0,1\\}^M$.\n",
    "Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "p(y_j=1\\mid{\\bf x}) & = & \\frac{e^{g_j(1)+\\sum_{i=1}^{N}h_{ij}(x_i,1)}}\n",
    "{e^{g_j(0)+\\sum_{i=1}^{N}h_{ij}(x_i,0)}+e^{g_j(1)+\\sum_{i=1}^{N}h_{ij}(x_i,1)}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{e^{g_j(1)-g_j(0)+\\sum_{i=1}^{N}[h_{ij}(x_i,1)-h_{ij}(x_i,0)]}}\n",
    "{1+e^{g_j(1)-g_j(0)+\\sum_{i=1}^{N}[h_{ij}(x_i,1)-h_{ij}(x_i,0)]}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{1}\n",
    "{1+e^{-\\left[g_j(1)-g_j(0)+\\sum_{i=1}^{N}[h_{ij}(x_i,1)-h_{ij}(x_i,0)]\\right]}}\n",
    "\\\\\n",
    "& = & \\sigma\\left(b_j+\\sum_{i=1}^{N}w_{ij}(x_i)\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "where $b_j\\doteq g_j(1)-g_j(0)$, $w_{ij}(x_i)\\doteq h_{ij}(x_i,1)-h_{ij}(x_i,0)$ and $\\sigma(\\cdot)$ is the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c09dd",
   "metadata": {},
   "source": [
    "For convenience, note that we may invert these relations and define $g_j(y_j)\\doteq b_j y_j$ and\n",
    "$h_{ij}(x_i,y_j)\\doteq w_{ij}(x_i)\\,y_j$, without loss of generality.\n",
    "The converse conditional distribution then becomes\n",
    "\\begin{eqnarray}\n",
    "p(x_i\\mid{\\bf y}) & = & \\frac{e^{f_i(x_i)+\\sum_{j=1}^{M}w_{ij}(x_i)\\,y_j}}{Z_{X_i}({\\bf y})}\\,,\n",
    "\\end{eqnarray}\n",
    "from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af735497",
   "metadata": {},
   "source": [
    "The marginal distribution of ${\\bf x}$ can also be derived.\n",
    "Recall that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & \\propto & \n",
    "e^{\\sum_{i=1}^{N}f_i(x_i)+\\sum_{j=1}^{M}g_{j}(y_j)+\\sum_{i=1}^{N}\\sum_{j=1}^{M}h_{ij}(x_i,y_j)}\\,,\n",
    "\\end{eqnarray}\n",
    "so that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & \\propto & \n",
    "\\sum_{y_1\\in\\{0,1\\}}\\cdots\\sum_{y_M\\in\\{0,1\\}}\n",
    "e^{\\sum_{i=1}^{N}f_i(x_i)+\\sum_{j=1}^{M}b_{j}y_j+\\sum_{i=1}^{N}\\sum_{j=1}^{M}w_{ij}(x_i)\\,y_j}\n",
    "%\\\\\n",
    "%& = &\n",
    "%e^{\\sum_{i=1}^{N}f_i(x_i)}\n",
    "%\\sum_{y_1\\in\\{0,1\\}}\\cdots\\sum_{y_{M-1}\\in\\{0,1\\}}\n",
    "%e^{\\sum_{j=1}^{M-1}b_{j}y_j+\\sum_{i=1}^{N}\\sum_{j=1}^{M-1}w_{ij}(x_i)\\,y_j}\n",
    "%\\sum_{y_M\\in\\{0,1\\}}\n",
    "%e^{b_M y_M+\\sum_{i=1}^{N} w_{ij}(x_i)\\,y_M}\n",
    "\\\\\n",
    "& = &\n",
    "e^{\\sum_{i=1}^{N}f_i(x_i)}\n",
    "\\sum_{y_1\\in\\{0,1\\}}e^{b_1 y_1+\\sum_{i=1}^{N} w_{ij}(x_i)\\,y_1}\n",
    "\\cdots\\sum_{y_{M}\\in\\{0,1\\}}\n",
    "e^{b_M y_M+\\sum_{i=1}^{N} w_{ij}(x_i)\\,y_M}\\,.\n",
    "\\end{eqnarray}\n",
    "Therefore, we obtain\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & \n",
    "\\frac{e^{\\sum_{i=1}^{N}f_i(x_i)}\\prod_{j=1}^{M}\\left(1+e^{b_j+\\sum_{i=1}^{N} w_{ij}(x_i)}\\right)}\n",
    "{Z_{X}}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae83f7",
   "metadata": {},
   "source": [
    "## Observed inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7dc8db",
   "metadata": {},
   "source": [
    "The input vector ${\\bf x}$ forms the observed part of the RBM, and hence requires specialised handling to match the assumed input distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea3417",
   "metadata": {},
   "source": [
    "### Bernoulli inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a643b56",
   "metadata": {},
   "source": [
    "In some analyses, the input ${\\bf x}$ is a vector of binary values, i.e. ${\\bf x}\\in\\{0,1\\}^{N}$. One example is from the field of natural language processing, where each vocabulary word (or token) is either in or not in a given document. Another example is\n",
    "from the field of image processing, where a black-and-white image has pixels that are either on or off.\n",
    "Thus, we obtain\n",
    "\\begin{eqnarray}\n",
    "p(x_i=1\\mid{\\bf y}) & = & \\frac{e^{f_i(1)+\\sum_{j=1}^{M}h_{ij}(1,y_j)}}\n",
    "{e^{f_i(0)+\\sum_{j=1}^{M}h_{ij}(0,y_j)}+e^{f_i(1)+\\sum_{j=1}^{M}h_{ij}(1,y_j)}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{1}\n",
    "{1+e^{-\\left[f_i(1)-f_i(0)+\\sum_{j=1}^{M}[h_{ij}(1,y_j)-h_{ij}(0,y_j)]\\right]}}\n",
    "\\\\\n",
    "& = & \\sigma\\left(a_i+\\sum_{j=1}^{M}w_{ij}'(y_j)\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "where $a_i\\doteq f_i(1)-f_i(0)$ and $w_{ij}'(y_j)\\doteq h_{ij}(1,y_j)-h_{ij}(0,y_j)$.\n",
    "Hence, we may define\n",
    "$f_i(x_i)\\doteq a_i x_i$ and $h_{ij}(x_i,y_j)\\doteq x_i w_{ij}'(y_j)$, without loss of generality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1e906",
   "metadata": {},
   "source": [
    "Upon also assuming Bernoulli outputs, we further find that $h_{ij}(x_i,y_j)\\doteq x_i w_{ij} y_j$, which\n",
    "gives rise to the standard, bilinear model\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \n",
    "\\frac{e^{{\\bf a}^{T}{\\bf x}+{\\bf b}^{T}{\\bf y}+{\\bf x}^{T}W{\\bf y}}}\n",
    "{Z_{X,Y}}\\,,\n",
    "\\end{eqnarray}\n",
    "for coefficient vectors ${\\bf a}=(a_1,\\ldots,a_N)$ and ${\\bf b}=(b_1,\\ldots,b_M)$, and coefficient matrix\n",
    "$W=[w_{ij}]$, where we now interpret all vectors column-wise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4a16e",
   "metadata": {},
   "source": [
    "The conditional distributions from above therefore take the forms\n",
    "\\begin{eqnarray}\n",
    "p(x_i=1\\mid{\\bf y}) & = & \\sigma\\left(\\left[{\\bf a}+W{\\bf y}\\right]_i\\right)\\,,\n",
    "\\\\\n",
    "p(y_j=1\\mid{\\bf x}) & = & \\sigma\\left(\\left[{\\bf b}+W^T{\\bf x}\\right]_j\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "and the marginal distribution becomes\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) =\n",
    "\\frac{e^{{\\bf a}^{T}{\\bf x}}\\prod_{j=1}^{M}\\left(1+e^{\\left[{\\bf b}+W^T{\\bf x}\\right]_j}\\right)}\n",
    "{Z_{X}}\\,.\n",
    "\\end{eqnarray}\n",
    "Note, however, that even for the simplifications inherent in the Bernoulli RBM, the partition functions $Z_{X}$\n",
    "and $Z_{X,Y}$ remain intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a60e20",
   "metadata": {},
   "source": [
    "### Free energy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e8768",
   "metadata": {},
   "source": [
    "The `free energy` of ${\\bf x}$, denoted here as $F({\\bf x})$, is obtained by\n",
    "reusing the energy distribution formulation, namely:\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & \\frac{e^{-F({\\bf x})}}{Z_X}\n",
    "\\\\\n",
    "\\Rightarrow F({\\bf x}) & = & -\\ln p({\\bf x}) - \\ln Z_{X}\n",
    "\\\\\n",
    "& = & -{\\bf a}^{T}{\\bf x}-\\sum_{j=1}^{M}\\ln\\left(\n",
    "1+e^{[{\\bf b}+W^T{\\bf x}]_{j}}\n",
    "\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "The  mean free energy of a dataset $X$ is then defined as\n",
    "\\begin{eqnarray}\n",
    "\\bar{F}(X) & \\doteq & \\frac{1}{D}\\sum_{d=1}^{D}F({\\bf x}_d)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that since $Z_X$ is unknown in practice, we cannot compute the mean log-likelihood of $X$, and so we cannot score an individual dataset. However, for fixed RBM parameters,\n",
    "the difference between the scores of two datasets is equal to the difference between\n",
    "their respective mean free energies. Hence, we could, for example, monitor the difference in scores between the training set and a validation set. When this difference starts to grow persistently larger, it is a sign that the RBM might be overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d6b6e",
   "metadata": {},
   "source": [
    "### Gaussian inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d459e3",
   "metadata": {},
   "source": [
    "In other situations, it is more realistic that the ${\\bf x}$ values are unrestricted, i.e.\n",
    "${\\bf x}\\in\\mathbb{R}^{N}$. Typically, we take ${\\bf x}$ to be Gaussian distributed.\n",
    "Recalling that the $x_i$ values (treated as variables) must be conditionally independent in an RBM,\n",
    "we conclude that\n",
    "\\begin{eqnarray}\n",
    "p(x_i\\mid{\\bf y}) & = & [2\\pi\\sigma_i^2({\\bf y})]^{-\\frac{1}{2}}\n",
    "e^{-\\left.[x_i-\\mu_i({\\bf y})]^2\\right/2\\sigma_i^2({\\bf y})}\n",
    "\\\\\n",
    "& = & [2\\pi\\sigma_i^2({\\bf y})]^{-\\frac{1}{2}}\n",
    "e^{-\\left.\\left[x_i^2-2x_i\\mu_i({\\bf y})+\\mu_i^2({\\bf y})\\right]\\right/2\\sigma_i^2({\\bf y})}\n",
    "\\\\\n",
    "& = & e^{\\alpha_i({\\bf y})+\\beta_i({\\bf y})\\,x_i+\\gamma_i({\\bf y})\\,x_i^2}\\,,\n",
    "\\end{eqnarray}\n",
    "for appropriately defined coefficient functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75025f47",
   "metadata": {},
   "source": [
    "Hence, given Bernoulli outputs, we might modify the standard model to include independent squared terms, namely\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \n",
    "\\frac{e^{{\\bf a}^{T}{\\bf x}+{\\bf x}^{T}\\Gamma({\\bf y}){\\bf x}+{\\bf b}^{T}{\\bf y}+{\\bf x}^{T}W{\\bf y}}}\n",
    "{Z_{X,Y}}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\Gamma({\\bf y})\\doteq {\\tt diag}({\\bf c}+D{\\bf y})$.\n",
    "Letting ${\\bf x}^2\\doteq (x_1^2,\\ldots,x_N^2)$ for convenience, this may be rewritten as\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \n",
    "\\frac{e^{\n",
    "  ({\\bf a}\\oplus{\\bf c})^{T}({\\bf x}\\oplus{\\bf x}^2)+{\\bf b}^{T}{\\bf y}\n",
    "      +({\\bf x}\\oplus{\\bf x}^2)^{T}(W\\oplus D){\\bf y}\n",
    "}}{Z_{X,Y}}\\,,\n",
    "\\end{eqnarray}\n",
    "where the operator $\\oplus$ denotes column-wise concatenation.\n",
    "Hence, with some care required when resampling $p({\\bf x}\\mid{\\bf y})$, we may notionally augment our feature vector ${\\bf x}$ with its squared elements and thus reuse the standard bilinear model.\n",
    "In other words, defining $\\tilde{\\bf x}\\doteq{\\bf x}\\oplus{\\bf x}^2$, \n",
    "$\\tilde{\\bf a}\\doteq{\\bf a}\\oplus{\\bf c}$ and $\\tilde{W}\\doteq W\\oplus D$, we have\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \n",
    "\\frac{e^{\n",
    "  \\tilde{\\bf a}^{T}\\tilde{\\bf x}+{\\bf b}^{T}{\\bf y}\n",
    "      +\\tilde{\\bf x}^{T}\\tilde{W}{\\bf y}\n",
    "}}{Z_{X,Y}}\\,.\n",
    "\\end{eqnarray}\n",
    "In particular, we immediately obtain the conditional distribution\n",
    "\\begin{eqnarray}\n",
    "p(y_j=1\\mid{\\bf x}) & = & \\sigma\\left(\\left[{\\bf b}+\\tilde{W}^T\\tilde{\\bf x}\\right]_j\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "and the marginal distribution\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = &\n",
    "\\frac{e^{\\tilde{\\bf a}^{T}\\tilde{\\bf x}}\\prod_{j=1}^{M}\n",
    "\\left(1+e^{\\left[{\\bf b}+\\tilde{W}^T\\tilde{\\bf x}\\right]_j}\\right)}\n",
    "{Z_{X}}\\,.\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2ad1d",
   "metadata": {},
   "source": [
    "For the conditional Gaussian, we obtain\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}\\mid{\\bf y}) & \\propto & \n",
    "e^{\n",
    "  \\tilde{\\bf a}^T \\tilde{\\bf x}\n",
    "  + \\tilde{\\bf x}^T \\tilde{W}{\\bf y}\n",
    "}\n",
    "\\\\\n",
    "& = & e^{\n",
    "  ({\\bf a}\\oplus{\\bf c})^T ({\\bf x}\\oplus{\\bf x}^2)\n",
    "  + ({\\bf x}\\oplus{\\bf x}^2)^T (W\\oplus D){\\bf y}\n",
    "}\n",
    "\\\\\n",
    "& = & \\prod_{i=1}^{N}e^{\n",
    "  [{\\bf a}+W{\\bf y}]_i x_i + [{\\bf c}+D{\\bf y}]_i x_i^2\\,.\n",
    " }\n",
    "\\end{eqnarray}\n",
    "Equating coefficients with the standard Gaussian form above then gives\n",
    "the individual variances and means as\n",
    "\\begin{eqnarray}\n",
    "\\sigma_i^2({\\bf y}) = -\\frac{1}{2[{\\bf c}+D{\\bf y}]_i}\\,, && \n",
    "\\mu_i({\\bf y}) = -\\frac{[{\\bf a}+W{\\bf y}]_i}{2[{\\bf c}+D{\\bf y}]_i}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d6cf4",
   "metadata": {},
   "source": [
    "Note that the contstraint $\\sigma_i^2({\\bf y})>0$ requires enforcement, such that \n",
    "${\\bf c}+D{\\bf y}<{\\bf 0}\\; \\forall{\\bf y}\\in\\{0,1\\}^M$. Thus, it is necessary that\n",
    "${\\bf c}<{\\bf 0}$, and sufficient that $D<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e151bd0",
   "metadata": {},
   "source": [
    "## Gradient Approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7e931",
   "metadata": {},
   "source": [
    "Reconsider the Boltzmann Machine\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \\frac{e^{-E({\\bf x},{\\bf y})}}{Z_{X,Y}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where the energy function $E({\\bf x},{\\bf y})$ implicitly has model parameters $\\Theta$.\n",
    "Assuming that the output ${\\bf y}$ is always hidden, then we wish to estimate $\\Theta$ by maximising\n",
    "the marginal distribution $p({\\bf x})$ over all cases of training data. Typically, this is\n",
    "achieved by some gradient ascent procedure.\n",
    "\n",
    "However, in general the partition function $Z_{X,Y}$ is intractable to compute, and thus the gradient is also intractable. The solution is to approximate the gradient. There are various approaches, including Gibbs sampling and mean field approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d63a9",
   "metadata": {},
   "source": [
    "For convenience, let us temporarily assume that the input ${\\bf x}$ and output ${\\bf y}$ are both discrete valued. However, the derivation below is also valid for continuous variables with summations replaced by integrations. \n",
    "\n",
    "Thus, let the joint distribution be\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \\frac{e^{-E({\\bf x},{\\bf y})}}\n",
    "                      {\\sum_{\\bf x'}\\sum_{\\bf y'}e^{-E({\\bf x'},{\\bf y'})}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "such that the conditional distribution is\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}) & = & \\frac{e^{-E({\\bf x},{\\bf y})}}\n",
    "                      {\\sum_{\\bf y'}e^{-E({\\bf x},{\\bf y'})}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and the marginal distribution is\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & \\frac{\\sum_{\\bf y}e^{-E({\\bf x},{\\bf y})}}\n",
    "                      {\\sum_{\\bf x'}\\sum_{\\bf y'}e^{-E({\\bf x'},{\\bf y'})}}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\ln p({\\bf x}) & = & \\ln\\sum_{\\bf y}e^{-E({\\bf x},{\\bf y})}\n",
    "                      -\\ln\\sum_{\\bf x'}\\sum_{\\bf y'}e^{-E({\\bf x'},{\\bf y'})}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5d6df",
   "metadata": {},
   "source": [
    "Then, for each model parameter $\\theta\\in\\Theta$, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\theta}\\ln p({\\bf x}) & = &\n",
    "-\\frac{\\sum_{\\bf y}e^{-E({\\bf x},{\\bf y})}\\frac{\\partial E}{\\partial\\theta}({\\bf x},{\\bf y})}\n",
    "{\\sum_{\\bf y}e^{-E({\\bf x},{\\bf y})}}\n",
    "+\\frac{\\sum_{\\bf x'}\\sum_{\\bf y'}e^{-E({\\bf x'},{\\bf y'})}\\frac{\\partial E}{\\partial\\theta}({\\bf x'},{\\bf y'})}\n",
    "{\\sum_{\\bf x'}\\sum_{\\bf y'}e^{-E({\\bf x'},{\\bf y'})}}\n",
    "\\\\\n",
    "& = &\n",
    "-\\sum_{\\bf y}p({\\bf y}\\mid{\\bf x})\\frac{\\partial E}{\\partial\\theta}({\\bf x},{\\bf y})\n",
    "+\\sum_{\\bf x'}\\sum_{\\bf y'}p({\\bf x'},{\\bf y'})\\frac{\\partial E}{\\partial\\theta}({\\bf x'},{\\bf y'})\n",
    "\\\\\n",
    "& = & -\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[\\frac{\\partial E}{\\partial\\theta}({\\bf x},{\\bf y})\\right]\n",
    "+\\mathbb{E}_{{\\bf x'},{\\bf y'}}\\left[\\frac{\\partial E}{\\partial\\theta}({\\bf x'},{\\bf y'})\\right]\n",
    "\\\\\n",
    "& = & -\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[\\frac{\\partial E}{\\partial\\theta}({\\bf x},{\\bf y})\\right]\n",
    "+\\mathbb{E}_{{\\bf x'}}\\left[\n",
    "  \\mathbb{E}_{{\\bf y'}\\mid{\\bf x'}}\\left[\\frac{\\partial E}{\\partial\\theta}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\\\\n",
    "& = & +\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[\\frac{\\partial(-E)}{\\partial\\theta}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{{\\bf x'}}\\left[\n",
    "  \\mathbb{E}_{{\\bf y'}\\mid{\\bf x'}}\\left[\\frac{\\partial(-E)}{\\partial\\theta}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca21316",
   "metadata": {},
   "source": [
    "We assume that $p({\\bf y}\\mid{\\bf x})$ is tractable to compute, and thus the conditional expectations are also\n",
    "tractable. \n",
    "However, we have noted above that $p({\\bf x})$ and $p({\\bf x},{\\bf y})$ generally are not tractable, so we still have to resort to approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dbe06",
   "metadata": {},
   "source": [
    "### Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c583ad",
   "metadata": {},
   "source": [
    "We note that, under suitable conditions, expectations obey\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{X}\\left[f(X)\\right] & = & \\lim_{K\\rightarrow\\infty}\n",
    "\\frac{1}{K}\\sum_{k=1}^{K}f({\\bf x}_k)\\,,\n",
    "\\end{eqnarray}\n",
    "where ${\\bf x}_k\\sim p(X)$. In particular, for the single sample ${\\bf x}'$, $f({\\bf x}')$ is an unbiased estimator of $\\mathbb{E}_{X}\\left[f(X)\\right]$. Thus, the above gradient could be approximated by the stochastic gradient\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\theta}\\ln p({\\bf x}) & \\approx &\n",
    "\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[\\frac{\\partial(-E)}{\\partial\\theta}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{{\\bf y'}\\mid{\\bf x'}}\\left[\\frac{\\partial(-E)}{\\partial\\theta}({\\bf x'},{\\bf y'})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943f67a0",
   "metadata": {},
   "source": [
    "How are we supposed to sample ${\\bf x'}$ if computing $p({\\bf x'})$ is intractable?\n",
    "This is where the Gibbs sampling comes in. \n",
    "Since we are assuming that the conditional distributions are tractable, then we approximate unconditional distributions by conditional ones. Thus, we let\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x'}) = \\sum_{\\bf y}p({\\bf x'}\\mid{\\bf y})\\,p({\\bf y})\n",
    "& \\Rightarrow & \\mathbb{E}_{\\bf x'}[\\cdot] = \n",
    "\\mathbb{E}_{\\bf y}\\left[\\mathbb{E}_{{\\bf x'}\\mid{\\bf y}}[\\cdot]\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "using the other conditional distribution\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}\\mid{\\bf y}) & = & \n",
    "\\frac{e^{-E({\\bf x},{\\bf y})}}\n",
    "     {\\sum_{\\bf x'}e^{-E({\\bf x'},{\\bf y})}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which is also assumed to be tractable to compute.\n",
    "\n",
    "Now we are faced with the fact that $p({\\bf y})$ is also intractable. Hence, we\n",
    "repeat the above step, letting\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}) = \\sum_{\\bf x}p({\\bf y}\\mid{\\bf x})\\,p({\\bf x})\n",
    "& \\Rightarrow & \\mathbb{E}_{\\bf y}[\\cdot] = \n",
    "\\mathbb{E}_{\\bf x}\\left[\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}[\\cdot]\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "We could repeat this cycle any number of times, corresponding to multiple steps of \n",
    "sequential Gibbs sampling. However, we are already given ${\\bf x}$, so we halt with the approximation\n",
    "that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{\\bf y}[\\cdot] & \\approx & \\mathbb{E}_{{\\bf y}|{\\bf x}}[\\cdot]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "on the basis that $f({\\bf x})$ is an unbiased estimate of \n",
    "$\\mathbb{E}_{\\bf x}[f({\\bf x})]$, using the same argument as above.\n",
    "\n",
    "This results in the final approximation\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\theta}\\ln p({\\bf x}) & \\approx &\n",
    "\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[\\frac{\\partial(-E)}{\\partial\\theta}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}\\left[\n",
    " \\mathbb{E}_{{\\bf x'}\\mid{\\bf y}}\\left[\n",
    "  \\mathbb{E}_{{\\bf y'}\\mid{\\bf x'}}\\left[\n",
    "   \\frac{\\partial(-E)}{\\partial\\theta}\\left({\\bf x'},{\\bf y'}\\right)\n",
    "  \\right]\n",
    " \\right]\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0806e1",
   "metadata": {},
   "source": [
    "In practice, Gibbs sampling of ${\\bf y}$ and ${\\bf x'}$ replaces the outer two expectations of the negative term. Hence, the Gibbs sampling algorithm is:\n",
    "1. For visible input ${\\bf x}$, compute the distribution \n",
    "$p({\\bf y}\\mid{\\bf x})$ of the hidden output, and compute\n",
    "the expectation term $\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}[\\cdot]$ in the gradient.\n",
    "2. Sample the hidden output ${\\bf y}$ from the distribution \n",
    "$p({\\bf y}\\mid{\\bf x})$. \n",
    "The pair ${\\bf x},{\\bf y}$ form the so-called *positive* case.\n",
    "3. Using ${\\bf y}$, compute the distribution $p({\\bf x'}\\mid{\\bf y})$, and sample ${\\bf x'}$.\n",
    "4. Using ${\\bf x'}$, compute the distribution\n",
    "$p({\\bf y'}\\mid{\\bf x'})$, and compute the expectation term $\\mathbb{E}_{{\\bf y'}\\mid{\\bf x'}}[\\cdot]$.\n",
    "The pair ${\\bf x'},{\\bf y'}$ form the *negative* case.\n",
    "5. Compute the approximate stochastic gradient as the difference of expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64459af6",
   "metadata": {},
   "source": [
    "### Mean field approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf70a3",
   "metadata": {},
   "source": [
    "Note that in the special case where $E({\\bf x},{\\bf y})$ is linear in ${\\bf y}$\n",
    "for parameter $\\theta$, e.g.\n",
    "\\begin{eqnarray}\n",
    "E({\\bf x},{\\bf y}) & = & {\\bf w}({\\bf x},\\theta)^{T}{\\bf y}+\\cdots\\,,\n",
    "\\end{eqnarray}\n",
    "then we have\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}\\left[\n",
    "  \\frac{\\partial E}{\\partial\\theta}({\\bf x},{\\bf y})\n",
    "\\right]\n",
    "& = & \\frac{\\partial E}{\\partial\\theta}\\left(\n",
    "  {\\bf x},\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}\\left[{\\bf y}\\right]\n",
    "\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "exactly. If, however, there are nonlinearities in ${\\bf y}$ then the above does not hold\n",
    "exactly, but it does still approximately hold true. This is the `mean field` approximation.\n",
    "\n",
    "To see how the mean field approximation works, we define\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\bf y}_{\\bf x} & = & \\bar{\\bf y}({\\bf x}) \\doteq \n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}[{\\bf y}]\\,,\n",
    "\\end{eqnarray}\n",
    "and consider the \n",
    "first-order Taylor approximation\n",
    "\\begin{eqnarray}\n",
    "E({\\bf x},{\\bf y}) & \\approx & E({\\bf x},\\bar{\\bf y}_{\\bf x})\n",
    "+ ({\\bf y}-\\bar{\\bf y}_{\\bf x})^{T}\\frac{\\partial E}{\\partial{\\bf y}}\n",
    "({\\bf x},\\bar{\\bf y}_{\\bf x})\n",
    "\\\\\n",
    "\\Rightarrow \\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[E({\\bf x},{\\bf y})\\right]\n",
    "& \\approx & E({\\bf x},\\bar{\\bf y}_{\\bf x})\n",
    "+ \\left(\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[{\\bf y}\\right]\n",
    "-\\bar{\\bf y}_{\\bf x}\\right)^{T}\n",
    "\\frac{\\partial E}{\\partial{\\bf y}}({\\bf x},\\bar{\\bf y}_{\\bf x})\n",
    "\\\\\n",
    "%\\Rightarrow \\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[E({\\bf x},{\\bf y})\\right]\n",
    "%& \\approx &\n",
    "& = &\n",
    "E\\left({\\bf x},\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}[{\\bf y}]\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Taking derivatives, we see that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[\n",
    "  \\frac{\\partial E}{\\partial\\theta}({\\bf x},{\\bf y})\n",
    "\\right]\n",
    "& \\approx & \\frac{\\partial E}{\\partial\\theta}\\left(\n",
    "  {\\bf x},\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[{\\bf y}\\right]\n",
    "\\right)\n",
    "\\end{eqnarray}\n",
    "also holds true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196266d",
   "metadata": {},
   "source": [
    "If we now apply the mean field approximation to the Gibbs sampling approximation\n",
    "of the gradient (above), then we obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\theta}\\ln p({\\bf x}) & \\approx &\n",
    "\\mathbb{E}_{{\\bf y}|{\\bf x}}\\left[\n",
    "  \\frac{\\partial(-E)}{\\partial\\theta}({\\bf x},{\\bf y})\n",
    "\\right]\n",
    "-\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}\\left[\n",
    " \\mathbb{E}_{{\\bf x'}\\mid{\\bf y}}\\left[\n",
    "  \\mathbb{E}_{{\\bf y'}\\mid{\\bf x'}}\\left[\n",
    "   \\frac{\\partial(-E)}{\\partial\\theta}\\left({\\bf x'},{\\bf y'}\\right)\n",
    "  \\right]\n",
    " \\right]\n",
    "\\right]\n",
    "\\\\\n",
    "& \\approx &\n",
    "\\frac{\\partial(-E)}{\\partial\\theta}\\left({\\bf x},\\bar{\\bf y}_{\\bf x}\\right)\n",
    "-\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}\\left[\n",
    " \\mathbb{E}_{{\\bf x'}\\mid{\\bf y}}\\left[\n",
    "   \\frac{\\partial(-E)}{\\partial\\theta}\\left({\\bf x'},\\bar{\\bf y}_{\\bf x'}\\right)\n",
    " \\right]\n",
    "\\right]\n",
    "\\\\\n",
    "& \\approx &\n",
    "\\frac{\\partial(-E)}{\\partial\\theta}\\left({\\bf x},\\bar{\\bf y}_{\\bf x}\\right)\n",
    "-\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}\\left[\n",
    "   \\frac{\\partial(-E)}{\\partial\\theta}\n",
    "   \\left(\\bar{\\bf x}_{\\bf y},\\bar{\\bf y}(\\bar{\\bf x}_{\\bf y})\\right)\n",
    "\\right]\n",
    "\\\\\n",
    "& \\approx &\n",
    "\\frac{\\partial(-E)}{\\partial\\theta}\\left({\\bf x},\\bar{\\bf y}_{\\bf x}\\right)\n",
    "-\\frac{\\partial(-E)}{\\partial\\theta}\n",
    "   \\left(\n",
    "    \\bar{\\bf x}\\left(\\bar{\\bf y}_{\\bf x}\\right),\n",
    "    \\bar{\\bf y}\\left(\\bar{\\bf x}\\left(\\bar{\\bf y}_{\\bf x}\\right)\\right)\n",
    "   \\right)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e4ba7",
   "metadata": {},
   "source": [
    "### Bernoulli RBM gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56194372",
   "metadata": {},
   "source": [
    "For the Bernoulli RBM, we have (from above) that\n",
    "\\begin{eqnarray}\n",
    "-E({\\bf x},{\\bf y}) & = & {\\bf a}^{T}{\\bf x}+{\\bf b}^{T}{\\bf y}+{\\bf x}^{T}W{\\bf y}\\,,\n",
    "\\end{eqnarray}\n",
    "and thus\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial(-E)}{\\partial{\\bf a}} = {\\bf x}\\,,\n",
    "&\\;\n",
    "\\frac{\\partial(-E)}{\\partial{\\bf b}} = {\\bf y}\\,,\n",
    "&\n",
    "\\frac{\\partial(-E)}{\\partial W} = {\\bf x}\\,{\\bf y}^{T}\\,.\n",
    "\\end{eqnarray}\n",
    "Furthermore, we find that\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\bf y}({\\bf x}) & = & [p(y_j=1\\mid{\\bf x})]_{j=1}^{M}\\,,\n",
    "\\\\\n",
    "\\bar{\\bf x}({\\bf y}) & = & [p(x_i=1\\mid{\\bf y})]_{i=1}^{N}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d5fbe",
   "metadata": {},
   "source": [
    "#### Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb81455",
   "metadata": {},
   "source": [
    "Under the Gibbs sampling approximation described above, we first sample \n",
    "${\\bf y}\\sim\\bar{\\bf y}({\\bf x})$ and let ${\\bf y}$ stand in for\n",
    "$\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}[\\cdot]$. Next, we sample\n",
    "${\\bf x'}\\sim\\bar{\\bf x}({\\bf y})$, and let ${\\bf x'}$ stand in for\n",
    "$\\mathbb{E}_{{\\bf x'}\\mid{\\bf y}}[\\cdot]$.\n",
    "This leaves only $\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}[\\cdot]$ and\n",
    "$\\mathbb{E}_{{\\bf y'}\\mid{\\bf x'}}[\\cdot]$ to be evaluated.\n",
    "Hence,\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial{\\bf a}}\\ln p({\\bf x}) & \\approx & {\\bf x} - {\\bf x'}\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial{\\bf b}}\\ln p({\\bf x}) & \\approx & \n",
    "\\bar{\\bf y}_{\\bf x}-\\bar{\\bf y}_{\\bf x'}\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial{\\bf W}}\\ln p({\\bf x}) & \\approx & \n",
    "{\\bf x}\\,\\bar{\\bf y}_{\\bf x}^{T}-{\\bf x'}\\bar{\\bf y}_{\\bf x'}^{T}\\,.\n",
    "\\end{eqnarray}\n",
    "In practice, this approximation does not work well, with the parameters seemingly wandering about randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740d48d",
   "metadata": {},
   "source": [
    "#### Hinton modified gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07bd6a0",
   "metadata": {},
   "source": [
    "[Hinton](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf) offers a practical guide to training RBMs, although I found the commentary to be rather terse. Briefly, Hinton asserts that the positive (or data) term in the expectations above should couple the binary input with binary (sampled) output. However, the negative (or reconstruction) term can seemingly forego sampling altogether. My interpretation is that this leads to the modified gradient scheme:\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial{\\bf a}}\\ln p({\\bf x}) & \\approx & \n",
    "{\\bf x} - \\bar{\\bf x}_{\\bf y}\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial{\\bf b}}\\ln p({\\bf x}) & \\approx & \n",
    "{\\bf y}-\\bar{\\bf y}(\\bar{\\bf x}_{\\bf y})\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial{\\bf W}}\\ln p({\\bf x}) & \\approx & \n",
    "{\\bf x}\\,{\\bf y}^{T}-\\bar{\\bf x}_{\\bf y}\\,\\bar{\\bf y}(\\bar{\\bf x}_{\\bf y})^{T}\\,,\n",
    "\\end{eqnarray}\n",
    "where we sample ${\\bf y}\\sim\\bar{\\bf y}({\\bf x})$ as before.\n",
    "\n",
    "This scheme seems to work well, after a burn-in training period of random fluctuations.\n",
    "It is interesting to note that in practice this Hinton-modified gradient appears to minmise the RMS error discussed in a later section. This would appear to be due to the fact that both the modified gradient above and the reconstruction probabilities (later)\n",
    "utilise some aspects of mean field approximations (see the next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4656701",
   "metadata": {},
   "source": [
    "#### Mean field approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089b0e9",
   "metadata": {},
   "source": [
    "The mean field approximation, described in detail above, is straightforward to\n",
    "apply. The resulting gradient approximation is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial{\\bf a}}\\ln p({\\bf x}) & \\approx & \n",
    "{\\bf x} - \\bar{\\bf x}\\left(\\bar{\\bf y}_{\\bf x}\\right)\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial{\\bf b}}\\ln p({\\bf x}) & \\approx & \n",
    "\\bar{\\bf y}_{\\bf x}-\\bar{\\bf y}\\left(\n",
    "  \\bar{\\bf x}\\left(\\bar{\\bf y}_{\\bf x}\\right)\n",
    "\\right)\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial{\\bf W}}\\ln p({\\bf x}) & \\approx & \n",
    "{\\bf x}\\,\\bar{\\bf y}_{\\bf x}^{T}-\\bar{\\bf x}\\left(\\bar{\\bf y}_{\\bf x}\\right)\n",
    "\\,\n",
    "\\bar{\\bf y}\\left(\n",
    "  \\bar{\\bf x}\\left(\\bar{\\bf y}_{\\bf x}\\right)\n",
    "\\right)^{T}\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Clearly, this is closely related to the Hinton-modified gradient above, except that \n",
    "now no Gibbs sampling is required. \n",
    "In practice, this gradient approximation appears to work very well, and seemingly has better convergence than the Hinton-modified gradient (although YMMV). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1042078",
   "metadata": {},
   "source": [
    "### Gaussian RBM gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873488e",
   "metadata": {},
   "source": [
    "For the Gaussian RBM (i.e. Gaussian input with Bernoulii output), the negative energy function is\n",
    "\\begin{eqnarray}\n",
    "-E({\\bf x},{\\bf y}) & = & {\\bf a}^{T}{\\bf x}+{\\bf c}^{T}{\\bf x}^2\n",
    "+{\\bf b}^{T}{\\bf y}+{\\bf x}^{T}W{\\bf y}+({\\bf x}^{2})^{T}D{\\bf y}\\,,\n",
    "\\end{eqnarray}\n",
    "and thus we obtain the above derivatives in ${\\bf a}$, ${\\bf b}$ and $W$, as well as\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial(-E)}{\\partial{\\bf c}} = {\\bf x}^2\\,,\n",
    "&&\n",
    "\\frac{\\partial(-E)}{\\partial D} = {\\bf x}^{2}\\,{\\bf y}^{T}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that to preserve the constraints $c_i<0$ and $d_{ij}<0$, we might choose\n",
    "$c_i\\doteq -e^{c'_i}$ and $d_{ij}\\doteq -e^{d_{ij}'}$, such that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial(-E)}{\\partial c_{i}'} = \n",
    "\\frac{\\partial(-E)}{\\partial c_{i}}\\frac{\\partial c_{i}}{\\partial c_{i}'} = x_{i}^2\\,c_{i}\\,,\n",
    "&&\n",
    "\\frac{\\partial(-E)}{\\partial d_{ij}'} =\n",
    "\\frac{\\partial(-E)}{\\partial d_{ij}}\\frac{\\partial d_{ij}}{\\partial d_{ij}'} =\n",
    "x_{i}^{2}\\,y_{j}\\,d_{ij}\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, we might simply rectify the updated estimates of ${\\bf c}$ and $D$ to thus obey the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72baae05",
   "metadata": {},
   "source": [
    "## Non-standard Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac4249",
   "metadata": {},
   "source": [
    "RBMs can be rather difficult to train, since the usual parameter update scheme described above does not really maximise any particular likelihood [(Hinton)](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf). Additionally, since computing $p({\\bf x})$ is intractable, we cannot properly score the updates to test for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc963517",
   "metadata": {},
   "source": [
    "In practice, we need to use some sort of approximation to $p({\\bf x})$. One approach is offered by the mean field approximation (described in an earlier section). We note that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & \\sum_{\\bf y}p({\\bf x}\\mid{\\bf y})\\,p({\\bf y})\n",
    "= \\mathbb{E}_{\\bf y}\\left[p({\\bf x}\\mid{\\bf y})\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "and hence, following previous reasoning, we have\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & \\approx & \n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}}\\left[p({\\bf x}\\mid{\\bf y})\\right]\n",
    "\\approx\n",
    "p\\left({\\bf x}\\mid\\bar{\\bf y}_{\\bf x}\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we recall that RBMs obey the conditional independence property\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}\\mid{\\bf y}) & = & \\prod_{i=1}^{N}p(x_i\\mid{\\bf y})\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, for a Bernoulli RBM we have\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}\\mid{\\bf y}) & = & \\prod_{i=1}^{N}\n",
    "p\\left(x_i=1\\mid{\\bf y}\\right)^{\\,x_i}\n",
    "\\,\n",
    "\\left[1 - p\\left(x_i=1\\mid{\\bf y}\\right)\\right]^{\\,1-x_i}\n",
    "\\\\\n",
    "& = &\n",
    "\\prod_{i=1}^{N}\n",
    "\\bar{x}_i\\left({\\bf y}\\right)^{x_i}\n",
    "\\,\\left[1-\\bar{x}_i\\left({\\bf y}\\right)\\right]^{1-x_i}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\bar{x}_i({\\bf y}) & \\doteq &\n",
    "p(x_i=1\\mid{\\bf y}) =\n",
    "\\mathbb{E}_{{\\bf x}\\mid{\\bf y}}[x_i] =\n",
    "\\left[\\bar{\\bf x}({\\bf y})\\right]_i\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The mean field approximate probability, i.e. the so-called *reconstruction* probability, is therefore\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & \\approx & \n",
    "\\prod_{i=1}^{N}\n",
    "\\bar{x}_i\\left(\\bar{\\bf y}_{\\bf x}\\right)^{x_i}\n",
    "\\,\\left[1-\\bar{x}_i\\left(\\bar{\\bf y}_{\\bf x}\\right)\\right]^{1-x_i}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd72eb",
   "metadata": {},
   "source": [
    "### Minimising the reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c634a0",
   "metadata": {},
   "source": [
    "As another approach, suppose we approximate $p({\\bf x})$, and score how closely this approximation is to the binary input data. Specifically, we minimise the mean square error\n",
    "\\begin{eqnarray}\n",
    "R^2 & \\doteq & \\frac{1}{D}\\sum_{d=1}^{D}\\sum_{i=1}^{N}(x_{di}-p_{di})^2\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "p_{di} & \\doteq & \\bar{x}_i({\\bf q}_d) = \\sigma\\left(a_i+\\sum_{j=1}^{M}w_{ij} q_{dj}\\right)\\,,\n",
    "\\\\\n",
    "q_{dj} & \\doteq & \\bar{y}_j({\\bf x}_d) = \\sigma\\left(b_j+\\sum_{i=1}^{N}x_{di} w_{ij}\\right)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9186f3",
   "metadata": {},
   "source": [
    "We note that, for arbitrary parameter $\\theta$, the gradient is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial R^2}{\\partial\\theta} & = & \n",
    "-\\frac{2}{D}\\sum_{d=1}^{D}\\sum_{i=1}^{N}\\delta_i(\\theta)\\,(x_{di}-p_{di})\n",
    "\\frac{\\partial p_{di}}{\\partial\\theta}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\delta_i(\\theta)$ is a notional 0/1 indicator that causes the summation over $i$ to be dropped if parameter $\\theta$ is indexed by $i$.\n",
    "Furthermore, we see that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial p_{di}}{\\partial\\theta} & = & \n",
    "p_{di}\\,(1-p_{di})\\left\\{ \n",
    "\\frac{\\partial a_i}{\\partial\\theta}\n",
    "+\\sum_{j=1}^{M}\\delta_j(\\theta)\\,\\frac{\\partial w_{ij}}{\\partial\\theta} q_{dj}\n",
    "+\\sum_{j=1}^{M}\\delta_j(\\theta)\\,w_{ij}\\frac{\\partial q_{dj}}{\\partial\\theta}\n",
    "\\right\\}\\,,\n",
    "\\end{eqnarray}\n",
    "since $\\sigma'(z)=\\sigma(z)\\,[1-\\sigma(z)]$.\n",
    "Similarly, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial q_{dj}}{\\partial\\theta} & = & \n",
    "q_{dj}\\,(1-q_{dj})\\left\\{ \n",
    "\\frac{\\partial b_j}{\\partial\\theta}\n",
    "+\\sum_{i=1}^{N}\\delta_i(\\theta)\\,x_{di}\\frac{\\partial w_{ij}}{\\partial\\theta}\n",
    "\\right\\}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70f3db",
   "metadata": {},
   "source": [
    "Consequently, we derive that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial q_{dj}}{\\partial a_i} & = & 0\\,,\\; \n",
    "\\frac{\\partial p_{di}}{\\partial a_i}=p_{di}\\,(1-p_{di})\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial R^2}{\\partial\\,{\\bf a}} & = & \n",
    "-2\\,{\\tt mean}(A)\\,,\\; A \\doteq (X-P)\\otimes P\\otimes (1-P)\\,,\n",
    "\\end{eqnarray}\n",
    "for element-wise multiplicative operator $\\otimes$, where the function \n",
    "${\\tt mean}(\\cdot)$ averages over the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150cc82",
   "metadata": {},
   "source": [
    "Similarly, we find that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial q_{dj}}{\\partial b_j} & = & q_{dj}\\,(1-q_{dj})\\,,\\; \n",
    "\\frac{\\partial p_{di}}{\\partial b_j}=p_{di}\\,(1-p_{di})\\,\n",
    "w_{ij}\\frac{\\partial q_{dj}}{\\partial b_j}\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial R^2}{\\partial\\,{\\bf b}} & = & \n",
    "-2\\,{\\tt mean}([AW]\\otimes B)\\,,\\; B \\doteq Q\\otimes (1-Q)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a76bd3",
   "metadata": {},
   "source": [
    "Lastly, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial q_{dj}}{\\partial w_{ij}} & = & x_{di}\\,q_{dj}\\,(1-q_{dj})\\,,\\;\n",
    "\\frac{\\partial p_{di}}{\\partial w_{ij}}=p_{di}\\,(1-p_{di})\\,\\left\\{\n",
    "q_{dj}+w_{ij}\\frac{\\partial q_{dj}}{\\partial w_{ij}}\n",
    "\\right\\}\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial R^2}{\\partial W} & = &\n",
    "-\\frac{2}{D}\\left\\{\n",
    "A^{T}Q+W\\otimes\\left([A\\otimes X]^{T}B\\right)\n",
    "\\right\\}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d1c0f",
   "metadata": {},
   "source": [
    "Note that we need to update the parameter estimates in the opposite direction of these gradients in order to minimise the reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b8e32",
   "metadata": {},
   "source": [
    "### Maximising the approximate likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a1af5",
   "metadata": {},
   "source": [
    "Following similar reasoning to that above, we could instead maximise the approximate likelihood defined earlier. Reusing the definition of the reconstruction probability, we recall that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}_{d}) & \\approx & \\prod_{i=1}^{N} p_{di}^{x_{di}}(1-p_{di})^{1-x_{di}}\\,.\n",
    "\\end{eqnarray}\n",
    "This leads to the average log-likelihood\n",
    "\\begin{eqnarray}\n",
    "L & = & \\frac{1}{D}\\ln\\prod_{d=1}^{D} p({\\bf x}_{d})\n",
    "\\\\\n",
    "& \\approx & \\frac{1}{D}\\sum_{d=1}^{D}\\sum_{i=1}^{N}\\left[\n",
    "x_{di}\\ln p_{di}+(1-x_{di})\\ln(1-p_{di})\n",
    "\\right]\n",
    "\\\\\n",
    "\\Rightarrow\\frac{\\partial L}{\\partial\\theta} & \\approx & \n",
    "\\frac{1}{D}\\sum_{d=1}^{D}\\sum_{i=1}^{N}\\delta_{i}(\\theta)\\,\\left[\n",
    "\\frac{x_{di}}{p_{di}}\n",
    "-\\frac{1-x_{di}}{1-p_{di}}\n",
    "\\right]\\,\\frac{\\partial p_{di}}{\\partial\\theta}\\,.\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a64518",
   "metadata": {},
   "source": [
    "Thus, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial a_i} & \\approx &\n",
    "\\frac{1}{D}\\sum_{d=1}^{D}\\left[\n",
    "x_{di}(1-p_{di})-(1-x_{di})p_{di}\n",
    "\\right]\n",
    "= \\frac{1}{D}\\sum_{d=1}^{D}(x_{di}-p_{di})\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial\\,{\\bf a}} & \\approx &\n",
    "{\\tt mean}(X-P)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62cad16",
   "metadata": {},
   "source": [
    "Similarly, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial b_j} & \\approx &\n",
    "\\frac{1}{D}\\sum_{d=1}^{D}\\sum_{i=1}^{N}(x_{di}-p_{di})\\,w_{ij}q_{dj}\\,(1-q_{dj})\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial\\,{\\bf b}} & \\approx &\n",
    "{\\tt mean}([(X-P)W]\\otimes B)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfe0d9",
   "metadata": {},
   "source": [
    "Lastly, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial w_{ij}}\n",
    "& \\approx & \\frac{1}{D}\\sum_{d=1}^{D}\n",
    "(x_{di}-p_{di})\\left[\n",
    "q_{dj}+w_{ij}x_{di}q_{dj}\\,(1-q_{dj})\n",
    "\\right]\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial W} & \\approx &\n",
    "\\frac{1}{D}\\left\\{\n",
    "(X-P)^{T}Q+W\\otimes\\left([(X-P)\\otimes X]^{T}B\\right)\n",
    "\\right\\}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa24434",
   "metadata": {},
   "source": [
    "## Sequential Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0071baa",
   "metadata": {},
   "source": [
    "[(Larochelle and Murray)](http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf)\n",
    "extend the RBM to the case where the input vector ${\\bf x}$ has internal Bayesian Network dependencies. \n",
    "They call this the Neural Autoregressive Distribution Estimator (NADE).\n",
    "In particular, it is assumed that the\n",
    "elements ${\\bf x}=(x_1,x_2,\\ldots,x_N)$ have been canonically ordered in some fashion,\n",
    "for example either randomly or by reason of causal effects. Thus, the input dependencies form an ordered Markov network (see the figure below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a903a4",
   "metadata": {},
   "source": [
    "![Network Restricted Boltzmann Machine graph](NADE.png \"Networked Restricted Boltzmann Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddec377",
   "metadata": {},
   "source": [
    "Borrowing Bayesian Network methodology, and temporarily ignoring the hidden output,\n",
    "we now suppose that the visible input has the joint distribution:\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & p(x_1,x_2,\\ldots,x_N)\n",
    "\\\\\n",
    "& \\doteq & p(x_1)\\,p(x_2\\mid x_1)\\,p(x_3\\mid x_1, x_2)\\cdots \n",
    "p(x_N\\mid x_1,x_2,\\ldots,x_{N-1})\n",
    "\\\\\n",
    "& = &\n",
    "\\prod_{i=1}^{N} p(x_i\\mid{\\bf x}_{1:i-1})\\,,\n",
    "\\end{eqnarray}\n",
    "where we define ${\\bf x}_{j:k}\\doteq (x_j,x_{j+1},\\ldots,x_k)$ for $j\\le k\\in\\{1,2,\\ldots,N\\}$. For convenience, whenever $j>k$ we let\n",
    "${\\bf x}_{j:k}=(\\,)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3c3f5",
   "metadata": {},
   "source": [
    "Next, we reintroduce the binary hidden output ${\\bf y}\\in\\{0,1\\}^{M}$, such that\n",
    "\\begin{eqnarray}\n",
    "p(x_i\\mid{\\bf x}_{1:i-1}) & = & \\sum_{{\\bf y}\\in\\{0,1\\}^{M}}\n",
    "p(x_i,{\\bf y}\\mid{\\bf x}_{1:i-1})\\,,\n",
    "\\end{eqnarray}\n",
    "where (from the diagram)\n",
    "\\begin{eqnarray}\n",
    "p(x_i,{\\bf y}\\mid{\\bf x}_{1:i-1}) & = &\n",
    "p(x_i\\mid{\\bf y},{\\bf x}_{1:i-1})\\,\n",
    "p({\\bf y}\\mid{\\bf x}_{1:i-1})\\,.\n",
    "\\end{eqnarray}\n",
    "In order to keep the networked model reasonably simple, we now ignore the input dependencies and define\n",
    "\\begin{eqnarray}\n",
    "p(x_i\\mid{\\bf y},{\\bf x}_{1:i-1}) & \\doteq & p(x_i\\mid {\\bf y})\\,.\n",
    "\\end{eqnarray}\n",
    "Effectively, we have returned to the standard RBM formulation, but have implicitly retained the dependencies amongst the input units by utilising their effects on the output units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed84ba5",
   "metadata": {},
   "source": [
    "Thus, for the Bernoulli RBM, we have\n",
    "\\begin{eqnarray}\n",
    "\\bar{x}_i({\\bf y}) & = &\n",
    "p(x_i=1\\mid{\\bf y}) = \\sigma\\left(\\left[{\\bf a}+W{\\bf y}\\right]_i\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "as before, but now we use the truncated models\n",
    "\\begin{eqnarray}\n",
    "\\bar{y}_j({\\bf x}_{1:k}) & = & p(y_j=1\\mid{\\bf x}_{1:k}) \\doteq \n",
    "\\sigma\\left(\\left[{\\bf b}+W_{1:k,:}^{T}{\\bf x}_{1:k}\\right]_j\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "for $k=1,2,\\ldots,N$,\n",
    "where $W_{1:k,:}$ is the matrix obtained by retaining the first $k$ rows of $W$.\n",
    "For convenience, with $i=1$ and $k=i-1$, we take $\\bar{y}_j({\\bf x}_{1:0})=\\sigma(b_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8c483",
   "metadata": {},
   "source": [
    "Reusing the derivations above, we similarly assume that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}_{1:i-1}) & = &\n",
    "\\prod_{j=1}^{M}\\bar{y}_j({\\bf x}_{1:i-1})^{y_j}\n",
    "\\left[1-\\bar{y}_j({\\bf x}_{1:i-1})\\right]^{1-y_j}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(x_i=1\\mid{\\bf x}_{1:i-1}) & = & \\sum_{{\\bf y}\\in\\{0,1\\}^M}\n",
    "\\bar{x}_i({\\bf y})\\,p({\\bf y}\\mid{\\bf x}_{1:i-1})\n",
    "= \\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i-1}}\\left[\\bar{x}_i({\\bf y})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "However, we note that this summation remains intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46db73b",
   "metadata": {},
   "source": [
    "In order to obtain a tractable model,\n",
    "[(Larochelle and Murray)](http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf)\n",
    "used a mean field approximation to $p(x_i\\mid{\\bf x}_{1:i-1})$, namely\n",
    "\\begin{eqnarray}\n",
    "p(x_i=1\\mid{\\bf x}_{1:i-1}) & \\approx & \\bar{x}_i(\\bar{\\bf y}({\\bf x}_{1:i-1}))\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the joint probability of input ${\\bf x}$ is\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & \\approx & \\prod_{i=1}^{N}\n",
    "\\bar{x}_i\\left(\\bar{\\bf y}\\left({\\bf x}_{1:i-1}\\right)\\right)^{\\,x_i}\\,\n",
    "\\left[1-\\bar{x}_i\\left(\\bar{\\bf y}\\left({\\bf x}_{1:i-1}\\right)\\right)\n",
    "\\right]^{\\,1-x_i}\\,.\n",
    "\\end{eqnarray}\n",
    "This is the essence of the NADE model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51a621",
   "metadata": {},
   "source": [
    "Observe that this is just a modified form of the approximate reconstruction probability derived in an earlier section. \n",
    "It follows that most of the maths we previously derived for the gradient of the log-likelihood still holds. Thus, we compute\n",
    "\\begin{eqnarray}\n",
    "\\bar{y}_j^{(i)}({\\bf x}) & \\doteq &\\bar{y}_j({\\bf x}_{1:i-1}) = \\sigma\\left(b_j+\\sum_{k=1}^{i-1}x_k w_{kj}\\right)\\,,\n",
    "\\\\\n",
    "\\bar{x}_i\\left(\\bar{\\bf y}^{(i)}_{\\bf x}\\right) \n",
    "& = & \\sigma\\left(a_i+\\sum_{j=1}^{M}w_{ij}\\bar{y}_j^{(i)}({\\bf x})\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x}) & \\approx & \\sum_{i=1}^{N}\\left\\{\n",
    "  x_i\\ln \\bar{x}_i\\left(\\bar{\\bf y}^{(i)}_{\\bf x}\\right)\n",
    "  +(1-x_i)\\ln\\left[1-\\bar{x}_i\\left(\\bar{\\bf y}^{(i)}_{\\bf x}\\right)\\right]\n",
    "\\right\\}\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we obtain the approximate gradients\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial a_i}\\ln p({\\bf x}) & \\approx & \n",
    "x_i-\\bar{x}_i\\left(\\bar{\\bf y}^{(i)}_{\\bf x}\\right)\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial b_j}\\ln p({\\bf x}) & \\approx & \n",
    "\\sum_{i=1}^{N}B_{ij}\\,,\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial w_{ij}}\\ln p({\\bf x}) & \\approx & \n",
    "\\left[x_i-\\bar{x}_i\\left(\\bar{\\bf y}^{(i)}_{\\bf x}\\right)\\right]\n",
    "\\,\\bar{y}_j^{(i)}({\\bf x})\n",
    "+x_i\\sum_{k=i+1}^{N}B_{kj}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "B_{ij} & = & \n",
    "\\left[x_i-\\bar{x}_i\\left(\\bar{\\bf y}^{(i)}_{\\bf x}\\right)\\right]\n",
    "  \\,w_{ij}\n",
    "  \\,\\bar{y}_j^{(i)}({\\bf x})\\left[1-\\bar{y}_j^{(i)}({\\bf x})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c0d6a",
   "metadata": {},
   "source": [
    "However, we might recall the various gradient schemes that we have derived so far, namely the Hinton-modified gradient and the mean field approximation, as well the explicit gradients of the reconstruction error and the log reconstruction probability.\n",
    "In application, all of these gradient schemes act to minimise the reconstruction error and maximise the reconstruction probability.\n",
    "\n",
    "Consequently, it seems reasonable to suppose that we might equally modify one of these existing gradient schemes to allow for dependencies between the input units. Note, however, that whereas previously we computed the component\n",
    "$\\bar{x}_i(\\bar{\\bf y}_{\\bf x})$ from the vector \n",
    "$\\bar{\\bf x}(\\bar{\\bf y}_{\\bf x})$, \n",
    "we now need to reverse this procedure and instead compute the vector by stacking the components incrementally. \n",
    "\\begin{eqnarray}\n",
    "%To avoid having to redefine our existing functions, \n",
    "%we instead introduce the sequential function\n",
    "%\\tilde{\\bf x}({\\bf x}) & \\doteq & [\\bar{x}_i(\\bar{\\bf y}^{(i)}_{\\bf x})]_{i=1}^{F}\\,.\n",
    "\\end{eqnarray}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
