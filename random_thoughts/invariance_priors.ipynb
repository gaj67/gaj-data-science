{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0da7c3",
   "metadata": {},
   "source": [
    "# Invariance Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242c66a",
   "metadata": {},
   "source": [
    "This document provides a summary of the idea of obtaining a prior probability density function via the specification of invariance properties. The approach was championed by\n",
    "Jaynes, which he called *transformation group invariance* [[1]](#References). \n",
    "\n",
    "Jaynes' basic proposition was that if there is a continous transformation between the viewpoints of two observers, and further that if knowledge of the transformation does not make the prior knowledge of the observers differ, then they must both, for consistency, assign *exactly* the same prior distribution. \n",
    "\n",
    "However, Jaynes himself offered an example that was an exception to this rule, namely\n",
    "the case of a contraction mapping from a circle to a smaller, concentric circle. In this case, Jaynes rightly states that the distribution conditional on the inner circle is not equal to, but instead *proportional* to, the distribution conditional on the outer circle, with the constant of proportionality being the ratio of the factors needed to normalise the respective proper distributions.\n",
    "\n",
    "In fact, the need for proportionality was also demonstrated by\n",
    "Milne [[2]](#References) for the improper distributions resulting from nonlinear scaling\n",
    "(as discussed in this document in detail in the applied section on\n",
    " [nonlinear scale invariance](#Nonlinear-scale-invariance)\n",
    ").\n",
    "\n",
    "Consequently, in this document we derive the general invariance relation, including proportionality, from first principles. We should note, however, that all of the examples for which Jaynes did *not* include proportionality actually result in a proportionality constant of unity, and hence remain correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdf160",
   "metadata": {},
   "source": [
    "## Conservation of probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d18ec",
   "metadata": {},
   "source": [
    "We start with a continuous space $\\mathcal{X}\\subseteq\\mathbb{R}^n$, with some as-yet undefined probability distribution $f:\\mathcal{X}\\mapsto\\mathbb{R}^{+}$\n",
    "that satisfies $\\int_\\mathcal{X}f(\\mathbf{x})\\,|d\\mathbf{x}|=1$, where\n",
    "$|d\\mathbf{x}|\\doteq dx_1dx_2\\ldots dx_n$ is taken to be some infinitesimal volume element surrounding each point $\\mathbf{x}\\in\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49417e",
   "metadata": {},
   "source": [
    "Next, we consider a continuous mapping $\\mathbf{h}:\\mathcal{X}\\mapsto\\mathcal{Y}$ into some transformed space $\\mathcal{Y}\\subseteq\\mathbb{R}^{m}$. This mapping induces another probability distribution $g:\\mathcal{Y}\\mapsto\\mathbb{R}^{+}$ that obeys\n",
    "$\\int_\\mathcal{Y}g(\\mathbf{y})\\,|d\\mathbf{y}|=1$, where $|d\\mathbf{y}|\\doteq|dy_1dy_2\\ldots dy_m|$ is an infinitesimal\n",
    "volume element surrounding the point $\\mathbf{y}=\\mathbf{h}(\\mathbf{x})\\in\\mathcal{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30b8d3",
   "metadata": {},
   "source": [
    "The first invariance is that of conservation of probability mass. Since the transformation $\\mathbf{h}$ is continuous, \n",
    "we suppose that the volume element $|d\\mathbf{x}|$ is continuously transformed into the volume element $|d\\mathbf{y}|$. \n",
    "Hence, the probability mass of \n",
    "the untransformed element must equal that of the transformed element, giving rise to\n",
    "the invariance\n",
    "\\begin{eqnarray}\n",
    "f(\\mathbf{x})\\,|d\\mathbf{x}| & = & g(\\mathbf{y})\\,|d\\mathbf{y}|\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "For common dimensionality $m=n$, we have\n",
    "$|d\\mathbf{y}|=|J|\\,|d\\mathbf{x}|$, with Jacobian \n",
    "$J(\\mathbf{x})\\doteq\\mathtt{det}\\left(\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{x}}\\right)$.\n",
    "The invariance therefore becomes\n",
    "\\begin{eqnarray}\n",
    "f(\\mathbf{x}) & = & \\left|J(\\mathbf{x})\\right|\\,g(\\mathbf{y})\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec932ae2",
   "metadata": {},
   "source": [
    "## Distributional invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f74b96",
   "metadata": {},
   "source": [
    "The next invariance is the key to the entire method. \n",
    "Consider \n",
    "two observers, X and Y, who both observe some event, but who take measurements\n",
    "$\\mathbf{x}\\in\\mathcal{X}$ and $\\mathbf{y}\\in\\mathcal{Y}$ of the event, respectively.\n",
    "We suppose that before the event both observers had exactly the same background knowledge.\n",
    "The invariance is to suppose that after the event, both observers still have the same knowledge, since they observed the same event. Clearly, we are excluding imperfect measurements where observation was partially obscured for either observer.\n",
    "\n",
    "As noted in the intoduction, Jaynes [[1]](#References) opined that if the \n",
    "knowledge of the two observers remains the same, then the observations are invariant to nature of the transformation, and furthermore observers X and Y must (for the sake of consistency) assign the same prior distribution to the event. Hence, Jaynes supposes that $g=f$.\n",
    "\n",
    "Milne [[2]](#References) disagreed, and demonstrated that although this invariance works for the linear scaling $y=\\alpha x$, it fails to work\n",
    "for the nonlinear scaling $y=\\alpha x^\\beta$. Hence, Milne proposed that the invariance instead holds up to a constant factor, and that, in general, $g=\\gamma f$ (although Milne explicitly stated it as $f=\\gamma g$, his further working out in fact only makes sense if $g=\\gamma f$ instead).\n",
    "\n",
    "In fact, as again noted in the introduction, Jaynes himself saw the occasional need for $g=\\gamma f$, as explicitly used in his solution [[3]](#References) to the Bertrand paradox \n",
    "(or supposed 'solution', since\n",
    "Drory [[4]](#References) counters Jaynes' claim to have a unique solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685037cd",
   "metadata": {},
   "source": [
    "To consolidate these two viewpoints, we may suppose that invariance means here that the transformation does not alter the shape of the probability distribution, but only moves points along it, and possibly changes its constant of normalisation. Hence, we take\n",
    "\\begin{eqnarray}\n",
    "g(\\mathbf{y}) & = & \\gamma f(\\mathbf{y})\\,,\n",
    "\\end{eqnarray}\n",
    "which now presupposes that $\\mathcal{Y}\\subseteq\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80d21f",
   "metadata": {},
   "source": [
    "It now follows that if $f$ and $g$ are *proper* distributions, such that they can be normalised, then $\\gamma$ is\n",
    "determined uniquely by\n",
    "\\begin{eqnarray}\n",
    "\\gamma & = & \n",
    "\\frac{\\int_\\mathcal{Y}g(\\mathbf{y})\\,|d\\mathbf{y}|}\n",
    "{\\int_\\mathcal{Y}f(\\mathbf{y})\\,|d\\mathbf{y}|}\n",
    "~=~\\frac{1}{\\int_\\mathcal{Y}f(\\mathbf{x})\\,|d\\mathbf{x}|}\\,.\n",
    "\\end{eqnarray}\n",
    "Clearly, we must have $\\gamma=1$ for $\\mathcal{Y}=\\mathcal{X}$, and\n",
    "$\\gamma>1$ for $\\mathcal{Y}\\subset\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d875f0",
   "metadata": {},
   "source": [
    "However, if the distributions are *improper*, then $\\gamma>0$ must be determined\n",
    "from the above invariance relation, once the forms of $f$ and $g$ have been found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9a6da",
   "metadata": {},
   "source": [
    "## Parameter invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298109f8",
   "metadata": {},
   "source": [
    "The third invariance follows from the second. If the distribution is invariant to the transformation (up to a normalising constant), then it is invariant to the specific values of the transformation parameters (or at least a subset of the parameters, as we shall see in a [later](#Nonlinear-scale-invariance) section). Let the mapping $\\mathbf{h}$ now explicitly be a function of parameters $\\mathbf{\\theta}$, such that \n",
    "$\\mathbf{y}=\\mathbf{h}(\\mathbf{x};\\mathbf{\\theta})$, and likewise\n",
    "$J(\\mathbf{x};\\theta)=\\mathtt{det}\\left(\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{x}}\\right)$.\n",
    "Then the combination of conservation of probability and distributional invariance leads to the unified \n",
    "invariance relation\n",
    "\\begin{eqnarray}\n",
    "f(\\mathbf{x}) & = & \n",
    "\\gamma\\,|J(\\mathbf{x};\\mathbf{\\theta})|\\,f(\\mathbf{h}(\\mathbf{x};\\mathbf{\\theta}))\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb7dd1",
   "metadata": {},
   "source": [
    "This relation is now invariant to changes in the parameter values, and so we may take derivatives with respect to the parameters. This gives rise to the parameter invariance\n",
    "relation\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial |J|}{\\partial\\theta}(\\mathbf{x};\\theta)\\,f(\\mathbf{h}(\\mathbf{x};\\theta))\n",
    "+\\left|J(\\mathbf{x};\\theta)\\right|\\,f'(\\mathbf{h}(\\mathbf{x};\\theta))\n",
    "\\,\\frac{\\partial\\mathbf{h}}{\\partial\\theta}(\\mathbf{x};\\theta) & = & \\mathbf{0}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac3cfb",
   "metadata": {},
   "source": [
    "Furthermore, if these derivatives are invariant to the values of the parameters, then the relations hold for all parameter values, including the special value $\\theta_0$ which leads to the identity mapping $\\mathbf{h}(\\mathbf{x};\\theta_0)=\\mathbf{x}$.\n",
    "Consequently, we may simplify the derivative invariances at $\\theta=\\theta_0$, and then solve the simplified equations for $f(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87dece",
   "metadata": {},
   "source": [
    "# Examples of Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b81ee9",
   "metadata": {},
   "source": [
    "## Nonlinear scale invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca95f50",
   "metadata": {},
   "source": [
    "We now examine the nonlinear scale invariance discussed by \n",
    "Milne [[2]](#References), namely \n",
    "$y=h(x;\\alpha,\\beta)=\\alpha x^\\beta$ over the domain(s) \n",
    "$\\mathcal{Y}=\\mathcal{X}=(0,\\infty)$ for $\\alpha>0$ and $\\beta\\ne 0$.\n",
    "We observe that \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial h}{\\partial\\alpha}~=~x^\\beta\\,, &&\n",
    "\\frac{\\partial h}{\\partial\\beta}~=~\\alpha x^\\beta \\ln x\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "|J| & = & \\left|\\frac{\\partial h}{\\partial x}\\right|\n",
    "~=~\\alpha|\\beta| x^{\\beta-1}~=~\\frac{|\\beta| h}{x}\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial |J|}{\\partial\\alpha} & = & |\\beta| x^{\\beta-1}\\,,\n",
    "\\;\\;\\;\n",
    " \\frac{\\partial |J|}{\\partial\\beta}~=~\\alpha x^{\\beta-1}\n",
    " (\\mathtt{sign}(\\beta)+|\\beta|\\ln x)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140438f",
   "metadata": {},
   "source": [
    "The derivatives of the unified invariance relation are therefore\n",
    "\\begin{eqnarray}\n",
    "|\\beta| x^{\\beta-1}\\,f(\\alpha x^\\beta)+\\alpha|\\beta| x^{\\beta-1}\\,\n",
    "f'(\\alpha x^\\beta)\\,x^\\beta & = & 0\\,,\n",
    "\\\\\n",
    "\\alpha x^{\\beta-1}(\\mathtt{sign}(\\beta)+|\\beta|\\ln x)\\,f(\\alpha x^\\beta)+\\alpha|\\beta| x^{\\beta-1}\\,\n",
    "f'(\\alpha x^\\beta)\\,\\alpha x^\\beta \\ln x & = & 0\\,.\n",
    "\\end{eqnarray}\n",
    "We note that the nonlinear scaling becomes the identity mapping for $\\alpha=1$ and $\\beta=1$. Hence, at these values, the derivatives reduce to\n",
    "\\begin{eqnarray}\n",
    "f(x)+xf'(x) & = & 0\\,,\n",
    "\\\\\n",
    "(1+\\ln x)\\,f(x)+x\\ln x\\,f'(x) & = & 0\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad109e",
   "metadata": {},
   "source": [
    "The $\\alpha$-derivative is clearly satisfied for $f(x)=\\frac{k}{x}$ with arbitrary constant $k>0$. However, the $\\beta$-derivative is then only satisfied for $k=0$.\n",
    "Consequently, we see that the nonlinear scaling is invariant to $\\alpha>0$, but is **not** invariant to $\\beta$, which must therefore be held constant to a value fixed in advance. In fact, from the unified invariance relation, we have\n",
    "\\begin{eqnarray}\n",
    "f(x) & = & \\gamma\\alpha|\\beta| x^{\\beta-1}\\,f(\\alpha x^\\beta)\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\frac{k}{x} & = &\n",
    " \\gamma\\alpha|\\beta| x^{\\beta-1}\\frac{k}{\\alpha x^\\beta}\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\gamma & = & \\frac{1}{|\\beta|}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa838501",
   "metadata": {},
   "source": [
    "This dependence of the renormalisation factor $\\gamma$ on $\\beta$ reinforces the fact that for each fixed value of $\\beta$ we obtain a family of transformations that are distributionally invariant to the scaling factor $\\alpha$.\n",
    "\n",
    "We see that taking $\\beta=1$ results in the linear scaling of Jaynes [[1]](#References),\n",
    "for which $\\gamma=1$.\n",
    "Also note that $\\gamma\\ne 1$ for $\\beta\\ne \\pm 1$, despite the fact that \n",
    "$\\mathcal{Y}=\\mathcal{X}$. This is due, as discussed in the section on\n",
    "[distributional invariance](#Distributional-invariance), \n",
    "to the fact that $f$ is an *improper* distribution that cannot actually be properly normalised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1aa73a",
   "metadata": {},
   "source": [
    "## Von Kries paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b1c4e",
   "metadata": {},
   "source": [
    "We now turn to the resolution of the Von Kries paradox, as also discussed (briefly) by \n",
    "Milne [[2]](#References).\n",
    "\n",
    "Suppose we know the specific density $\\rho$ (my notation) of a fluid is restricted to the range $\\rho\\in[a,b]$ for some $0<a<b<\\infty$. If we know nothing else, then the principle of insufficient reason suggests all values are equally likely, and so we might take the uniform prior $f(\\rho)=\\frac{1}{b-a}$.\n",
    "\n",
    "However, the specific volume $\\nu$ (again, my notation) is inversely related to the specific density via $\\nu=\\frac{1}{\\rho}$, giving rise to the Jacobian\n",
    "$J(\\rho)=\\frac{d\\nu}{d\\rho}=-\\frac{1}{\\rho^2}$. The corresponding prior $g(\\nu)$ therefore satisfies the conservation of probability relation\n",
    "\\begin{eqnarray}\n",
    "f(\\rho)~=~|J(\\rho)|\\,g(\\nu) & \\Rightarrow & \n",
    "g(\\nu)~=~\\frac{1}{\\nu^2}f\\left(\\frac{1}{\\nu}\\right)\n",
    "~=~\\frac{1}{(b-a)\\nu^2}\\,.\n",
    "\\end{eqnarray}\n",
    "The paradox is therefore that the specific volume is deterministically known from the specific density, but uniform ignorance of the specific density does not translate to uniform ignorance of the specific volume.  In other words, we seem to have acquired some knowledge of the specific volume for free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6acf1e",
   "metadata": {},
   "source": [
    "The resolution of the paradox, of course, is that we must already have added this knowledge by the *unwarranted* assumption of a uniform prior for the specific density.\n",
    "\n",
    "Instead we take ignorance to mean \n",
    "[distributional invariance](#Distributional-invariance). From the\n",
    "[previous](#Nonlinear-scale-invariance) section, we see that \n",
    "$\\nu=\\frac{1}{\\rho}$ is a specific form of the nonlinear transformation\n",
    "$\\nu=\\alpha\\rho^\\beta$ for $\\alpha=1$ and $\\beta=-1$. \n",
    "Consequently, we already know that $\\gamma=\\frac{1}{|\\beta|}=1$, such that the two prior\n",
    "distributions are given by\n",
    "\\begin{eqnarray}\n",
    "f(\\rho)~=~\\frac{k}{\\rho}\\,, && \n",
    "g(\\nu)~=~\\frac{1}{\\nu^2}f\\left(\\frac{1}{\\nu}\\right)~=~\\frac{k}{\\nu}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Since we are assuming the finite domain $\\rho\\in[a,b]$, we easily obtain\n",
    "the normalisation constant as $k=\\frac{1}{\\ln b-\\ln a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7d743",
   "metadata": {},
   "source": [
    "We observe that now ignorance of the specific density gives rise to the same form as ignorance of the specific volume, which resolves the paradox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034005b",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979e601",
   "metadata": {},
   "source": [
    "Let us consider (briefly) the problem of fitting the straight-line model \n",
    "$y=\\alpha+\\beta x$, for $x,y\\in\\mathbb{R}$. Typically, in practice, we observe pairs of values $(x, y)$, and from these data $D$ we wish to infer the posterior distribution $p(\\alpha,\\beta\\mid D)$.\n",
    "However, here we are not concerned with the data but with the prior distributions, $p(\\alpha)$ and $p(\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850be07",
   "metadata": {},
   "source": [
    "Firstly, we observe that $\\alpha\\in\\mathbb{R}$ uniquely specifies the intersection of the model line with the $y$-axis, and hence $\\alpha$ is a location parameter.\n",
    "Consequently, if we know nothing else about $\\alpha$, then\n",
    "we might suppose that the prior distribution, $p(\\alpha)$, is invariant to the translation \n",
    "$\\alpha'=\\alpha+\\nu$. From the section on [parameter invariance](#Parameter-invariance),\n",
    "we therefore obtain the invariance relation\n",
    "\\begin{eqnarray}\n",
    "p(\\alpha) & = & \\gamma\\,p(\\alpha+\\nu)\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "As usual, we take the derivative with respect to the transformation parameter $\\nu$, and then evaluate the result at the point $\\nu=0$, for which the transformation becomes an identity. This gives\n",
    "\\begin{eqnarray}\n",
    "0~=~\\gamma\\,p'(\\alpha) & \\Rightarrow & p(\\alpha)~=~k\\,,\\;\\;\\gamma~=~1\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, $\\alpha$ has an improper, uniform invariance prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8fde44",
   "metadata": {},
   "source": [
    "Next, we observe that the slope $\\beta\\in\\mathbb{R}$ is the tangent of the angle\n",
    "$\\theta\\in(-\\frac{\\pi}{2},\\frac{\\pi}{2})$ that the model line is rotated\n",
    "counter-clockwise from the $x$-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94309de",
   "metadata": {},
   "source": [
    "Some trigonometry gives us, for example:\n",
    "\\begin{eqnarray}\n",
    "\\beta & = & \\tan\\theta ~=~ \\frac{\\sin\\theta}{\\cos\\theta}\n",
    "~=~\\frac{\\mathtt{sign}(\\theta)\\,\\sqrt{1-\\cos^2\\theta}}{\\cos\\theta}\n",
    "\\\\\n",
    "\\Rightarrow \\cos\\theta & = & \\frac{1}{\\sqrt{1+\\beta^2}}\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we see that\n",
    "\\begin{eqnarray}\n",
    "\\frac{d\\tan\\theta}{d\\theta} & = & \n",
    "\\frac{d}{d\\theta}\\left(\\frac{\\sin\\theta}{\\cos\\theta}\\right)\n",
    "~=~\\frac{\\cos\\theta}{\\cos\\theta}+\\frac{\\sin^2\\theta}{\\cos^2\\theta}~=~\n",
    "1+\\tan^2\\theta\n",
    "\\\\\n",
    "& = & \\frac{1}{\\cos^2\\theta}~=~\\sec^2\\theta\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\frac{d\\beta}{d\\theta} & = & 1+\\beta^2\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a7167",
   "metadata": {},
   "source": [
    "If we are ignorant about any further properties of $\\beta$, then we may suppose that the\n",
    "prior distribution, $p(\\beta)$, is invariant to the rotational transformation\n",
    "$\\beta'=\\tan(\\theta+\\psi)$. The unified invariance relation is then\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(\\beta)\\,\\left|\\frac{d\\beta}{d\\theta}\\right| & = &\n",
    "\\gamma\\,p(\\beta')\\,\\left|\\frac{\\partial\\beta'}{\\partial\\theta}\\right|\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\beta'}{\\partial\\theta} & = &\n",
    "\\frac{\\partial\\beta'}{\\partial\\psi}~=~1+\\beta'^2\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the completed relation is\n",
    "\\begin{eqnarray}\n",
    "(1+\\beta^2)\\,p(\\beta) & = &\n",
    "\\gamma\\,(1+\\beta'^2)\\,p(\\beta')\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f4d38",
   "metadata": {},
   "source": [
    "Once again, we take the derivative with respect to the transformation parameter $\\psi$. This gives\n",
    "\\begin{eqnarray}\n",
    "0 & = & \\gamma\\,\\left[\n",
    "2\\beta'(1+\\beta'^2)\\,p(\\beta')+(1+\\beta'^2)^2\\,p'(\\beta')\n",
    "\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we evaluate the derivative at the point $\\psi=0$ at which the transformation becomes the identity. With simplification, this gives\n",
    "\\begin{eqnarray}\n",
    "2\\beta\\,p(\\beta)+(1+\\beta^2)\\,p'(\\beta) & = & 0\\,,\n",
    "\\\\\n",
    "\\Rightarrow \n",
    "\\frac{p'(\\beta)}{p(\\beta)} & = & -\\frac{2\\beta}{1+\\beta^2}\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\ln p(\\beta) & = & \\ln k-\\ln(1+\\beta^2)\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "p(\\beta) & = & \\frac{k}{1+\\beta^2}\\,.\n",
    "\\end{eqnarray}\n",
    "Substitution back into the unified invariance relation then gives $\\gamma=1$.\n",
    "Furthermore, we recognise that $p(\\beta)$ is just the Cauchy distribution, which properly normalises with $k=\\frac{1}{\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd0f92",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a0c66",
   "metadata": {},
   "source": [
    "[1] E.T. Jaynes (1964): \"*Prior probabilities and transformation groups*\"\n",
    "([pdf](https://bayes.wustl.edu/etj/articles/groups.pdf))\n",
    "\n",
    "[2] P. Milne (1983): \"*A note on scale invariance*\"\n",
    "([ref](https://www.jstor.org/stable/686933))\n",
    "\n",
    "[3] E.T. Jaynes (1973): \"*The well-posed problem*\"\n",
    "([pdf](https://bayes.wustl.edu/etj/articles/well.pdf))\n",
    "\n",
    "[4] A. Drory (2015): \"*Failure and uses of Jaynesâ€™ principle of transformation groups*\"\n",
    "([ref](https://www.researchgate.net/publication/273477459_Failure_and_Uses_of_Jaynes%27_Principle_of_Transformation_Groups) to pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
