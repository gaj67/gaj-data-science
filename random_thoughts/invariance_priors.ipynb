{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0becd350",
   "metadata": {},
   "source": [
    "Author: G. Jarrad\n",
    "\n",
    "This document describes and summarises various methodologies for obtaining continuous probability distributions based on specified invariance properties. Typically, such an invariance distribution might be used as an *ignorance* prior, when little is known in advance about the distributional properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38e817",
   "metadata": {},
   "source": [
    "# Transformation Group Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242c66a",
   "metadata": {},
   "source": [
    "The approach of *transformation group invariance* was championed by\n",
    "Jaynes [[1]](#References \"Reference: Prior probabilities and transformation groups\"). \n",
    "Jaynes' basic proposition was that if there is a continous transformation between the viewpoints of two observers, and further that if knowledge of the transformation does not make the prior knowledge of the observers differ, then they must both, for consistency, assign *exactly* the same prior distribution. \n",
    "\n",
    "However, Jaynes himself offered an example that was an exception to this rule, namely\n",
    "the case of a contraction mapping from a circle to a smaller, concentric circle. In this case, Jaynes rightly states that the distribution conditional on the inner circle is not equal to, but instead *proportional* to, the distribution conditional on the outer circle, with the constant of proportionality being the ratio of the factors needed to normalise the respective proper distributions.\n",
    "\n",
    "In fact, the need for proportionality was also demonstrated by\n",
    "Milne [[2]](#References \"Reference: A note on scale invariance\") \n",
    "for improper distributions resulting from nonlinear transformation\n",
    "(as discussed later in the \n",
    "[example](#Nonlinear-rescaling-plus-relocation \"Section: Nonlinear rescaling plus relocation\") of nonlinear scale invariance).\n",
    "\n",
    "Consequently, in this chapter we derive the general invariance relations, including proportionality, from first principles. We should note, however, that all of the examples for which Jaynes did *not* include proportionality actually result in a proportionality constant of unity, and hence remain correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2840d",
   "metadata": {},
   "source": [
    "## Transformation geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786f2b5",
   "metadata": {},
   "source": [
    "We begin with two continuous spaces, $\\mathcal{U}\\subseteq\\mathbb{R}^n$\n",
    "and $\\mathcal{V}\\subseteq\\mathbb{R}^m$, and a continuous transformation,\n",
    "$\\mathbf{h}: \\mathcal{U}\\mapsto\\mathcal{V}$,\n",
    "such that the point $\\mathbf{u}=(u_1,\\ldots,u_n)\\in\\mathcal{U}$ is mapped to its counterpart\n",
    "$\\mathbf{v}=(v_1,\\ldots,v_m)\\in\\mathcal{V}$ via $\\mathbf{v}=\\mathbf{h}(\\mathbf{u})$.\n",
    "\n",
    "We further suppose that $\\mathbf{h}$ is differentiable, such that an infinitesimal \n",
    "displacement from $\\mathbf{u}$, denoted by $d\\mathbf{u}\\doteq(du_1,\\ldots,du_n)$, is mapped to its counterpart displacement\n",
    "$d\\mathbf{v}\\doteq(dv_1,\\ldots,dv_n)$ from $\\mathbf{v}$ via\n",
    "\\begin{eqnarray}\n",
    "d\\mathbf{v} & = & \\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{u}}(\\mathbf{u})\\,d\\mathbf{u}\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Similarly, we suppose that the infinitesimal volume element \n",
    "$dU=|d\\mathbf{u}|\\doteq du_1\\cdots du_n$ about $\\mathbf{u}$ is mapped to its \n",
    "counterpart volume element\n",
    "$dV=|d\\mathbf{v}|\\doteq dv_1\\cdots dv_m$ about $\\mathbf{v}$. \n",
    "At this juncture, we further require\n",
    "$\\mathbf{h}$ to be invertible, such that no information is lost in the transformation. Consequently, spaces $\\mathcal{U}$ and $\\mathcal{V}$ must share common dimensionality,\n",
    "i.e. $n=m$. By geometrical reasoning, the two volume elements are now related via\n",
    "\\begin{eqnarray}\n",
    "|d\\mathbf{v}|~=~\\left|J(\\mathbf{u})\\right|\\,|d\\mathbf{u}|\\,,\n",
    "&\\;\\;\\;\\;&\n",
    "J~\\doteq~\n",
    "\\mathtt{det}\\left[\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{u}}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $J$ is known as the *Jacobian* of the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdf160",
   "metadata": {},
   "source": [
    "## Conservation of probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49417e",
   "metadata": {},
   "source": [
    "In addition to the transformation geometry of the previous\n",
    "[section](#Transformation-geometry \"Section: Transformation geometry\"), we now suppose\n",
    "that spaces $\\mathcal{U}$ and $\\mathcal{V}$ have associated (but as yet unspecified) probability distributions denoted by $p:\\mathcal{U}\\mapsto\\mathbb{R}^{+}$ and\n",
    "$q:\\mathcal{V}\\mapsto\\mathbb{R}^{+}$, respectively. \n",
    "We initially assume that $p$ and $q$ are *proper* distributions, such that they obey\n",
    "$\\int_\\mathcal{U}p(\\mathbf{u})\\,|d\\mathbf{u}|=1$ and\n",
    "$\\int_\\mathcal{V}q(\\mathbf{v})\\,|d\\mathbf{v}|=1$, respectively. \n",
    "It then follows that however we partition $\\mathcal{U}$ and $\\mathcal{V}$, the total probability mass must always be unity.\n",
    "This is the key to the first invariance principle of conservation of probability mass.\n",
    "\n",
    "Note, however, that we shall later relax this constraint (without justification), allowing $p$ and $q$\n",
    "to be *improper* distributions that do not normalise to unity (in fact, their normalisation factors will be infinite)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf0ccc",
   "metadata": {},
   "source": [
    "Suppose we notionally partition $\\mathcal{U}$ into a finite number $N$ of finite volume elements,\n",
    "$(\\delta U_1,\\ldots,\\delta U_N)$, such that $\\mathcal{U}=\\bigcup_{i=1}^{N}\\delta U_i$.\n",
    "Then the total probability mass must remain constant:\n",
    "\\begin{eqnarray}\n",
    "\\int_\\mathcal{U}p(\\mathbf{u})\\,|d\\mathbf{u}| & = &\n",
    "\\sum_{i=1}^{N}\\int_{\\delta U_i}p(\\mathbf{u})\\,|d\\mathbf{u}|~=~1\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09beb4d",
   "metadata": {},
   "source": [
    "Next, we apply transformation $\\mathbf{h}$ to continuously map each volume element\n",
    "$\\delta U_i$ into its counterpart $\\delta V_i$. Since we assumed that $\\mathbf{h}$ is invertible, then the volume elements in $\\mathcal{V}$ cannot overlap. Additionally, since $\\mathcal{U}$ is mapped entirely into $\\mathcal{V}$, there cannot be any space left over. Consequently, we must have partitioned $\\mathcal{V}$ as $\\mathcal{V}=\\bigcup_{i=1}^{N}\\delta V_i$, such that\n",
    "\\begin{eqnarray}\n",
    "\\int_\\mathcal{V}q(\\mathbf{v})\\,|d\\mathbf{v}| & = &\n",
    "\\sum_{i=1}^{N}\\int_{\\delta V_i}q(\\mathbf{v})\\,|d\\mathbf{v}|~=~1\\,.\n",
    "\\end{eqnarray}\n",
    "It therefore follows that\n",
    "\\begin{eqnarray}\n",
    "\\sum_{i=1}^{N}\\left(\n",
    "\\int_{\\delta U_i}p(\\mathbf{u})\\,|d\\mathbf{u}|-\n",
    "\\int_{\\delta V_i}q(\\mathbf{v})\\,|d\\mathbf{v}|\n",
    "\\right)\n",
    "& = & 0\\,.\n",
    "\\end{eqnarray}\n",
    "Now, since this relation holds for arbitrary $N$, it must hold for all $N$, such that\n",
    "\\begin{eqnarray}\n",
    "\\int_{\\delta U}p(\\mathbf{u})\\,|d\\mathbf{u}| & = &\n",
    "\\int_{\\delta V}q(\\mathbf{v})\\,|d\\mathbf{v}|\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In particular, in the limit as $N\\rightarrow\\infty$ we have $\\delta U\\rightarrow dU$\n",
    "and $\\delta V\\rightarrow dV$, such that the relation\n",
    "reduces to\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{u})\\,|d\\mathbf{u}| & = & q(\\mathbf{v})\\,|d\\mathbf{v}|\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30b8d3",
   "metadata": {},
   "source": [
    "From the transformation geometry of the previous \n",
    "[section](#Transformation-geometry \"Section: Transformation geometry\"), \n",
    "we therefore obtain the (standard) probability conservation relation\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{u}) & = & \\left|J(\\mathbf{u})\\right|\\,q(\\mathbf{\\mathbf{h}(\\mathbf{u})})\\,,\n",
    "\\end{eqnarray}\n",
    "for all $\\mathbf{u}\\in\\mathcal{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec932ae2",
   "metadata": {},
   "source": [
    "## Distributional invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f74b96",
   "metadata": {},
   "source": [
    "The second invariance is the key to the entire method of transformation groups. \n",
    "Consider \n",
    "two observers, X and Y, who both observe some event, but who take measurements\n",
    "$\\mathbf{u}\\in\\mathcal{U}$ and $\\mathbf{v}\\in\\mathcal{V}$ of the event, respectively.\n",
    "We suppose that before the event both observers had exactly the same background knowledge.\n",
    "The invariance is to suppose that after the event, both observers still have the same knowledge, since they observed the same event. Clearly, we are excluding imperfect measurements where observation was partially obscured for either observer.\n",
    "\n",
    "As noted in the intoduction, \n",
    "Jaynes [[1]](#References \"Reference: Prior probabilities and transformation groups\") \n",
    "opined that if the \n",
    "knowledge of the two observers remains the same, then the observations are invariant to nature of the transformation, and furthermore observers X and Y must (for the sake of consistency) assign the same prior distribution to the event. Hence, Jaynes supposes that $q=p$.\n",
    "\n",
    "Milne [[2]](#References \"Reference: A note on scale invariance\") \n",
    "disagreed, and demonstrated that although this invariance works for the linear scaling $y=\\alpha x$, it fails to work\n",
    "for the nonlinear scaling $y=\\alpha x^\\beta$. Hence, Milne proposed that the invariance instead holds up to a constant factor. \n",
    "[Aside: In fact, taking $p=f$ and $q=g$, \n",
    "Milne explicitly stated the constraint as $f=\\gamma g$, altough his further working out only makes sense if $g=\\gamma f$ instead.]\n",
    "\n",
    "Indeed, as noted in the introduction, Jaynes himself saw the occasional need for $q=\\gamma p$, as explicitly used in his solution \n",
    "[[3]](#References \"Reference: The well-posed problem\") \n",
    "to the Bertrand paradox \n",
    "(or supposed 'solution', since\n",
    "Drory \n",
    "[[4]](#References \"Reference: Failure and uses of Jaynes’ principle of transformation groups\") \n",
    "counters Jaynes' claim to have a unique solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685037cd",
   "metadata": {},
   "source": [
    "To consolidate these two viewpoints, we may suppose that invariance means here that the transformation does not alter the shape of the probability distribution, but only moves points along it, and possibly changes its constant of normalisation. Hence, we define\n",
    "*distributional invariance* as\n",
    "\\begin{eqnarray}\n",
    "q(\\mathbf{v}) & = & \\gamma p(\\mathbf{v})\\,,\n",
    "\\end{eqnarray}\n",
    "for all $\\mathbf{v}\\in\\mathcal{V}$, and for some constant $\\gamma>0$.\n",
    "Note that this now presupposes that $\\mathcal{V}\\subseteq\\mathcal{U}$,\n",
    "since $p:\\mathcal{U}\\mapsto\\mathbb{R}^+$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80d21f",
   "metadata": {},
   "source": [
    "It now follows that if $p$ and $q$ are *proper* distributions, \n",
    "as assumed in the previous \n",
    "[section](#Conservation-of-probability \"Section: Conservation of probability\"),\n",
    "then $\\gamma$ is determined uniquely by\n",
    "\\begin{eqnarray}\n",
    "\\gamma & = & \n",
    "\\frac{\\int_\\mathcal{V}q(\\mathbf{v})\\,|d\\mathbf{v}|}\n",
    "{\\int_\\mathcal{V}p(\\mathbf{v})\\,|d\\mathbf{v}|}\n",
    "~=~\\frac{1}{\\int_\\mathcal{V}p(\\mathbf{u})\\,|d\\mathbf{u}|}\\,.\n",
    "\\end{eqnarray}\n",
    "Clearly, we must then have $\\gamma=1$ for $\\mathcal{V}=\\mathcal{U}$, and\n",
    "$\\gamma>1$ for $\\mathcal{V}\\subset\\mathcal{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d875f0",
   "metadata": {},
   "source": [
    "However, if the distributions are *improper*, then $\\gamma$ must be determined\n",
    "from the above invariance relation, once the forms of $p$ and $q$ have been found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9a6da",
   "metadata": {},
   "source": [
    "## Unconditional probability invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298109f8",
   "metadata": {},
   "source": [
    "The third invariance follows from the distributional invariance of the previous\n",
    "[section](#Distributional-invariance \"section: Distributional invariance\"). \n",
    "Stated briefly, if the distribution is invariant to the transformation (up to a normalising constant), then it is invariant to the specific values of the transformation parameters (or at least a subset of the parameters, as we shall see in a later \n",
    "[example](#Nonlinear-rescaling-plus-relocation \"Section: Nonlinear rescaling plus relocation\")). \n",
    "\n",
    "Let the transformation $\\mathbf{h}$ now be redefined explicitly as a function,\n",
    "$\\mathbf{h}:\\mathcal{U}\\times\\Psi\\mapsto\\mathcal{V}$,\n",
    "of parameters $\\mathbf{\\psi}\\in\\Psi$, such that \n",
    "$\\mathbf{v}=\\mathbf{h}(\\mathbf{u};\\mathbf{\\psi})$, and likewise\n",
    "$J(\\mathbf{u};\\psi)=\\mathtt{det}\\left(\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{u}}\\right)$.\n",
    "Then the combination of \n",
    "[conservation of probability](#Conservation-of-probability \"Section: Conservation of probability\")\n",
    "and \n",
    "[distributional invariance](#Distributional-invariance \"Section: Distributional invariance\")\n",
    "requires that the (unconditional) probability distribution $p(\\mathbf{u})$ \n",
    "obeys the unified invariance relation\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{u}) & = & \n",
    "\\gamma\\,|J(\\mathbf{u};\\mathbf{\\psi})|\\,p(\\mathbf{h}(\\mathbf{u};\\mathbf{\\psi}))\\,,\n",
    "\\end{eqnarray}\n",
    "for every point $\\mathbf{u}\\in\\mathcal{U}$, and *typically* (see the caveat further below)\n",
    "for all parameter values $\\psi\\in\\Psi$. We shall call this the *unconditional (probability) invariance* relation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb7dd1",
   "metadata": {},
   "source": [
    "Since this relation is invariant to changes in the transformation parameter values $\\psi$, we may take derivatives with respect to $\\psi$, giving rise to the parameter invariance relation(s)\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial |J|}{\\partial\\psi}(\\mathbf{u};\\psi)\\, p(\\mathbf{h}(\\mathbf{u};\\psi))\n",
    "+\\left|J(\\mathbf{u};\\psi)\\right|\n",
    "\\, \\frac{\\partial\\mathbf{h}}{\\partial\\psi}(\\mathbf{u};\\psi)\n",
    "\\, p'(\\mathbf{h}(\\mathbf{u};\\psi))& = & \\mathbf{0}\\,.\n",
    "\\end{eqnarray}\n",
    "These invariances again hold for all $\\mathbf{u}\\in\\mathcal{U}$, and typically for all\n",
    "$\\psi\\in\\Psi$ (as per the caveat below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac3cfb",
   "metadata": {},
   "source": [
    "Furthermore, if these derivatives are invariant to the values of the parameters, then the relations hold for all parameter values, including the special value $\\psi_0$ which leads to the identity mapping $\\mathbf{h}(\\mathbf{u};\\psi_0)=\\mathbf{u}$.\n",
    "Consequently, we may simplify the derivative invariances at $\\psi=\\psi_0$, and then solve the simplified equations for $p(\\mathbf{u})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e6929",
   "metadata": {},
   "source": [
    "**Caveat**: Note that we stated above that the invariance relation in $\\mathbf{u}$ is also typically satisfied for all transformation parameters $\\psi$. However, in some \n",
    "circumstances, as we shall see in the \n",
    "[example](#Nonlinear-rescaling-plus-relocation \"Section: Nonlinear rescaling plus relocation\") \n",
    "on nonlinear rescaling,\n",
    "the invariance relation might be satisfied only for variation in some transformation parameters but not others. When this occurs, we have a *family* of invariance relations\n",
    "dependent on (the constant values of) the non-invariant parameters. This includes the normalisation factor $\\gamma$, which then more generally becomes $\\gamma(\\psi)$.\n",
    "Consequently, the more general invariance relation is in fact\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{u}) & = & \n",
    "\\gamma(\\psi)\\,|J(\\mathbf{u};\\mathbf{\\psi})|\\,p(\\mathbf{h}(\\mathbf{u};\\mathbf{\\psi}))\\,,\n",
    "\\end{eqnarray}\n",
    "for all $\\mathbf{u}\\in\\mathcal{U}$, and for all $\\psi\\in\\Psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a0976",
   "metadata": {},
   "source": [
    "## Conditional probability invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd41cd",
   "metadata": {},
   "source": [
    "So far in this [chapter](#Transformation-Group-Invariance \"Chapter: Transformation Group Invariance\"), we have considered only the unconditional probability distributions $p(\\mathbf{u})$ and $q(\\mathbf{v})$. As Jaynes repeatedly mentions, all distributions are conditional in the sense that they depend (at the minimum) upon prior information about the problem to be solved (including, but not limited to, the respective domains $\\mathcal{U}$ and\n",
    "$\\mathcal{V}$). Hence, by *unconditional* we mean not conditioned on any distributional parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd90dab",
   "metadata": {},
   "source": [
    "This now leads us to consider conditional, i.e., parameterised, distributions of the form \n",
    "$p(\\mathbf{u}\\mid\\theta)$ and $q(\\mathbf{v}\\mid\\phi)$. \n",
    "In addition, we also have unconditional, i.e. prior, distributions of the form\n",
    "$p(\\theta)$ and $q(\\phi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88703fa0",
   "metadata": {},
   "source": [
    "The prior distributions are handled by the unconditional invariance relation of the previous\n",
    "[section](#Unconditional-probability-invariance \"Section: Unconditional probability invariance\").\n",
    "In particular, we consider the transformation \n",
    "$\\mathbf{g}:\\Theta\\times\\Psi\\mapsto\\Phi$ that maps distributional parameters $\\theta\\in\\Theta$ to \n",
    "$\\phi=\\mathbf{g}(\\theta;\\psi)\\in\\Phi\\subseteq\\Theta$. \n",
    "The corresponding invariance relation is then\n",
    "\\begin{eqnarray}\n",
    "p(\\theta) ~=~ \\gamma_\\mathbf{g}\\,\n",
    "\\left|\n",
    "J_\\mathbf{g}(\\theta;\\psi)\\right|\n",
    "\\,\n",
    "p(\\mathbf{g}(\\theta;\\psi))\n",
    "\\,,\n",
    "&\\;\\;\\;\\;&\n",
    "J_\\mathbf{g}~\\doteq~\\mathtt{det}\\left[\\frac{\\partial\\mathbf{g}}{\\partial\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for all $\\theta\\in\\Theta$ and all $\\psi\\in\\Psi$, with constant $\\gamma_\\mathbf{g}>0$.\n",
    "Note for later use that the infinitesimal volumes $|d\\theta|$ and $|d\\phi|$ are related\n",
    "by $|d\\phi|~=~|J_\\mathbf{g}(\\theta;\\psi)|\\,|d\\theta|$,\n",
    "from the \n",
    "[transformation geometry](#Transformation-geometry \"Section: Transformation geometry\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2fa976",
   "metadata": {},
   "source": [
    "Next, we consider the joint distributions, namely\n",
    "$p(\\mathbf{u},\\theta)=p(\\mathbf{u}\\mid\\theta)\\,p(\\theta)$ and\n",
    "$q(\\mathbf{v},\\phi)=q(\\mathbf{v}\\mid\\phi)\\,q(\\phi)$, which in this form are also\n",
    "to be considered unconditional. To the existing transformation $\\mathbf{g}$, we thus\n",
    "add the additional transformation \n",
    "$\\mathbf{h}:\\mathcal{U}\\times\\Theta\\times\\Psi\\mapsto\\mathcal{V}$, which maps $\\mathbf{u}\\in\\mathcal{U}$ to \n",
    "$\\mathbf{v}=\\mathbf{h}(\\mathbf{u};\\theta,\\psi)\\in\\mathcal{V}\\subseteq\\mathcal{U}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c150c30",
   "metadata": {},
   "source": [
    "From the\n",
    "[transformation geometry](#Transformation-geometry \"Section: Transformation geometry\"), we deduce that the infinitesimal joint displacement $d(\\mathbf{u},\\theta)$ is related to the corresponding joint displacement \n",
    "$d(\\mathbf{v},\\phi)$ via\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{c}\n",
    "d\\mathbf{v}\n",
    "\\\\\n",
    "d\\phi\n",
    "\\end{array}\\right]\n",
    "& = &\n",
    "\\left[\\begin{array}{cc}\n",
    "\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{u}} &\n",
    "\\frac{\\partial\\mathbf{h}}{\\partial\\theta}\n",
    "\\\\\n",
    "\\mathbf{0} &\n",
    "\\frac{\\partial\\mathbf{g}}{\\partial\\theta}\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "d\\mathbf{u}\n",
    "\\\\\n",
    "d\\theta\n",
    "\\end{array}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "The Jacobian of the joint transformation is therefore\n",
    "\\begin{eqnarray}\n",
    "J_{\\mathbf{h},\\mathbf{g}} & \\doteq & \\mathtt{det}\n",
    "\\left[\\begin{array}{cc}\n",
    "\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{u}} &\n",
    "\\frac{\\partial\\mathbf{h}}{\\partial\\theta}\n",
    "\\\\\n",
    "\\mathbf{0} &\n",
    "\\frac{\\partial\\mathbf{g}}{\\partial\\theta}\n",
    "\\end{array}\\right]\n",
    "~=~\\mathtt{det}\\left[\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{u}}\\right]\\,\n",
    "\\mathtt{det}\\left[\\frac{\\partial\\mathbf{g}}{\\partial\\theta}\\right]\n",
    "\\,,\n",
    "\\\\\\Rightarrow\n",
    "J_{\\mathbf{h},\\mathbf{g}}(\\mathbf{u},\\theta;\\psi)\n",
    "& \\equiv & J_\\mathbf{h}(\\mathbf{u};\\theta,\\psi)\\,J_\\mathbf{g}(\\theta;\\psi)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd7b42",
   "metadata": {},
   "source": [
    "Consequently, the infinitesimal joint volumes $|d(\\mathbf{u},\\theta)|$ and \n",
    "$|d(\\mathbf{v},\\phi)|$ are related via\n",
    "\\begin{eqnarray}\n",
    "|d(\\mathbf{v},\\phi)| & = & \n",
    "|J_{\\mathbf{h},\\mathbf{g}}(\\mathbf{u},\\theta;\\psi)|\\,|d(\\mathbf{u},\\theta)|\n",
    "\\\\\\Rightarrow\n",
    "|d\\mathbf{v}|\\,|d\\phi|\n",
    "& = & |J_\\mathbf{h}(\\mathbf{u};\\theta,\\psi)|\\,|d\\mathbf{u}|\\,\n",
    "|J_\\mathbf{g}(\\theta;\\psi)|\\,|d\\theta|\\,,\n",
    "\\\\\\Rightarrow\n",
    "|d\\mathbf{v}| & = &  |J_\\mathbf{h}(\\mathbf{u};\\theta,\\psi)|\\,|d\\mathbf{u}|\\,,\n",
    "\\end{eqnarray}\n",
    "since we noted earlier that $|d\\phi|=|J_\\mathbf{g}(\\theta;\\psi)|\\,|d\\theta|$ for\n",
    "the prior distributions.\n",
    "Hence, the relation above holds for the conditional distributions, such that the joint invariance must factor into a prior invariance and a conditional invariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf7c87",
   "metadata": {},
   "source": [
    "To see this in more detail, note that the invariance relation for the joint distribution(s) is\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{u},\\theta) & = & \\gamma_{\\mathbf{h},\\mathbf{g}}\\,\n",
    "|J_{\\mathbf{h},\\mathbf{g}}(\\mathbf{u};\\theta,\\psi)|\\,\n",
    "p(\\mathbf{h}(\\mathbf{u};\\theta,\\psi),\\mathbf{g}(\\theta;\\psi))\n",
    "\\\\\\Rightarrow\n",
    "p(\\mathbf{u}\\mid\\theta)\\,p(\\theta) & = & \\gamma_{\\mathbf{h},\\mathbf{g}}\\,\n",
    "|J_\\mathbf{h}(\\mathbf{u};\\theta,\\psi)|\\,\n",
    "p(\\mathbf{h}(\\mathbf{u};\\theta,\\psi)\\mid\\mathbf{g}(\\theta;\\psi))\\,\n",
    "|J_\\mathbf{g}(\\theta;\\psi)|\\,\n",
    "p(\\mathbf{g}(\\theta;\\psi))\\,,\n",
    "\\\\\\Rightarrow\n",
    "\\gamma_\\mathbf{g}\\,p(\\mathbf{u}\\mid\\theta) & = &\n",
    "\\gamma_{\\mathbf{h},\\mathbf{g}}\\,\n",
    "|J_\\mathbf{h}(\\mathbf{u};\\theta,\\psi)|\\,\n",
    "p(\\mathbf{h}(\\mathbf{u};\\theta,\\psi)\\mid\\mathbf{g}(\\theta;\\psi))\\,,\n",
    "\\end{eqnarray}\n",
    "where the last line follows directly from the prior invariance relation,\n",
    "$p(\\theta)=\\gamma_\\mathbf{g}|J_\\mathbf{g}(\\theta;\\psi)|\\,p(\\mathbf{g}(\\theta;\\psi))$,\n",
    "given earlier.\n",
    "Hence, letting $\\gamma_{\\mathbf{h},\\mathbf{g}}=\\gamma_\\mathbf{h}\\gamma_\\mathbf{g}$, \n",
    "we finally obtain the *conditional (probability) invariance* relation\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{u}\\mid\\theta) & = & \\gamma_\\mathbf{h}\\,\n",
    "|J_\\mathbf{h}(\\mathbf{u};\\theta,\\psi)|\\,\n",
    "p(\\mathbf{h}(\\mathbf{u};\\theta,\\psi)\\mid\\mathbf{g}(\\theta;\\psi))\\,,\n",
    "\\end{eqnarray}\n",
    "which holds for all $\\mathbf{u}\\in\\mathcal{U}$, all $\\theta\\in\\Theta$, and \n",
    "*typically* for all $\\psi\\in\\Psi$. \n",
    "\n",
    "However, from the caveat given in\n",
    "the previous \n",
    "[section](#Unconditional-probability-invariance \"Section: Unconditional probability invariance\"),\n",
    "if invariance does **not** in fact hold for all subsets of the transformation parameter $\\psi$, then we require the more general invariance relation\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{u}\\mid\\theta) & = & \\gamma_\\mathbf{h}(\\psi)\\,\n",
    "|J_\\mathbf{h}(\\mathbf{u};\\theta,\\psi)|\\,\n",
    "p(\\mathbf{h}(\\mathbf{u};\\theta,\\psi)\\mid\\mathbf{g}(\\theta;\\psi))\\,,\n",
    "\\end{eqnarray}\n",
    "which **does** now hold for all $\\psi\\in\\Psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87dece",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b81ee9",
   "metadata": {},
   "source": [
    "### Nonlinear rescaling plus relocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef8126",
   "metadata": {},
   "source": [
    "Consider a scaling parameter $\\sigma\\in(0,\\infty)$ \n",
    "and a location parameter $\\mu\\in\\mathbb{R}$, such that the\n",
    "prior distriubtion $p(\\sigma,\\mu)$ is invariant both to further rescaling and to translation.\n",
    "\n",
    "Jaynes [[1]](#References \"Reference: Prior probabilities and transformation groups\"),\n",
    "solved this problem for simple translation $\\mu'=\\mu+\\nu$ and linear rescaling\n",
    "$\\sigma'=\\alpha\\sigma$. However, \n",
    "Milne [[2]](#References \"Reference: A note on scale invariance\"), \n",
    "solved the rescaling invariance problem (without translation) for nonlinear\n",
    "scaling $\\sigma'=\\alpha\\sigma^\\beta$. Here we combine both approaches.\n",
    "\n",
    "Let the prior distribution $p(\\mu)$ be invariant to the transformation\n",
    "\\begin{eqnarray}\n",
    "f(\\mu;\\nu) & = & \\mu+\\nu\\,,\n",
    "\\end{eqnarray}\n",
    "which has Jacobian\n",
    "\\begin{eqnarray}\n",
    "J_f(\\mu;\\nu) & = & \\frac{\\partial f}{\\partial\\mu}~=~1\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the transformation becomes the identity for $\\nu=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10ec8a",
   "metadata": {},
   "source": [
    "The [unconditional invariance](#Unconditional-probability-invariance \"Section: Unconditional probability invariance\")\n",
    "relation is thus\n",
    "\\begin{eqnarray}\n",
    "p(\\mu) & = & \\gamma_f\\,p(\\mu+\\nu)\\,.\n",
    "\\end{eqnarray}\n",
    "Taking the derivative with respect to the transformation parameter $\\nu$, and setting\n",
    "$\\nu=1$, gives\n",
    "\\begin{eqnarray}\n",
    "0~=~\\gamma_f\\,p'(\\mu) & \\Rightarrow & p(\\mu)~=~k_f\\,,\n",
    "\\end{eqnarray}\n",
    "for abitrary constant $k_f>0$. Note that this is an improper prior distribution.\n",
    "Substitution back into the invariance relation then gives $\\gamma_f=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a56ee9",
   "metadata": {},
   "source": [
    "Next, suppose that the prior distribution $p(\\sigma)$ is invariant to the transformation\n",
    "\\begin{eqnarray}\n",
    "g(\\sigma;\\alpha,\\beta) & = & \\alpha\\sigma^\\beta\\,,\n",
    "\\end{eqnarray}\n",
    "with $\\alpha>0$ and $\\beta\\ne 0$.\n",
    "Note that the transformation becomes the identity for $\\alpha=1$ and $\\beta=1$.\n",
    "The Jacobian of the transformation is then\n",
    "\\begin{eqnarray}\n",
    "J_g(\\mu;\\alpha,\\beta) & = & \\frac{\\partial g}{\\partial\\sigma}\n",
    "~=~\\alpha\\beta\\sigma^{\\beta-1}\\,,\n",
    "\\end{eqnarray}\n",
    "such that the\n",
    "[unconditional invariance](#Unconditional-probability-invariance \"Section: Unconditional probability invariance\")\n",
    "relation is given by\n",
    "\\begin{eqnarray}\n",
    "p(\\sigma) & = & \\gamma_g\\,\\alpha|\\beta|\\sigma^{\\beta-1}\\,p(\\alpha\\sigma^\\beta)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822da82",
   "metadata": {},
   "source": [
    "Taking derivatives with respect to the transformation parameters $\\alpha$ and $\\beta$\n",
    "therefore give\n",
    "\\begin{eqnarray}\n",
    "0 & = & \\gamma_g\\, |\\beta|\\sigma^{\\beta-1}\\left[\n",
    "p(\\alpha\\sigma^\\beta)\n",
    " +\\alpha\\,p'(\\alpha\\sigma^\\beta)\\,\\sigma^\\beta\n",
    "\\right]\\,,\n",
    "\\\\\n",
    "0 & = & \\gamma_g\\,\\alpha\\,\\sigma^{\\beta-1}\\left[\n",
    " \\mathtt{sign}(\\beta)\\,p(\\alpha\\sigma^\\beta)\n",
    " +|\\beta|\\,\\ln\\sigma\\,p(\\alpha\\sigma^\\beta)\n",
    " +|\\beta|\\,p'(\\alpha\\sigma^\\beta)\\,\\alpha\\sigma^\\beta\\,\\ln\\sigma\n",
    "\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe41081",
   "metadata": {},
   "source": [
    "Now, by substituting $\\alpha=1$ and $\\beta=1$, and simplifying the results,\n",
    "we obtain\n",
    "\\begin{eqnarray}\n",
    "p(\\sigma)+\\sigma\\,p'(\\sigma) & = & 0\\,,\n",
    "\\\\\n",
    "\\left[1+\\ln\\sigma\\right]\\,p(\\sigma)+\\sigma\\ln\\sigma\\,p'(\\sigma) & = & 0\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2d7b9",
   "metadata": {},
   "source": [
    "It turns out that we cannot (usefully) satisfy both of these equations simultaneously.\n",
    "The former equation (due to variation in $\\alpha$) has the solution\n",
    "\\begin{eqnarray}\n",
    "p(\\sigma) & = & \\frac{k_g}{\\sigma}\\,,\n",
    "\\end{eqnarray}\n",
    "for arbitrary constant $k_g>0$. However, the latter equation (due to variation in $\\beta$) then reduces to $k_g=0$.\n",
    "\n",
    "Consequently, we see that the nonlinear scaling **is** invariant to $\\alpha>0$, but is **not** invariant to $\\beta$, which must therefore be held constant to a value fixed in advance. In fact, from the unconditional invariance relation above, we have\n",
    "\\begin{eqnarray}\n",
    "p(\\sigma) & = & \\gamma_g\\,\\alpha|\\beta|\\sigma^{\\beta-1}\\,p(\\alpha\\sigma^\\beta)\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\frac{k_g}{\\sigma} & = &\n",
    " \\gamma_g\\,\\alpha|\\beta|\\sigma^{\\beta-1}\\frac{k_g}{\\alpha\\sigma^\\beta}\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\gamma_g & = & \\frac{1}{|\\beta|}\\,.\n",
    "\\end{eqnarray}\n",
    "This dependence of the renormalisation factor $\\gamma_g$ on $\\beta$ means that for each fixed value of $\\beta$ we obtain a family of transformations that are distributionally invariant to the scaling factor $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20370446",
   "metadata": {},
   "source": [
    "In fact, we actually require the \n",
    "[general form](#Unconditional-probability-invariance \"Section: Unconditional probability invariance\")\n",
    "of the unconditional invariance, namely\n",
    "\\begin{eqnarray}\n",
    "p(\\sigma) & = & \\gamma_g(\\alpha,\\beta)\\,\\alpha|\\beta|\\sigma^{\\beta-1}\\,p(\\alpha\\sigma^\\beta)\\,,\n",
    "\\\\\n",
    "\\Rightarrow 0 & = &  \\left[\n",
    "\\alpha\\frac{\\partial\\gamma_g}{\\partial\\alpha}+\\gamma_g\n",
    "\\right]\\,p(\\sigma')+\\gamma_g\\alpha\\sigma^\\beta\\,p'(\\sigma')\\,,\n",
    "\\\\\n",
    "0 & = & \\frac{\\partial\\gamma_g}{\\partial\\beta}|\\beta|\\,p(\\sigma')+\n",
    "\\gamma_g\\left[\n",
    " \\mathtt{sign}(\\beta)\\,p(\\sigma')\n",
    " +|\\beta|\\,\\ln\\sigma\\,p(\\sigma')\n",
    " +|\\beta|\\,p'(\\sigma')\\,\\alpha\\sigma^\\beta\\,\\ln\\sigma\n",
    "\\right]\\,,\n",
    "\\\\\n",
    "\\Rightarrow 0 & = & \\frac{\\partial\\gamma_g}{\\partial\\beta}|\\beta|\n",
    "+\\gamma_g\\,\\mathtt{sign}(\\beta)-\n",
    "\\alpha\\frac{\\partial\\gamma_g}{\\partial\\alpha}\\,|\\beta|\\ln\\sigma\\,.\n",
    "\\end{eqnarray}\n",
    "However, since $\\gamma_g(\\alpha,\\beta)$ is not a function of $\\sigma$, then \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\gamma_g}{\\partial\\alpha} & = & 0 ~\\Rightarrow~\n",
    "\\frac{\\partial\\gamma_g}{\\partial\\beta}\n",
    "~=~-\\gamma_g\\frac{\\mathtt{sign}(\\beta)}{|\\beta|}\n",
    "~\\Rightarrow~\\gamma_g~=~\\frac{1}{|\\beta|}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b7690",
   "metadata": {},
   "source": [
    "We see that taking $\\beta=1$ results in the linear scaling of \n",
    "Jaynes [[1]](#References \"Reference: Prior probabilities and transformation groups\"),\n",
    "for which $\\gamma_g=1$.\n",
    "Also note that now $\\gamma_g\\ne 1$ in general for $\\beta\\ne 1$ (except for $\\beta=-1$), despite the fact that both\n",
    "$\\sigma,\\sigma'\\in(0,\\infty)$. This is due, as discussed in the \n",
    "[section](#Distributional-invariance \"Section: Distributional invariance\")\n",
    "on distributional invariance, \n",
    "to the fact that $p(\\sigma)$ is an *improper* distribution that cannot actually be properly normalised. Despite this, we maintain (without proof) that\n",
    "[conservation of probability](#Conservation-of-probability \"Section: Conservation of probability\") continues to hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a95443",
   "metadata": {},
   "source": [
    "Finally, we now turn to the subsequent problem posed by\n",
    "Jaynes [[1]](#References \"Reference: Prior probabilities and transformation groups\"),\n",
    "namely that of invariance of the conditional distribution $p(x\\mid\\sigma,\\mu)$ to\n",
    "rescaling and relocation, specifically of the form $x'=(\\sigma'/\\sigma)(x-\\mu)+\\mu'$.\n",
    "Allowing for nonlinear rescaling, we therefore suppose that $p(x\\mid\\sigma,\\mu)$\n",
    "is invariant to the transformation\n",
    "\\begin{eqnarray}\n",
    "h(x;\\sigma,\\mu;\\alpha,\\beta,\\nu) & = & \\alpha\\sigma^{\\beta-1}\\,x\n",
    "-(\\alpha\\sigma^{\\beta-1}-1)\\,\\mu+\\nu\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this transformation becomes the identity for $\\alpha=1$, $\\beta=1$ and\n",
    "$\\nu=0$. Also note that its Jacobian is\n",
    "\\begin{eqnarray}\n",
    "J_h(x;\\sigma,\\mu;\\alpha,\\beta,\\nu) & = & \\frac{\\partial h}{\\partial x}\n",
    "~=~\\alpha\\sigma^{\\beta-1}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bf761",
   "metadata": {},
   "source": [
    "Hence, via \n",
    "[conditional invariance](#Conditional-probability-invariance \"Section: Conditional probability invariance\"), we obtain the further invariance relation\n",
    "\\begin{eqnarray}\n",
    "p(x\\mid\\sigma,\\mu) & = & \\gamma_h\\,|J_h|\\,p(x'\\mid\\sigma',\\mu')\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "p(x\\mid\\sigma,\\mu) & = & \\gamma_h\\,\\alpha\\sigma^{\\beta-1}\\,\n",
    "p(\\alpha\\sigma^{\\beta-1}\\,x\n",
    "-(\\alpha\\sigma^{\\beta-1}-1)\\,\\mu+\\nu\n",
    "\\mid\\alpha\\sigma^\\beta,\\mu+\\nu)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aedd7b",
   "metadata": {},
   "source": [
    "We observed above that the transformation $g$ for $\\sigma$ is not invariant to variation\n",
    "in the transformation parameter $\\beta$. Hence, taking only derivatives with respect to the other transformation parameters, $\\alpha$ and $\\nu$, we obtain\n",
    "\\begin{eqnarray}\n",
    "0 & = & \\gamma_h\\,\\sigma^{\\beta-1}\\left[\n",
    "  p(x'\\mid\\sigma',\\mu')\n",
    " +\\alpha\\,\\frac{\\partial p}{\\partial x}(x'\\mid\\sigma',\\mu')\n",
    "  \\,\\sigma^{\\beta-1}(x-\\mu)\n",
    " +\\alpha\\,\\frac{\\partial p}{\\partial\\sigma}(x'\\mid\\sigma',\\mu')\n",
    "  \\,\\sigma^\\beta\n",
    "\\right]\\,,\n",
    "\\\\\n",
    "0 & = & \\gamma_h\\,\\alpha\\sigma^{\\beta-1}\\left[\n",
    "  \\frac{\\partial p}{\\partial x}(x'\\mid\\sigma',\\mu')\n",
    " +\\frac{\\partial p}{\\partial\\mu}(x'\\mid\\sigma',\\mu')\n",
    "\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Thus, at $\\alpha=1$, $\\beta=1$ and $\\nu=0$, we have\n",
    "\\begin{eqnarray}\n",
    "p+(x-\\mu)\\frac{\\partial p}{\\partial x}+\\sigma\\frac{\\partial p}{\\partial\\sigma} & = & 0\\,,\n",
    "\\\\\n",
    "\\frac{\\partial p}{\\partial x}+\\frac{\\partial p}{\\partial\\mu} & = & 0\\,.\n",
    "\\end{eqnarray}\n",
    "The latter equation indicates that $p(x\\mid\\sigma,\\mu)$ is a function of $x-\\mu$, and\n",
    "the former equation is then satisfied in general for\n",
    "\\begin{eqnarray}\n",
    "p(x\\mid\\sigma,\\mu) & = & \\frac{k_h}{\\sigma}\\varphi\\left(\\frac{x-\\mu}{\\sigma}\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\varphi(\\cdot)$ is known as a *radial basis function*. Observe that this\n",
    "transformation invariance therefore holds for the Gaussian distribution with basis\n",
    "function\n",
    "$\\varphi(z)=e^{-\\frac{1}{2}z^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47edc1ad",
   "metadata": {},
   "source": [
    "Finally, substituting this back into the conditional invariance relation, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{k_h}{\\sigma}\\varphi\\left(\\frac{x-\\mu}{\\sigma}\\right)\n",
    "~=~\n",
    "\\gamma_h\\,\\alpha\\sigma^{\\beta-1}\\,\\frac{k_h}{\\alpha\\sigma^\\beta}\n",
    "\\varphi\\left(\\frac{x-\\mu}{\\sigma}\\right)\n",
    "& \\;\\;\\Rightarrow\\;\\; & \\gamma_h~=~1\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca95f50",
   "metadata": {},
   "source": [
    "Lastly, just for the sake of consistency, we check the derivative of the invariance relation with respect to $\\beta$, namely\n",
    "\\begin{eqnarray}\n",
    "0 & = & \\gamma_h\\,\\alpha\\left[\n",
    "  \\sigma^{\\beta-1}\\ln\\sigma\\,p(x'\\mid\\sigma',\\mu')\n",
    " +\\sigma^{\\beta-1}\\frac{\\partial p}{\\partial x}(x'\\mid\\sigma',\\mu')\\,\n",
    "  \\alpha\\sigma^{\\beta-1}(x-\\mu)\\,\\ln\\sigma\n",
    "\\right.\n",
    "\\\\\n",
    "&& \\;\\;\\;\\;\\;\\;\\;\\;\\left.\n",
    "{}+\\sigma^{\\beta-1}\\frac{\\partial p}{\\partial\\sigma}(x'\\mid\\sigma',\\mu')\\,\n",
    "  \\alpha\\sigma^\\beta\\ln\\sigma\n",
    "\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Evaluating this at $\\alpha=1$, $\\beta=1$ and $\\nu=0$ gives\n",
    "\\begin{eqnarray}\n",
    "  \\ln\\sigma\\,p\n",
    " +(x-\\mu)\\,\\ln\\sigma\\frac{\\partial p}{\\partial x}\n",
    " +\\sigma\\ln\\sigma\\frac{\\partial p}{\\partial\\sigma} & = & 0\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "It turns out that this is just the invariance relation for $\\alpha$ multiplied by\n",
    "$\\ln\\sigma$, and hence is entirely consistent. In fact, this consistency \n",
    "means that $p(x\\mid\\sigma,\\mu)$ is also invariant to changes in $\\beta$, and\n",
    "is thus the reason that $\\gamma_h=1$ is not a function of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1aa73a",
   "metadata": {},
   "source": [
    "### Von Kries paradox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b1c4e",
   "metadata": {},
   "source": [
    "We now turn to the resolution of the Von Kries paradox, as also discussed (briefly) by \n",
    "Milne [[2]](#References \"Reference: A note on scale invariance\").\n",
    "\n",
    "Suppose we know the specific density $\\rho$ (my notation) of a fluid is restricted to the range $\\rho\\in[a,b]$ for some $0<a<b<\\infty$. If we know nothing else, then the principle of insufficient reason suggests all values are equally likely, and so we might take the uniform prior $f(\\rho)=\\frac{1}{b-a}$.\n",
    "\n",
    "However, the specific volume $\\nu$ (again, my notation) is inversely related to the specific density via $\\nu=\\frac{1}{\\rho}$, giving rise to the Jacobian\n",
    "$J(\\rho)=\\frac{d\\nu}{d\\rho}=-\\frac{1}{\\rho^2}$. The corresponding prior $g(\\nu)$ therefore satisfies the conservation of probability relation\n",
    "\\begin{eqnarray}\n",
    "f(\\rho)~=~|J(\\rho)|\\,g(\\nu) & \\Rightarrow & \n",
    "g(\\nu)~=~\\frac{1}{\\nu^2}f\\left(\\frac{1}{\\nu}\\right)\n",
    "~=~\\frac{1}{(b-a)\\nu^2}\\,.\n",
    "\\end{eqnarray}\n",
    "The paradox is therefore that the specific volume is deterministically known from the specific density, but uniform ignorance of the specific density does not translate to uniform ignorance of the specific volume.  In other words, we seem to have acquired some knowledge of the specific volume for free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6acf1e",
   "metadata": {},
   "source": [
    "The resolution of the paradox, of course, is that we must already have added this knowledge by the *unwarranted* assumption of a uniform prior for the specific density.\n",
    "\n",
    "Instead we take ignorance to mean \n",
    "[distributional invariance](#Distributional-invariance \"Section: Distributional invariance\"). \n",
    "From the previous\n",
    "[section](#Nonlinear-rescaling-plus-relocation \"Section: Nonlinear rescaling plus relocation\"), \n",
    "we see that \n",
    "$\\nu=\\frac{1}{\\rho}$ is a specific form of the nonlinear transformation\n",
    "$\\nu=\\alpha\\rho^\\beta$ for $\\alpha=1$ and $\\beta=-1$. \n",
    "Consequently, we already know that $\\gamma=\\frac{1}{|\\beta|}=1$, such that the two prior\n",
    "distributions are given by\n",
    "\\begin{eqnarray}\n",
    "f(\\rho)~=~\\frac{k}{\\rho}\\,, &\\;\\;\\;\\;& \n",
    "g(\\nu)~=~\\frac{1}{\\nu^2}f\\left(\\frac{1}{\\nu}\\right)~=~\\frac{k}{\\nu}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Since we assumed a finite domain $\\rho\\in[a,b]$, we can easily obtain\n",
    "the normalisation constant as $k=\\frac{1}{\\ln b-\\ln a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7d743",
   "metadata": {},
   "source": [
    "We observe that now ignorance of the specific density gives rise to the same form as ignorance of the specific volume, which resolves the paradox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b034005b",
   "metadata": {},
   "source": [
    "### Linear regression coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979e601",
   "metadata": {},
   "source": [
    "Let us consider (briefly) the problem of fitting the straight-line model \n",
    "$y=\\alpha+\\beta x$, for $x,y\\in\\mathbb{R}$. Typically, in practice, we observe pairs of values $(x, y)$, and from these data $D$ we wish to infer the posterior distribution $p(\\alpha,\\beta\\mid D)$.\n",
    "However, here we are not concerned with the data but with the prior distributions, $p(\\alpha)$ and $p(\\beta)$, of the regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850be07",
   "metadata": {},
   "source": [
    "Firstly, we observe that $\\alpha\\in\\mathbb{R}$ uniquely specifies the intersection of the model line with the $y$-axis, and hence $\\alpha$ is a location parameter.\n",
    "Consequently, if we know nothing else about $\\alpha$, then\n",
    "we might suppose that the prior distribution, $p(\\alpha)$, is invariant to the translation \n",
    "$\\alpha'=\\alpha+\\nu$. The \n",
    "[unconditional invariance](#Unconditional-probability-invariance \"Section: Unconditional probability invariance\") \n",
    "relation is thus\n",
    "\\begin{eqnarray}\n",
    "p(\\alpha) & = & \\gamma_1\\,p(\\alpha+\\nu)\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "As usual, we take the derivative with respect to the transformation parameter $\\nu$, and then evaluate the result at the point $\\nu=0$, for which the transformation becomes an identity. This gives\n",
    "\\begin{eqnarray}\n",
    "0~=~\\gamma_1\\,p'(\\alpha) & \\Rightarrow & p(\\alpha)~=~k_1\\,,\\;\\;\\gamma_1~=~1\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, $\\alpha$ has an improper, uniform invariance prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8fde44",
   "metadata": {},
   "source": [
    "Next, we observe that the slope $\\beta\\in\\mathbb{R}$ is the tangent of the angle\n",
    "$\\theta\\in(-\\frac{\\pi}{2},\\frac{\\pi}{2})$ that the model line is rotated\n",
    "counter-clockwise from the $x$-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94309de",
   "metadata": {},
   "source": [
    "Some trigonometry gives us, for example:\n",
    "\\begin{eqnarray}\n",
    "\\beta & = & \\tan\\theta ~=~ \\frac{\\sin\\theta}{\\cos\\theta}\n",
    "~=~\\frac{\\mathtt{sign}(\\theta)\\,\\sqrt{1-\\cos^2\\theta}}{\\cos\\theta}\n",
    "\\\\\n",
    "\\Rightarrow \\cos\\theta & = & \\frac{1}{\\sqrt{1+\\beta^2}}\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we see that\n",
    "\\begin{eqnarray}\n",
    "\\frac{d\\tan\\theta}{d\\theta} & = & \n",
    "\\frac{d}{d\\theta}\\left(\\frac{\\sin\\theta}{\\cos\\theta}\\right)\n",
    "~=~\n",
    "\\frac{\\cos^2\\theta+\\sin^2\\theta}{\\cos^2\\theta}\n",
    "\\\\& = &\n",
    "1+\\tan^2\\theta\n",
    "~=~\\frac{1}{\\cos^2\\theta}~=~\\sec^2\\theta\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\frac{d\\beta}{d\\theta} & = & 1+\\beta^2\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a7167",
   "metadata": {},
   "source": [
    "If we are ignorant about any further properties of $\\beta$, then we may suppose that the\n",
    "prior distribution, $p(\\beta)$, is invariant to the rotational transformation\n",
    "$\\beta'=\\tan(\\theta+\\psi)$. The\n",
    "[unconditional invariance](#Unconditional-probability-invariance \"Section: Unconditional probability invariance\") \n",
    "relation is then\n",
    "\\begin{eqnarray}\n",
    "p(\\beta)\\,\\left|\\frac{d\\beta}{d\\theta}\\right| & = &\n",
    "\\gamma_2\\,p(\\beta')\\,\\left|\\frac{\\partial\\beta'}{\\partial\\theta}\\right|\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\beta'}{\\partial\\theta} & = &\n",
    "\\frac{\\partial\\beta'}{\\partial\\psi}~=~1+\\beta'^2\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the completed relation is\n",
    "\\begin{eqnarray}\n",
    "(1+\\beta^2)\\,p(\\beta) & = &\n",
    "\\gamma_2\\,(1+\\beta'^2)\\,p(\\beta')\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f4d38",
   "metadata": {},
   "source": [
    "Once again, we take the derivative with respect to the transformation parameter $\\psi$. This gives\n",
    "\\begin{eqnarray}\n",
    "0 & = & \\gamma_2\\,\\left[\n",
    "2\\beta'(1+\\beta'^2)\\,p(\\beta')+(1+\\beta'^2)^2\\,p'(\\beta')\n",
    "\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we evaluate the derivative at the point $\\psi=0$ at which the transformation becomes the identity. With simplification, this gives\n",
    "\\begin{eqnarray}\n",
    "2\\beta\\,p(\\beta)+(1+\\beta^2)\\,p'(\\beta) & = & 0\\,,\n",
    "\\\\\n",
    "\\Rightarrow \n",
    "\\frac{p'(\\beta)}{p(\\beta)} & = & -\\frac{2\\beta}{1+\\beta^2}\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\ln p(\\beta) & = & \\ln k_2-\\ln(1+\\beta^2)\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "p(\\beta) & = & \\frac{k_2}{1+\\beta^2}\\,.\n",
    "\\end{eqnarray}\n",
    "Substitution back into the unified invariance relation then gives $\\gamma_2=1$.\n",
    "Furthermore, we recognise that $p(\\beta)$ is just the Cauchy distribution, which properly normalises with $k_2=\\frac{1}{\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba298b",
   "metadata": {},
   "source": [
    "### Time-scale invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed521684",
   "metadata": {},
   "source": [
    "Consider the conditional distribution $p(t\\mid\\lambda)$ for a temporal variate\n",
    "$t\\in[0,\\infty)$ and a frequency parameter $\\lambda\\in(0,\\infty)$. The proper scale of time is thus given by $\\tau\\doteq\\frac{1}{\\lambda}$.\n",
    "\n",
    "Next, as per Jaynes' suggestion, consider two observers, X and Y, whose watches run at different rates. Hence, observer X has time-scale $\\tau$, but observer Y has time-scale\n",
    "$\\tau'=\\alpha\\tau$, for $\\alpha>0$. Assuming the prior $p(\\tau)$ is invariant to this transformation, we obtain, from the examples on \n",
    "[rescaling](#Nonlinear-rescaling-plus-relocation \"Section: Nonlinear rescaling plus relocation\")\n",
    "and the [Von Kries paradox](#Von-Kries-paradox \"Section: Von Kries paradox\"),\n",
    "that\n",
    "\\begin{eqnarray}\n",
    "p(\\tau)~=~\\frac{k}{\\tau}\\,, &\\;\\;\\;\\;& p(\\lambda)~=~\\frac{k}{\\lambda}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, we see that if observer X measures time as $t$, then observer Y measures times as $t'=\\frac{\\tau'}{\\tau} t=\\alpha t$. Assuming that $p(t\\mid\\lambda)$ is invariant to this transformation, we obtain the\n",
    "[conditional invariance](#Conditional-probability-invariance \"Section: Conditional invariance\")\n",
    "relation\n",
    "\\begin{eqnarray}\n",
    "p(t\\mid\\lambda) & = & \\gamma\\alpha\\,p\\left(\\alpha t\\mid\\frac{1}{\\alpha}\\lambda\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "for some $\\gamma>0$, where \n",
    "we define $\\lambda'\\doteq\\frac{1}{\\tau'}=\\frac{\\lambda}{\\alpha}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28830a9e",
   "metadata": {},
   "source": [
    "Hence, taking the derivative with respect to the transformation parameter $\\alpha$\n",
    "gives\n",
    "\\begin{eqnarray}\n",
    "0 & = &  \\gamma\\left[\n",
    "p(t'\\mid\\lambda')\n",
    "+\\alpha\\,t\\frac{\\partial p}{\\partial t}(t'\\mid\\lambda')\n",
    "-\\frac{\\lambda}{\\alpha}\\frac{\\partial p}{\\partial\\lambda}(t'\\mid\\lambda')\n",
    "\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Lastly, we substitute $\\alpha=1$, at which point the transformation is the identity, and simplify the result, giving\n",
    "\\begin{eqnarray}\n",
    "p(t\\mid\\lambda)\n",
    "+t\\frac{\\partial p}{\\partial t}(t\\mid\\lambda)\n",
    "-\\lambda\\frac{\\partial p}{\\partial\\lambda}(t\\mid\\lambda)\n",
    "& = & 0\\,.\n",
    "\\end{eqnarray}\n",
    "This has the solution\n",
    "\\begin{eqnarray}\n",
    "p(t\\mid\\lambda) & = & \\lambda e^{-\\lambda t}\\,,\n",
    "\\end{eqnarray}\n",
    "which we recognise as the *exponential distribution*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d03bc0",
   "metadata": {},
   "source": [
    "# Minimum Information Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145de1f",
   "metadata": {},
   "source": [
    "Apart from the transformation group invariance discussed in the previous\n",
    "[chapter](#Transformation-Group-Invariance \"Chapter: Transformation Group Invariance\"),\n",
    "there are other methodologies for choosing prior distributions on the grounds on *noninformativeness*. One such method, as discussed by Zellner [[5]](#References \"Reference: An introduction to Bayesian inference in econometrics\"),\n",
    "is that of choosing the prior to minimise a formal notion of entropic information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19882fda",
   "metadata": {},
   "source": [
    "## Minimum information principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d93ae",
   "metadata": {},
   "source": [
    "To set the scene, consider the conditional distribution $p(\\mathbf{u}\\mid\\theta)$,\n",
    "for $\\mathbf{u}\\in\\mathcal{U}$ and $\\theta\\in\\Theta$.\n",
    "The information-theoretic entropy of this distribution is defined as\n",
    "\\begin{eqnarray}\n",
    "H(\\mathcal{U}\\mid\\theta) & \\doteq &\n",
    "-\\int_\\mathcal{U} p(\\mathbf{u}\\mid\\theta)\\,\\log p(\\mathbf{u}\\mid\\theta)\\,|d\\mathbf{u}|\\,,\n",
    "\\end{eqnarray}\n",
    "which is conditional on some arbitrary but fixed value of the distributional parameters\n",
    "$\\theta$. Hence, for some prior distribution $p(\\theta)$, the average entropy is just the conditional entropy, defined as\n",
    "\\begin{eqnarray}\n",
    "H(\\mathcal{U}\\mid\\Theta) & \\doteq &\n",
    "\\int_\\Theta H(\\mathcal{U}\\mid\\theta)\\,p(\\theta)\\,|d\\theta|\\,.\n",
    "\\end{eqnarray}\n",
    "The prior distribution $p(\\theta)$ itself has the entropy\n",
    "\\begin{eqnarray}\n",
    "H(\\Theta) & \\doteq &\n",
    "-\\int_\\Theta p(\\theta)\\,\\log p(\\theta)\\,|d\\theta|\\,,\n",
    "\\end{eqnarray}\n",
    "such that the joint entropy is given by\n",
    "\\begin{eqnarray}\n",
    "H(\\mathcal{U},\\Theta) & \\doteq & \n",
    "-\\int_\\Theta\\int_\\mathcal{U} p(\\mathbf{u},\\theta)\\,\\log\n",
    "p(\\mathbf{u},\\theta)\\,|d\\mathbf{u}|\\,|d\\theta|\n",
    "\\\\& = &\n",
    "-\\int_\\Theta\\int_\\mathcal{U} p(\\mathbf{u}\\mid\\theta)\\,p(\\theta)\\,\\log\\left[\n",
    "p(\\mathbf{u}\\mid\\theta)\\,p(\\theta)\\right]\\,|d\\mathbf{u}|\\,|d\\theta|\n",
    "\\\\& = &\n",
    "H(\\mathcal{U}\\mid\\Theta)+H(\\Theta)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bd3e7",
   "metadata": {},
   "source": [
    "The maximum entropy principle now asserts that the distribution with the greatest entropy embodies the least prior information, such that a gain in information corresponds to a loss in entropy. In fact, Zellner [[5]](#References \"Reference: An introduction to Bayesian inference in econometrics\") would appear to define information as the negative of entropy.\n",
    "\n",
    "We therefore consider the change in entropy specified via\n",
    "\\begin{eqnarray}\n",
    "G(\\mathcal{U},\\Theta) & \\doteq & H(\\mathcal{U}\\mid\\Theta)-H(\\Theta)\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that as $p(\\theta)$ becomes more informative, then\n",
    "$H(\\Theta)$ decreases, and (roughly speaking) $G(\\mathcal{U},\\Theta)$ increases.\n",
    "Thus, $G(\\mathcal{U},\\Theta)$ apparently measures the additional information contained in the prior distribution $p(\\theta)$ that is not also present in the sampling distribution $p(\\mathbf{u}\\mid\\theta)$. Consequently, minimising $G$ should correspond to the choosing the least informative prior $p(\\theta)$, relative to\n",
    "a fixed distribution $p(\\mathbf{u}\\mid\\theta)$. \n",
    "This is the *minimum information principle*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393584a",
   "metadata": {},
   "source": [
    "Subject to the constraint that \n",
    "\\begin{eqnarray}\n",
    "\\int_\\Theta p(\\theta)\\,|d\\theta| & = & 1\\,,\n",
    "\\end{eqnarray}\n",
    "we therefore introduce the Lagrangian multiplier $\\lambda$, and minimise the functional\n",
    "\\begin{eqnarray}\n",
    "F[p] & \\doteq & \\lambda+\\int_\\Theta\\left\\{\n",
    "H(\\mathcal{U}\\mid\\theta)+\\log p(\\theta)-\\lambda\n",
    "\\right\\}\\,p(\\theta)\\,|d\\theta|\\,.\n",
    "\\end{eqnarray}\n",
    "By the calculus of variations, $F$ is minimised when\n",
    "\\begin{eqnarray}\n",
    "H(\\mathcal{U}\\mid\\theta)-\\lambda+\\frac{1}{\\ln b}+\\log p(\\theta) & = & 0\\,,\n",
    "\\end{eqnarray}\n",
    "where $b$ is the base of the logarithm.\n",
    "Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "p(\\theta) & = & k\\,e^{-H(\\mathcal{U}\\mid\\theta)\\,\\ln b}\\,,\n",
    "\\end{eqnarray}\n",
    "as the *minimum information prior*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f2627",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c66d4",
   "metadata": {},
   "source": [
    "### Gaussian distribution prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad57fe",
   "metadata": {},
   "source": [
    "Following Zellner [[5]](#References \"Reference: An introduction to Bayesian inference in econometrics\"), we consider the univariate Gaussian distribution\n",
    "\\begin{eqnarray}\n",
    "p(x\\mid\\mu,\\sigma) & = & \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n",
    "e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\ln p(x\\mid\\mu,\\sigma) & = & -\\frac{1}{2}\\ln(2\\pi\\sigma^2)\n",
    "-\\frac{(x-\\mu)^2}{2\\sigma^2}\\,,\n",
    "\\\\\n",
    "\\Rightarrow H(X\\mid\\mu,\\sigma) & = &\n",
    "\\int_{-\\infty}^{\\infty}\\left[\n",
    "\\frac{1}{2}\\ln(2\\pi\\sigma^2)\n",
    "+\\frac{(x-\\mu)^2}{2\\sigma^2}\n",
    "\\right]\\,p(x\\mid\\mu,\\sigma),dx\n",
    "\\\\& = &\n",
    "\\frac{1}{2}\\ln(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\mathtt{Var}(X)\n",
    "\\\\& = &\n",
    "\\frac{1}{2}\\left[1+\\ln(2\\pi\\sigma^2)\\right]\n",
    "~=~\\frac{1+\\ln(2\\pi)}{2}+\\ln\\sigma\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since $\\mathtt{Var}[X]=\\sigma^2$.\n",
    "The\n",
    "[minimum information](#Minimum-information-principle \"Section: Minimum information principle\") prior is therefore (with logarithmic base $b=e$) given by\n",
    "\\begin{eqnarray}\n",
    "p(\\mu,\\sigma) & = & k\\,e^{-H(X\\mid\\theta)}~=~\\frac{k'}{\\sigma}\\,.\n",
    "\\end{eqnarray}\n",
    "This agrees with the joint prior found using\n",
    "[transformation group invariance](#Nonlinear-rescaling-plus-relocation \"Section: Nonlinear rescaling plus relocation\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa82da7",
   "metadata": {},
   "source": [
    "### Exponential distribution prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714ca2f",
   "metadata": {},
   "source": [
    "We turn now to the exponential distribution\n",
    "\\begin{eqnarray}\n",
    "p(t\\mid\\lambda) & = & \\lambda e^{-\\lambda t}\\,,\n",
    "\\\\\\Rightarrow\n",
    "\\ln p(t\\mid\\lambda) & = & \\ln\\lambda-\\lambda t\\,,\n",
    "\\\\\\Rightarrow\n",
    "H(T\\mid\\lambda) & = & \\int_{0}^{\\infty}\\left[\n",
    "\\lambda t-\\ln\\lambda\n",
    "\\right]\\,p(t\\mid\\lambda)\\,dt\n",
    "\\\\& = &\n",
    "\\lambda E[T\\mid\\lambda]-\\ln\\lambda~=~1-\\ln\\lambda\\,,\n",
    "\\end{eqnarray}\n",
    "since we know that $E[T\\mid\\lambda]=\\frac{1}{\\lambda}$.\n",
    "The\n",
    "[minimum information](#Minimum-information-principle \"Section: Minimum information principle\") prior is therefore\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda) & = & k\\,e^{-H(T\\mid\\lambda)}~=~k'\\lambda\\,.\n",
    "\\end{eqnarray}\n",
    "However, this form of prior is unexpected, since we saw that \n",
    "[invariance to scaling](#Time-scale-invariance \"Section: Time-scale invariance\")\n",
    "led to the prior $p(\\lambda)\\propto\\frac{1}{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695123d7",
   "metadata": {},
   "source": [
    "In fact, if we reparameterise using the time-scale $\\tau=\\frac{1}{\\lambda}$, then we instead obtain\n",
    "\\begin{eqnarray}\n",
    "p(t\\mid\\tau) & = & \\frac{1}{\\tau} e^{-\\frac{t}{\\tau}}\\,,\n",
    "\\\\\\Rightarrow\n",
    "\\ln p(t\\mid\\tau) & = & -\\ln\\tau-\\frac{t}{\\tau}\\,,\n",
    "\\\\\\Rightarrow\n",
    "H(T\\mid\\tau) & = & \\int_{0}^{\\infty}\\left[\n",
    "\\frac{t}{\\tau}+\\ln\\tau\n",
    "\\right]\\,p(t\\mid\\tau)\\,dt\n",
    "\\\\& = &\n",
    "\\frac{1}{\\tau}\\,E[T\\mid\\tau]+\\ln\\tau~=~1+\\ln\\tau\\,,\n",
    "\\end{eqnarray}\n",
    "since $E[T\\mid\\tau]=\\tau$.\n",
    "The minimum information prior is now\n",
    "\\begin{eqnarray}\n",
    "p(\\tau) & = & k\\,e^{-H(T\\mid\\tau)}~=~\\frac{k'}{\\tau}\\,.\n",
    "\\end{eqnarray}\n",
    "This has the form of prior expected from \n",
    "[invariance to scaling](#Time-scale-invariance \"Section: Time-scale invariance\").\n",
    "Hence, we conclude that the \n",
    "[minimum information](#Minimum-information-principle \"Section: Minimum information principle\") prior is **not** generally invariant to transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd0f92",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a0c66",
   "metadata": {},
   "source": [
    "[1] E.T. Jaynes (1964): \"*Prior probabilities and transformation groups*\"\n",
    "([pdf](https://bayes.wustl.edu/etj/articles/groups.pdf \"bayes.wustl.edu\"))\n",
    "\n",
    "[2] P. Milne (1983): \"*A note on scale invariance*\"\n",
    "([ref](https://www.jstor.org/stable/686933 \"www.jstor.org\"))\n",
    "\n",
    "[3] E.T. Jaynes (1973): \"*The well-posed problem*\"\n",
    "([pdf](https://bayes.wustl.edu/etj/articles/well.pdf \"bayes.wustl.edu\"))\n",
    "\n",
    "[4] A. Drory (2015): \"*Failure and uses of Jaynes’ principle of transformation groups*\"\n",
    "([ref](https://www.researchgate.net/publication/273477459_Failure_and_Uses_of_Jaynes%27_Principle_of_Transformation_Groups \"www.researchgate.net\") to pdf)\n",
    "\n",
    "[5] A. Zelllner (1996): \"*An introduction to Bayesian inference in econometrics*\"\n",
    "([ref](https://www.wiley.com/en-au/An+Introduction+to+Bayesian+Inference+in+Econometrics-p-9780471169376\n",
    "\"www.wiley.com\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
