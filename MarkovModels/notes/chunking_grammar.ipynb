{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4a90c3",
   "metadata": {},
   "source": [
    "# Chunking Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfff0e",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explore a simple theory of sequence analysis using a context-free grammar\n",
    "that incorporates sequential dependencies. The theory is derived from fundamental principles using the notion\n",
    "of *chunking*. The initial rationale for this model comes from the short (and somewhat cryptic) papers of\n",
    "[(Kupiec)](#References \"References [1a,1b]\"),\n",
    "although its motivation from the idea of chunking is my own invention, and ultimately the model equations do not\n",
    "agree with those of (Kupiec).\n",
    "\n",
    "Further motivation for this model comes from the hierarchical HMMs of\n",
    "[(Fine, Singer and Tishby)](#References \"Reference [2]: The Hierarchical Hidden Markov Model: Analysis and Applications\")\n",
    "and\n",
    "[(Bui, Phung and Venkatesh)](#References \"Reference [3]: Hierarchical Hidden Markov Models with General State Hierarchy\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa707daa",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008b2f1",
   "metadata": {},
   "source": [
    "The context-free grammar $\\mathcal{G}$ under consideration is not restricted to Chomsky normal form (CNF).\n",
    "A CNF grammar has only binary rules, e.g. $\\texttt{NP}\\rightarrow\\texttt{D}\\oplus\\texttt{N}$, \n",
    "and unary rules, e.g. $\\texttt{N}\\rightarrow\\textit{cat}$,\n",
    "where here $\\texttt{NP}$ (noun-phrase), $\\texttt{D}$ (determiner) and $\\texttt{N}$ (noun) are non-terminal symbols and $\\textit{cat}$ is a terminal symbol. Instead, we shall allow arbitrary-length non-terminal rules,\n",
    "e.g. $\\texttt{NP}\\rightarrow\\texttt{D}\\oplus\\texttt{J}\\oplus\\texttt{N}$.\n",
    "The productions of such rules form contiguous *chunks* of non-terminal symbols (henceforth called *states*),\n",
    "e.g. $(\\texttt{D},\\texttt{J},\\texttt{N})$, having sequential dependencies between the states.\n",
    "\n",
    "The characteristic of being context-free is interpreted here in the Markov sense as meaning that each state depends only upon the previous state, and not upon past history. An example of a context-free derivation exhibiting sequential dependencies is shown in the figure below.\n",
    "\n",
    "<img src=\"context_free_bottom_up.png\" \n",
    "     title=\"Context-free, bottom-up chunking model of a sentence with sequential dependencies\" \n",
    "     width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8c49e",
   "metadata": {},
   "source": [
    "The grammar $\\mathcal{G}$ defines a finite vocabulary\n",
    "$\\mathcal{Y}=\\{\\nu_1,\\nu_2,\\ldots\\}$ of discrete terminal symbols, called *tokens*,\n",
    "and a finite set $\\mathcal{S}=\\{\\sigma_1,\\sigma_2,\\ldots\\}$ of discrete non-terminal symbols, called *states*. \n",
    "The subset $\\mathcal{S}_\\mathtt{leaf}\\subseteq\\mathcal{S}$ of states that may directly be used to generate tokens are called *leaf* states.\n",
    "The subset $\\mathcal{S}_\\mathtt{root}\\subseteq\\mathcal{S}$ of states that may be used at the root of a derivation are called *root* states. \n",
    "The subset $\\mathcal{S}_\\mathtt{int}\\subseteq\\mathcal{S}$ of states that may appear in a derivation between\n",
    "the root state and the leaf states are called *intermediate* states.\n",
    "Although $\\mathcal{S}=\\mathcal{S}_\\mathtt{root}\\cup\\mathcal{S}_\\mathtt{int}\\cup\\mathcal{S}_\\mathtt{leaf}$,\n",
    "there is, in general, no further restriction regarding whether the various subsets overlap or are mutually exclusive. Such restrictions, if required, must be built into the grammar by the presence of\n",
    "so-called *structural zeroes* in the conditional probability tables that dictate the stochastic nature\n",
    "of the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894cca11",
   "metadata": {},
   "source": [
    "### Sequence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62abfe9",
   "metadata": {},
   "source": [
    "The stochastic grammar $\\mathcal{G}$ should be capable of generating sequences. For the example \n",
    "sentence \"*The black cat purred.*\",\n",
    "the specific derivation shown above has a particular probability of being generated. In abbreviated form, this\n",
    "probability is\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "P(\\texttt{D}\\mid\\triangleleft)\\,P(\\textit{The}\\mid\\texttt{D})\\,P({\\oplus}\\mid\\texttt{D})\\,\n",
    "P(\\texttt{J}\\mid\\texttt{D})\\,\\,P(\\textit{black}\\mid\\texttt{J})\\,P(\\oplus\\mid\\texttt{J})\\,\n",
    "P(\\texttt{N}\\mid\\texttt{J})\\,\\,P(\\textit{cat}\\mid\\texttt{N})\\,P(\\square\\mid\\texttt{N})\\,\n",
    "\\\\&&\n",
    "P(\\texttt{NP}\\mid\\texttt{N})\\,P(\\oplus\\mid\\texttt{NP})\\,\n",
    "P(\\texttt{V}\\mid\\texttt{NP})\\,P(\\textit{purred}\\mid\\texttt{V})\\,P(\\square\\mid\\texttt{V})\\,\n",
    "P(\\texttt{VP}\\mid\\texttt{V})\\,\n",
    "P(\\square\\mid\\texttt{VP})\\,P(\\texttt{S}\\mid\\texttt{VP})\\,\n",
    "P(\\square\\mid\\texttt{S})\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Mnemonically, the leaf state sequence is \n",
    "$\\triangleleft\\texttt{D}\\oplus\\texttt{J}\\oplus\\texttt{N}\\square\\texttt{V}\\triangleright$,\n",
    "which corresponds to the partitioning, or *chunking*, \n",
    "$\\langle(\\texttt{D},\\texttt{J},\\texttt{N})(\\texttt{V})\\rangle$\n",
    "of the complete token sequence $\\langle\\textit{The},\\textit{black},\\textit{cat},\\textit{purred}\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48897d",
   "metadata": {},
   "source": [
    "Some explanation is clearly in order here. Firstly, the *marker* symbol '$\\triangleleft$' is used to indicate the start of a sequence. Marker symbols are used to denote the internal context of the stochastic process.\n",
    "Marker symbols are never externalised, and hence operate in conjunction with the context-free\n",
    "state-to-state transitions. Thus, the corresponding marker '$\\triangleright$' indicates the end of a complete sequence. In addition, the marker '$\\square$' denotes the end of a subsequence (or *chunk*), \n",
    "and the corresponding marker '$\\oplus$' denotes a continuation of the subsequence.\n",
    "\n",
    "Starting a new sequence automatically starts a new *chunk*\n",
    "(explained further in a [later](#Chunking \"Section: Chunking\") section)\n",
    "at the *leaf* state level, which is the lowest non-terminal level.\n",
    "Starting a new chunk triggers the generation of an initial state.\n",
    "The leaf level is special in that the production of a leaf state triggers the generation of its \n",
    "corresponding *token* (or terminal symbol). The production of tokens also operates \n",
    "in conjunction with the context-free state-to-state transitions.\n",
    "\n",
    "While a chunk is *open*, i.e. the rule has not yet completed, a stochastic choice is made as to whether to *close* the chunk or keep it open. As mentioned, the marker '$\\square$' is used to indicate closure,\n",
    "such that $P(\\square\\mid X)$ is the probability of closing the chunk immediately after state $X$.\n",
    "Conversely, $P(\\oplus\\mid X)=1-P(\\square\\mid X)$ is the probability of keeping the chunk open.\n",
    "If the chunk remains open, then there is a transition to a leaf state in the next position, and this state is\n",
    "appended to the open chunk (hence the reason for the marker '$\\oplus$').\n",
    "\n",
    "When a chunk is closed, this (usually) triggers the generation of a parent state assigned to the chunk at a higher level. If no open parent chunk currently exists, then one is created. The parent state is then appended\n",
    "to the open parent chunk.\n",
    "At this higher level, a stochastic decision is again made as to whether to close the parent chunk or keep it open. If the chunk remains open, then a new chunk is started at the leaf level, and a leaf state\n",
    "is generated.\n",
    "\n",
    "The generation process terminates when a single-state chunk is closed and no parent chunk exists at\n",
    "the next higher level. The closure of this highest-level *root* chunk automatically triggers the\n",
    "closure of the derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda290d",
   "metadata": {},
   "source": [
    "### Sequence parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc852eab",
   "metadata": {},
   "source": [
    "The converse of sequence generation, as discussed in the\n",
    "[previous](#Sequence-generation \"Section: Sequence generation\") section,\n",
    "is sequence parsing. Here the goal is to start with an observed sequence of tokens, and to produce the\n",
    "(or a) most probable derivation. However, since most generative grammars are designed in a top-down fashion, \n",
    "and parsing usually proceeds in a bottom-up fashion, there is typically a disconnection between the two\n",
    "approaches.\n",
    "\n",
    "However, the aim is to design a simplified grammar that can easily be used for both sequence generation\n",
    "and sequence parsing. \n",
    "For example, one parsing model of the derivation shown in the previous section might be\n",
    "\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "P(\\texttt{D}\\mid\\triangleleft,\\textit{The})\\,P(\\oplus\\mid\\texttt{D})\\,\n",
    "P(\\texttt{J}\\mid\\texttt{D},\\textit{black})\\,P(\\oplus\\mid\\texttt{J})\\,\n",
    "P(\\texttt{N}\\mid\\texttt{J}),\\textit{cat})\\,P(\\square\\mid\\texttt{N})\\,\n",
    "\\\\&&\n",
    "P(\\texttt{NP}\\mid\\texttt{N})\\,P(\\oplus\\mid\\texttt{NP})\\,\n",
    "P(\\texttt{V}\\mid\\texttt{NP},\\textit{purred})\\,P(\\square\\mid\\texttt{V})\\,\n",
    "P(\\texttt{VP}\\mid\\texttt{V})\\,\n",
    "P(\\square\\mid\\texttt{VP})\\,P(\\texttt{S}\\mid\\texttt{VP})\\,\n",
    "P(\\square\\mid\\texttt{S})\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "As noted above, the generative grammar defines terms like $P(\\texttt{D}\\mid\\triangleleft)$ and\n",
    "$P(\\textit{The}\\mid\\texttt{D})$, not $P(\\texttt{D}\\mid\\triangleleft,\\textit{The})$.\n",
    "However, via Bayes' rule we find that\n",
    "\\begin{eqnarray}\n",
    "P(\\texttt{D}\\mid\\triangleleft,\\textit{The}) & = &\n",
    "\\frac{\n",
    "P(\\texttt{D}\\mid\\triangleleft)\\,P(\\textit{The}\\mid\\texttt{D})\n",
    "}{\n",
    "\\sum_{\\sigma\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "P(\\sigma\\mid\\triangleleft)\\,P(\\textit{The}\\mid\\sigma)\n",
    "}\\,,\n",
    "\\end{eqnarray}\n",
    "and so we may define the probabilities required for parsing in terms of the probabilities required for\n",
    "generating a derivation.\n",
    "\n",
    "Conversely, we may (in some circumstances) define the derivation probabilities in terms of the parsing probabilities.\n",
    "The conversion between derivation and parsing relies on the fact that, again via Bayes' rule, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{P(\\nu_m\\mid\\sigma_i)}{P(\\nu_m)} & = &\n",
    "\\frac{P(\\sigma_i\\mid\\nu_m)}{P(\\sigma_i)}\\,,\n",
    "\\end{eqnarray}\n",
    "for all leaf states $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ and all tokens $\\nu_m\\in\\mathcal{Y}$.\n",
    "Consequently, although we typically do not know the unconditional token probability $P(\\nu_m)$ for $\\nu_m\\in\\mathcal{Y}$, we may substitute\n",
    "\\begin{eqnarray}\n",
    "P(\\nu_m\\mid\\sigma_i) & \\propto &\n",
    "\\frac{P(\\sigma_i\\mid\\nu_m)}{P(\\sigma_i)}\\,,\n",
    "\\end{eqnarray}\n",
    "on the basis that the unknown term $P(\\nu_m)$ cancels out when conditioning on the observed tokens,\n",
    "which typically involves only summations over the leaf states $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$,\n",
    "e.g.\n",
    "\\begin{eqnarray}\n",
    "P(\\texttt{D}\\mid\\triangleleft,\\textit{The}) & = &\n",
    "\\frac{\n",
    "\\frac{P(\\texttt{D}\\mid\\triangleleft)\\,P(\\texttt{D}\\mid\\textit{The})}{P(\\texttt{D})}\n",
    "}{\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\frac{P(\\sigma_i\\mid\\triangleleft)\\,P(\\sigma_i\\mid\\textit{The})}{P(\\sigma_i)}\n",
    "}\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b111efd",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9359f5",
   "metadata": {},
   "source": [
    "*Chunking* is the process of partitioning a sequence into contiguous subsequences, such that each subsequence, \n",
    "henceforth called a *chunk*, has self-consistent semantics with respect to the grammar $\\mathcal{G}$.\n",
    "For example, an English sentence could be chunked into noun phrases and verb phrases, et cetera.\n",
    "Note that although a chunk is a subsequence, an arbitrary subsequence is not necessarily a chunk.\n",
    "Also note that chunks may themselves be chunked, leading to a nested derivation tree. However, this tree is\n",
    "not restricted to binary branches, nor does it exclude unary 'branches' from parent state to child state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1096b68",
   "metadata": {},
   "source": [
    "Suppose the stochastic process has generated a complete sequence \n",
    "$\\mathbf{y}_{1:T}=\\langle y_1,y_2,\\ldots,y_T\\rangle$ of tokens $y_t\\in\\mathcal{Y}$. \n",
    "Here the marker symbols '<' and '>'\n",
    "respectively denote the start and end of a complete (i.e. terminated) sequence. For our example sentence,\n",
    "with the derivation shown in the \n",
    "[introduction](#Introduction \"Section: Introduction\"),\n",
    "we have the token sequence \n",
    "$\\mathbf{y}_{1:4}=\\langle\\textit{The},\\textit{black},\\textit{cat},\\textit{purred}\\rangle$,\n",
    "with the corresponding chunking\n",
    "$\\mathbf{y}^\\mathtt{chunk}_{1:4}=\\langle(\\textit{The},\\textit{black},\\textit{cat})(\\textit{purred})\\rangle$, where the marker symbols\n",
    "'(' and ')' respectively denote the start and end of a chunk of contiguous elements.\n",
    "\n",
    "The leaf states of the derivation have also been chunked, namely as\n",
    "$\\mathbf{s}^\\mathtt{leaf}_{1:4}=\\langle(\\texttt{D},\\texttt{J},\\texttt{N})(\\texttt{V})\\rangle$, and the\n",
    "*intermediate* states (those at levels above the leaf states but below the *root* state) have been chunked\n",
    "as\n",
    "$\\mathbf{s}^\\mathtt{int}_{1:2}=\\langle(\\texttt{NP},\\texttt{VP})\\rangle$.\n",
    "The root level chunk is $\\mathbf{s}^\\mathtt{root}_{1}=\\langle(\\texttt{S})\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d321d",
   "metadata": {},
   "source": [
    "During the process of chunking, a key notion is whether a given state subsequence $\\mathbf{s}_{r:t}$ comprises a complete chunk or only part of a chunk.\n",
    "A complete chunk, represented as $\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t)$, has a definite start and a definite end, where '(' denotes the immutable start of the chunk, and ')' denotes the immutable end of the chunk.\n",
    "This is called *closed* because no further states may be appended.\n",
    "\n",
    "Conversely, an incomplete chunk has a definite start but (as yet) only an indefinite end,\n",
    "and is represented as $\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t]$, where the marker ']' denotes the *mutable* 'end' of\n",
    "the chunk. This is called *open* because it may potentially have zero, one or more additional states appended to it, before being closed.\n",
    "\n",
    "Hence, from the derivation, we have $\\mathbf{s}^\\mathtt{leaf}_{1:1}=(\\texttt{D}]$ and\n",
    "$\\mathbf{s}^\\mathtt{leaf}_{1:2}=(\\texttt{D},\\texttt{J}]$, but \n",
    "$\\mathbf{s}^\\mathtt{leaf}_{1:3}=(\\texttt{D},\\texttt{J},\\texttt{N})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99fb46",
   "metadata": {},
   "source": [
    "### Hierarchical chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009d6dc",
   "metadata": {},
   "source": [
    "The procedure described for [sequence generation](#Sequence-generation \"Section: Sequence generation\")\n",
    "essentially produces a sequence derivation that represents hierarchical chunking.\n",
    "A summary of this procedure, slightly modified, is as follows:\n",
    "\n",
    "1. The first token $y_1$ in a sequence $\\mathbf{y}_{1:T}$ is paired with its corresponding leaf state $s_1$. This state\n",
    "starts an open leaf chunk, $\\mathbf{s}^\\mathtt{leaf}_{1:1}=(s_1]$.\n",
    "\n",
    "2. For $1\\le r\\le t\\le T$, consider the current open leaf chunk \n",
    "$\\mathbf{s}_{r:t}^\\mathtt{leaf}=(s_r,s_{r+1},\\ldots,s_t]$.\n",
    "\n",
    "\t* If $t<T$, then a stochastic decision is made whether to close the chunk or keep it open.\n",
    "However, if $t=T$, then every open chunk will be closed, in order from the lowest level to the highest level.\n",
    "\n",
    "\t* If the chunk is kept open, then the state $s_{t+1}$ of the next token $y_{t+1}$ is appended to the \n",
    "chunk, giving $\\mathbf{s}_{r:t+1}^\\mathtt{leaf}=(s_r,\\ldots,s_t,s_{t+1}]$. The chunking process now loops\n",
    "to position $t+1$.\n",
    "\n",
    "3. However, if the chunk is closed, then it is now represented by \n",
    "    $\\mathbf{s}_{r:t}^\\mathtt{leaf}=(s_r,\\ldots,s_t)$.\n",
    "    \n",
    "    * A higher level parent state $s_{r:t}^\\mathtt{int}$ is now assigned to the closed leaf chunk,\n",
    "    and this parent state is appended to the open parent chunk (which is created as necessary).\n",
    "\n",
    "    * If $t<T$, then a stochastic decision is made whether to close the parent chunk or keep it open.\n",
    "However, if $t=T$, then the parent chunk will be closed.\n",
    "  \n",
    "    * If the parent chunk remains open, then the closed lead chunk $\\mathbf{s}_{r:t}^\\mathtt{leaf}$ is succeeded by an adjacent open leaf chunk\n",
    "$\\mathbf{s}_{t+1:t+1}^\\mathtt{leaf}=(s_{t+1}]$. The chunking process now loops to step 2 at position $t+1$.\n",
    "\n",
    "\t* However, if the parent chunk is closed, then a grandparent state is assigned, and a \n",
    "    closure decision is made at the higher level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8bafb0",
   "metadata": {},
   "source": [
    "We shall not pursue hierarchical chunking any further here, although a full treatment\n",
    "is required for [parsing](#Sequence-parsing \"Section: Sequence parsing\").\n",
    "Instead, we consider a simplified process that uses only the leaf state level and a single, intermediate state level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c75fc0",
   "metadata": {},
   "source": [
    "### Two-level chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506df1d",
   "metadata": {},
   "source": [
    "For our simplified model, we consider only the complete sequence $\\mathbf{y}_{1:T}$ of tokens,\n",
    "along with a corresponding sequence $\\mathbf{s}^\\mathtt{leaf}_{1:T}$ of leaf states, and an arbitrary-length, secondary sequence $\\mathbf{s}^\\mathtt{int}$ of intermediate states. For convenience, we drop the superscript '$\\mathtt{leaf}$', on the understanding that use of a state variable $S_t$ implies\n",
    "a leaf state $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ spanning token $y_t$. \n",
    "Similarly, use of the state variable $S_{r:t}$ implies\n",
    "an intermediate state $\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$ assigned to a closed chunk that spans \n",
    "leaf states $\\mathbf{s}_{r:t}$ and thus tokens $\\mathbf{y}_{r:t}$.\n",
    "In addition, we recognise that the sequence generation process has internal context, which we have\n",
    "represented using marker symbols. Hence, we let the\n",
    "variable $M_t$ denote the context at token position $t$. For more precision, we also let\n",
    "$M^-_t$ denote the context immediately before position $t$ but after position $t-1$, and also let\n",
    "$M^+_t$ denote the context immediately after position $t$ but before position $t+1$.\n",
    "\n",
    "The first state $s_1$ in the leaf sequence $\\mathbf{s}_{1:T}$ is generated with probability\n",
    "$P(S_1=\\sigma_i\\mid M^-_1=\\triangleleft)\\doteq\\iota^\\triangleleft_i$, where\n",
    "$\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\\iota^\\triangleleft_i=1$. \n",
    "The vector $\\mathbf{\\iota}^\\triangleleft$ specifies the *start-of-sequence* leaf state probabilities.\n",
    "Note, however, that the start of a sequence implies the start of the first chunk.\n",
    "Thus, for an arbitrary (open or closed) chunk $\\mathbf{s}_{r:t}$ that starts at position $r$, we can also define\n",
    "$P(S_r=\\sigma_i\\mid M^-_{r}=\\square)\\doteq\\iota^\\square_i$, where $\\mathbf{\\iota}^\\square$\n",
    "specifies the *start-of-chunk* leaf state probabilities. Since this latter quantity will see frequent use, we often drop the superscript '$\\square$' for convenience. \n",
    "Hence, instead of $P(S_1=\\sigma_i\\mid M^-_1=\\triangleleft)$ we could use \n",
    "$P(S_1=\\sigma_i\\mid M^-_1=\\square)$, if we do not care to distinguish the first chunk from consequent chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ce066",
   "metadata": {},
   "source": [
    "Similarly, the last state $s_T$ in the leaf sequence $\\mathbf{s}_{1:T}$ closes a complete sequence,\n",
    "and the probability of closure is given by\n",
    "$P(M^+_T=\\triangleright\\mid S_T=\\sigma_i)\\doteq\\tau^\\triangleright_i$, \n",
    "where the marker symbol '$\\triangleright$' denotes the *end-of-sequence*.\n",
    "Conversely, for an incomplete sequence the\n",
    "probability of being left open is \n",
    "$\\bar{\\tau}^\\triangleright_i\\doteq 1-\\tau^\\triangleright_i$.\n",
    "Once again, the closure of a sequence implies the closure of the last chunk, and hence\n",
    "we could instead use $P(M^+_T=\\square\\mid S_T=\\sigma_i)$. More generally, we let\n",
    "$P(M^+_t=\\square\\mid S_t=\\sigma_i)\\doteq\\tau^\\square_i$ be the probability of closing an open chunk\n",
    "$\\mathbf{s}_{r:t}$, and let $P(M^+_t=\\oplus\\mid S_t=\\sigma_i)\\doteq\\bar{\\tau}^\\square_i$ denote the\n",
    "complementary probability of leaving the chunk open. We typically drop the superscript '$\\square$' due to the frequent use of *end-of-chunk* probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb3d2e",
   "metadata": {},
   "source": [
    "Since both the [sequence generation](#Sequence-generation \"Section: Sequence generation\")\n",
    "process and [sequence parsing](#Sequence-parsing \"Section: Sequence parsing\")\n",
    "process traverse the tokens from left to right,\n",
    "in general we consider a single, arbitrary chunk \n",
    "$\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t]$ that starts at position $r$\n",
    "with context $M^-_r=\\square$, and continues without closure up to and including position $t$.\n",
    "By default, we consider a chunk as being open until explicitly closed. In other words, unless otherwise specified, we assume that the closure decision $M^+_t$ has yet to be made.\n",
    "However, in some circumstances we do have further information. For instance, if we know the chunk is closed \n",
    "with context $M^+_t=\\square$, then we use the representation $\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t)$ to indicate\n",
    "that the subsequence closure symbol ')' has additional probability. Alternatively, if we know the chunk \n",
    "has been closed at position $t$ but do not yet know the starting position of the chunk, then we have context $M^-_r=\\oplus$ with representation $\\mathbf{s}_{r:t}=[s_r,\\ldots,s_t)$. \n",
    "In exceptional circumstances, we might know only $M^-_r=\\oplus$ and $M^+_t=\\oplus$, giving \n",
    "representation $\\mathbf{s}_{r:t}=[s_r,\\ldots,s_t]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c61bf",
   "metadata": {},
   "source": [
    "### Forward chunk recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83b5ae",
   "metadata": {},
   "source": [
    "Consider the open chunk $\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t]$ that spans the subsequence $\\mathbf{y}_{r:t}$ of tokens.\n",
    "Now, by consideration of the derivation shown in the\n",
    "[introduction](#Introduction \"Section: Introduction\"), we observe in general that a chunk starting at position $r$ is usually preceded by some intermediate state \n",
    "$S_{*:r-1}=\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$, where the index symbol '*' indicates that the start of the\n",
    "previous chunk is indeterminate.\n",
    "The exception is the first chunk with $r=1$, which is preceded by the context $M^-_1=\\triangleleft$.\n",
    "\n",
    "We further suppose that the 'last' position $t$ of the chunk has some leaf state\n",
    "$s_t=\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$.\n",
    "Hence, we define the *start-chunk* probability $\\alpha_{r:t}(p,i)$ as\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}(p,i) & \\doteq & \n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},S_t=\\sigma_i\\mid M^+_{r-1}=\\square,S_{*:r-1}=\\sigma_p)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{Y}_{r:t}=(Y_r,\\ldots,Y_t)$ and $Y_t$ is a variable denoting the stochastic choice\n",
    "of token $\\nu_m\\in\\mathcal{Y}$ at position $t$.\n",
    "The exceptional first-chunk case is defined via\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1:t}(\\triangleleft,i) & \\doteq & \n",
    "P(\\mathbf{Y}_{1:t}=\\mathbf{y}_{1:t},S_t=\\sigma_i\\mid M^-_1=\\triangleleft)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5e056",
   "metadata": {},
   "source": [
    "As [previously](#Two-level-chunking \"Section: Two-level chunking\") \n",
    "discussed, the first chunk starts with state $s_1=\\sigma_i$ with probability\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1:1}(\\triangleleft,i) & \\doteq &\n",
    "P(Y_1=y_1,S_1=\\sigma_i\\mid M^-_1=\\triangleleft)~=~\n",
    "P(S_1=\\sigma_i\\mid M^-_1=\\triangleleft)\\,P(Y_1=y_1\\mid S_1=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "As an aside, note that if we are parsing some observed sequence $\\mathbf{y}$, then we may pre-compute the \n",
    "*leaf-token* probabilities\n",
    "$\\breve{\\mathbf{B}}=[\\breve{b}_{it}]$ for each $\\breve{b}_{it}\\doteq P(Y_t=y_t\\mid S_t=\\sigma_i)$.\n",
    "Alternatively, if the process is generating tokens, then\n",
    "arbitrary token $\\nu_m\\in\\mathcal{Y}$ may be generated\n",
    "from state $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ with probability\n",
    "\\begin{eqnarray}\n",
    "P(Y_t=\\nu_m\\mid S_t=\\sigma_i) & \\doteq & b_{im}\\,,\n",
    "\\end{eqnarray}\n",
    "via the pre-specified *emission* matrix $\\mathbf{B}=[b_{im}]$.\n",
    "Hence, for consistency between generation and parsing, we define\n",
    "\\begin{eqnarray}\n",
    "\\breve{b}_{it} & \\doteq &\n",
    "\\sum_{\\nu_m\\in\\mathcal{Y}}\\delta(y_t=\\nu_m)\\,b_{im}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the first chunk has starting probability\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1:1}(\\triangleleft,i) & \\doteq & \\iota_i^\\triangleleft\\,\\breve{b}_{i1}\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "More generally, the first state of an arbitrary chunk starting at position $r>1$ has probability\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:r}(p,i) & \\doteq &\n",
    "P(Y_r=y_r,S_r=\\sigma_i\\mid S_{*:r-1}=\\sigma_p)~=~d_{pi}\\,\\breve{b}_{ir}\\,,\n",
    "\\end{eqnarray}\n",
    "where the *intermediate-to-leaf* state transition matrix $\\mathbf{D}=[d_{pi}]$\n",
    "specifies the generation of leaf state $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ \n",
    "after intermediate state $\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$ with probability\n",
    "\\begin{eqnarray}\n",
    "P(S_r=\\sigma_i\\mid S_{*:r-1}=\\sigma_p,M^+_{*:r-1}=\\oplus) & \\doteq & d_{pi}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that here we take $M^+_{*:r-1}=\\oplus$ to mean that the intermediate chunk containing state $\\sigma_p$\n",
    "remains open, which in turn means that the sequence does not close here, such that there **must** be \n",
    "a transition to a successive leaf state at position $r$.\n",
    "\n",
    "In the special case where the previous intermediate state $S_{*:r-1}$ is unknown, we define\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:r}(\\square,i) & \\doteq &\n",
    "P(Y_r=y_r,S_r=\\sigma_i\\mid M^-_r=\\square)~=~\\iota^\\square_i\\,\\breve{b}_{ir}\\,,\n",
    "\\end{eqnarray}\n",
    "and more generally\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}(\\square,i) & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},S_r=\\sigma_i\\mid M^-_r=\\square)\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Having selected the initial leaf state $s_r$ of the open chunk $\\mathbf{s}_{r:r}$, the process\n",
    "makes a choice to either close the chunk with probability \n",
    "$P(M^+_r=\\square\\mid S_r=\\sigma_i)=\\tau_i$, or leave it open with probability \n",
    "$P(M^+_r=\\oplus\\mid S_t=\\sigma_i)=\\bar{\\tau}_i$.\n",
    "If the chunk is left open, then it **must** be expanded to include\n",
    "position $r+1$, and a subsequent leaf state $s_{r+1}=\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}$ will be chosen\n",
    "with probability\n",
    "\\begin{eqnarray}\n",
    "P(S_{r+1}=\\sigma_j\\mid S_r=\\sigma_i,M^+_r=\\oplus) & \\doteq & a_{ij}\\,,\n",
    "\\end{eqnarray}\n",
    "via the *leaf-to-leaf* state transition matrix $\\mathbf{A}=[a_{ij}]$.\n",
    "This stochastic process repeats iteratively to ultimately generate the chunk $\\mathbf{s}_{r:t}$.\n",
    "At this point, if the chunk $\\mathbf{s}_{r:t}$ remains open, then we obtain the *forward* recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t+1}(p,j) & = &\n",
    "%\\sum_{j=1}^{\\left|\\mathcal{S}_\\mathtt{leaf}\\right|}\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t}(p,i)\\,\\bar{\\tau}_i\\,a_{ij}\\,\\breve{b}_{j,t+1}\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf56d04",
   "metadata": {},
   "source": [
    "### Backward chunk recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad3b48",
   "metadata": {},
   "source": [
    "In [forward recursion](#Forward-chunk-recursion \"Section: Forward chunk recursion\"),\n",
    "we considered an open-right chunk $\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t]$ that\n",
    "formed the start of some complete chunk.\n",
    "The analogue to forward recursion is therefore *backward* recursion,\n",
    "commencing from an open-left chunk $\\mathbf{s}_{t+1:w}=[s_{t+1},\\ldots,s_w)$\n",
    "that forms the end of some complete chunk.\n",
    "Since this chunk is closed on the right, the last leaf state $s_w$ will have triggered a transition\n",
    "to an intermediate state, say $s_{*:w}=\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$.\n",
    "We also assume that since the chunk is open (by default) on the left, then at position $t$ there is either a continuation of the same chunk or the end of a previous chunk, with some leaf state, say\n",
    "$s_t=\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$. In other words, we do not yet know the context $M_{t+1}$.\n",
    "Since the chunk $\\mathbf{s}_{t+1:w}$ spans the tokens $\\mathbf{y}_{t+1:w}$, we now define the\n",
    "*end-chunk* probability $\\beta_{t:w}(i,p)$ as\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:w}(i,p) & \\doteq &\n",
    "P(\\mathbf{Y}_{t+1:w}=\\mathbf{y}_{t+1:w},M^+_w=\\square,S_{*:w}=\\sigma_p\\mid S_t=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Observe that if the chunk at position $t$ remains open, with probability\n",
    "$P(M^+_t=\\oplus\\mid S_t=\\sigma_i)=\\bar{\\tau}_i$, then this represents part of the *same* chunk.\n",
    "Hence, the recurrence relation is\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:w}(i,p) & = & \\bar{\\tau}_i\\,a_{ij}\\,b_{j,y_{t+1}}\\,\\beta_{t+1:w}(j,p)\\,.\n",
    "\\end{eqnarray}\n",
    "The edge case occurs for $t=w$, at which point the chunk at position $t$ must become closed\n",
    "with probability $P(M^+_t=\\square\\mid S_t=\\sigma_i)=\\tau_i$, and the process will transition\n",
    "to intermediate state $s_{*:t}=\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$ with probability\n",
    "\\begin{eqnarray}\n",
    "P(S_{*:t}=\\sigma_p\\mid S_t=\\sigma_i,M^+_t=\\square) & = & u_{ip}\\,,\n",
    "\\end{eqnarray}\n",
    "as specified by the *leaf-to-intermediate* state transition matrix $\\mathbf{U}=[u_{ip}]$.\n",
    "Hence, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:t}(i,p) & \\doteq & \n",
    "P(M^+_t=\\square,S_{*:t}=\\sigma_p\\mid S_t=\\sigma_i)~=~\\tau_i\\,u_{ip}\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Finally, note that if we do not know or do not care about the final intermediate state $\\sigma_p$, then we may \n",
    "marginalise over it to obtain\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:w}(i,\\square) & \\doteq &\n",
    "P(\\mathbf{Y}_{t+1:w}=\\mathbf{y}_{t+1:w},M^+_w=\\square\\mid S_t=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "Also note that at the end of a complete sequence $\\mathbf{y}_{1:T}$ we may use\n",
    "$P(M^+_T=\\triangleright\\mid S_T=\\sigma_i)\\doteq\\tau^\\triangleright_i$ in place of\n",
    "$P(M^+_T=\\square\\mid S_T=\\sigma_i)\\doteq\\tau^\\square$, whereupon\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:T}(i,p) & \\doteq &\n",
    "P(\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},M^+_T=\\triangleright,S_{*:T}=\\sigma_p\\mid S_t=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, if we again ignore \n",
    "intermediate state $\\sigma_p$, then we have\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:T}(i,\\triangleright) & \\doteq &\n",
    "P(\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},M^+_T=\\triangleright\\mid S_t=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f8807",
   "metadata": {},
   "source": [
    "### Chunks and multi-chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ff47c",
   "metadata": {},
   "source": [
    "We now consider a single, closed chunk $\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t)$.\n",
    "For $r>1$ the chunk must have started from a previous closed chunk with some\n",
    "intermediate state, say $s_{*:r-1}=\\sigma_q\\in\\mathcal{S}_\\mathtt{int}$.\n",
    "Likewise, the chunk ends at position $t$ and must have transitioned from leaf state to another intermediate state, say $s_{r:t}=\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$.\n",
    "In general, the complete chunk may be split into a *start-chunk* subsequence and an *end-chunk* subsequence.\n",
    "The split may occur at any position $r\\le w\\le t$, with arbitrary leaf state $s_w=\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$.\n",
    "Hence, the probability of the complete chunk is\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{r:t}(q,p) & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M^+_t=\\square,S_{r:t}=\\sigma_p\\mid M^+_{r-1}=\\square,S_{*:r-1}=\\sigma_q)\n",
    "\\\\& = &\n",
    "\\sum_{w=r}^{t}\n",
    "%\\sum_{i=1}^{\\left|\\mathcal{S}_\\mathtt{leaf}\\right|}\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:w}(q,i)\\,\\beta_{w:t}(i,p)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note from [forward recursion](#Forward-chunk-recursion \"Section: Forward chunk recursion\") that\n",
    "we also have a variant for the start of a sequence, which corresponds to \n",
    "computing $\\gamma_{1:t}(\\triangleleft,p)$ from $\\alpha_{1:w}(\\triangleleft,p)$,\n",
    "and also a variant for the start of an arbitrary chunk (without further context), which corresponds\n",
    "to computing $\\gamma_{r:t}(\\square,p)$ from $\\alpha_{r:w}(\\square,p)$.\n",
    "Similarly, from \n",
    "[backward recursion](#Backward-chunk-recursion \"Section: Backward chunk recursion\"),\n",
    "we have variants for marginalising over the final intermediate state $\\sigma_p$, \n",
    "both for the end of a sequence with\n",
    "$\\gamma_{r:T}(q,\\triangleright)$ computed via $\\beta_{w:T}(q,\\triangleright)$,\n",
    "and the end of a chunk with\n",
    "$\\gamma_{r:T}(q,\\square)$ computed from $\\beta_{w:T}(q,\\square)$.\n",
    "Thus, in general, the probability of observing a subsequence $\\mathbf{y}_{r:t}$ of tokens\n",
    "as a single, complete chunk is given by\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M^+_t=\\square\\mid M^-_r=\\square)\n",
    "~=~\\gamma_{r:t}(\\square,\\square)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac23d9",
   "metadata": {},
   "source": [
    "In practice, these definitions are interesting but not very useful. More precisely, the formulae derived\n",
    "so far could possibly (with more theory) be used for partitioning a sequence of tokens into chunks, but they are not helpful from the point of view of estimating the grammar $\\mathcal{G}$, nor from the point of view of analysing an entire sequence.\n",
    "Let us therefore turn from consideration of a single leaf chunk to consideration of a subsequence of one or more contiguous leaf chunks, henceforth called a *multi-chunk*. If the multi-chunk contains more than one chunk,\n",
    "then all chunks bar the last one must be closed on the right, and all chunks bar the first one must be\n",
    "closed on the left. We require more context before we can know if the multi-chunk is itself closed on\n",
    "the left and/or the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387ba24",
   "metadata": {},
   "source": [
    "There are various ways we could define a multi-chunk. One way is to extend our previous definition of a single chunk.\n",
    "For example, we could combine two chunks, say $\\gamma_{r:s}(q,v)$ and\n",
    "$\\gamma_{s+1:t}(v,p)$. More generally, for a closed multi-chunk comprised of\n",
    "an arbitrary number of adjacent, closed chunks, we have\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\gamma}_{r:t}(q,p) & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M^+_t=\\square,S_{r:t}=\\sigma_p\\mid M^+_{r-1}=\\square,S_{*:r-1}=\\sigma_q)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Unfortunately, our notation is ambiguous, because this definition takes exactly the same form as that for\n",
    "a single chunk $\\gamma_{r:t}(q,p)$. Rather than modify the notation, we shall simply rely on the context\n",
    "as to whether we are dealing with a single chunk or a multi-chunk.\n",
    "Now, for $r=t$ the multi-chunk reduces to a single chunk, with probability\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\gamma}_{t:t}(q,p) & \\doteq &\n",
    "P(Y_t=y_t,M^+_t=\\square,S_{t:t}=\\sigma_p\\mid M^+_{t-1}=\\square, S_{*:t-1}=\\sigma_q)\n",
    "~=~\\gamma_{t:t}(q,p)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "For $r<t$, the recurrence relation is given by\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\gamma}_{r:t}(q,p) & = &\n",
    "\\gamma_{r:t}(q,p)\n",
    "+\\sum_{s=r}^{t-1}\n",
    "\\sum_{\\sigma_v\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\bar{\\gamma}_{r:s}(q,v)\\,\\gamma_{s+1:t}(v,p)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this definition essentially determines all of the ways that a multi-chunk may be partitioned,\n",
    "which in practice might not be very efficient.\n",
    "Also note that since we are marginalising over all internal structure of the multi-chunk, we need only consider\n",
    "combining a multi-chunk with a single chunk (on either the left or the right), otherwise the summation over the\n",
    "combination of two multi-chunks will count internal chunks multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ae41c",
   "metadata": {},
   "source": [
    "As an alternative formulation, let us now simplify matters by ignoring the initial dependence on the previous intermediate\n",
    "state, which implies that we now have no prior context at the start of the multi-chunk.\n",
    "Similarly, let us ignore the final intermediate state, and consider instead only the final leaf state.\n",
    "Thus, consider an open multi-chunk $\\mathbf{s}_{r:t}=(s_r,\\ldots,s_t]$ that is closed on the left and open (by default) on the right. We may model this situation via\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}^\\mathtt{multi}(\\square,i) & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},S_{t}=\\sigma_i\\mid M^-_r=\\square)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this takes the same form as the single-chunk \n",
    "[forward](#Forward-chunk-recursion \"Section: Forward chunk recursion\")\n",
    "probability $\\alpha_{r:t}(\\square,i)$.\n",
    "The difference is that for a single chunk we implicitly assume there are no intra-chunk closures, whereas now for a multi-chunk we potentially have internal closures representing closed chunks. Clearly our notation is somewhat\n",
    "ambiguous.\n",
    "\n",
    "Now, given this multi-chunk, a decision is made to either close the last chunk in the multi-chunk\n",
    "with probability $\\tau_i$, or leave it open with probability $\\bar{\\tau}_i$. If it is closed,\n",
    "then there must be a transition to some intermediate state.\n",
    "This situation is modelled via\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}^\\mathtt{int}(\\square,p) & \\doteq & \n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M^+_t=\\square,S_{*:t}=\\sigma_p\\mid M^-_r=\\square) ~=~\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\\alpha_{r:t}^\\mathtt{multi}(\\square,i)\\,\\tau_i\\,u_{ip}\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17940d35",
   "metadata": {},
   "source": [
    "Initially, for $r=t$ the multi-chunk contains only a single chunk, and the (open) multi-chunk probability reduces to\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{t:t}^\\mathtt{multi}(\\square,i) & \\doteq &\n",
    "P(Y_t=y_t,S_t=\\sigma_i\\mid M^-_t=\\square)~=~\\iota^\\square_i\\,\\breve{b}_{it}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, at the start of the sequence we may instead use\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1:1}^\\mathtt{multi}(\\triangleleft,i) & \\doteq &\n",
    "P(Y_1=y_1,S_1=\\sigma_i\\mid M^-_1=\\triangleleft)~=~\\iota^\\triangleleft\\,\\breve{b}_{i1}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "For $r<t$, the multi-chunk may contain one or more single chunks. In general,\n",
    "either the last chunk in the multi-chunk started at position $t$ with the closure of a previous chunk at position $t-1$, or else the last chunk also remained open (on the left) at position $t-1$.\n",
    "For the former case, the leaf state $s_t=\\sigma_i$ must be the result of an intermediate-to-leaf state transition, and for the latter case it results from a leaf-to-leaf state transition.\n",
    "Hence, the recurrence relation is\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}^\\mathtt{multi}(\\square,i) & = &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t-1}^\\mathtt{multi}(\\square,j)\\,\\bar{\\tau}_j\\,a_{ji}\\,\\breve{b}_{it}\n",
    "+\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\alpha_{r:t-1}^\\mathtt{int}(\\square,p)\\,d_{pi}\\,\\breve{b}_{it}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Despite differences in notation, this matches the relation given by \n",
    "[(Kupiec)](#References \"References [1a,1b]\"), even though our model here for $\\alpha^\\mathtt{int}$\n",
    "completely differs from the model of (Kupiec)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4464e",
   "metadata": {},
   "source": [
    "For our purposes, we may now dispense with $\\alpha^\\mathtt{int}$ by observing that\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}^\\mathtt{multi}(\\square,i) & = &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t-1}^\\mathtt{multi}(\\square,j)\\,\\bar{\\tau}_j\\,a_{ji}\\,\\breve{b}_{it}\n",
    "+\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\\alpha_{r:t-1}^\\mathtt{multi}(\\square,j)\\,\\tau_j\\,u_{jp}\n",
    "\\,d_{pi}\\,\\breve{b}_{it}\n",
    "\\\\& = &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t-1}^\\mathtt{multi}(\\square,j)\\,\\left\\{\n",
    "\\bar{\\tau}_j\\,a_{ji}+\n",
    "\\tau_j\\,\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "u_{jp}\\,d_{pi}\n",
    "\\right\\}\\,\\breve{b}_{it}\n",
    "\\\\& = &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t-1}^\\mathtt{multi}(\\square,j)\\,\\tilde{a}_{ij}\\,\\breve{b}_{it}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where now $\\tilde{a}_{ij}$ represents either a direct leaf-to-leaf state transition or\n",
    "an indirect combination of a leaf-to-intermediate state transition and an intermediate-to-leaf state transition.\n",
    "In other words, $\\tilde{a}_{ij}$ operates both within a chunk and across chunk boundaries.\n",
    "In matrix form, this corresponds to\n",
    "\\begin{eqnarray}\n",
    "\\tilde{\\mathbf{A}} & \\doteq & \n",
    "\\mathtt{diag}(\\mathbf{1}-\\boldsymbol{\\tau})\\,\\mathbf{A}\n",
    "+\\mathtt{diag}(\\boldsymbol{\\tau})\\,\\mathbf{U}\\mathbf{D}\\,,\n",
    "\\end{eqnarray}\n",
    "which may be pre-computed, making the forward recursion efficient to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308546bd",
   "metadata": {},
   "source": [
    "Next, consider a multi-chunk $\\mathbf{s}_{r:t}=[s_r,\\ldots,s_t)$ that is open on the left and closed on the right. Analogously to the closure $\\beta_{r:t}(i,\\square)$ of a single chunk,\n",
    "we define the closure $\\beta^\\mathtt{multi}_{r:t}(i,\\square)$ of a multi-chunk via\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:t}^\\mathtt{multi}(i,\\square) & \\doteq & \n",
    "P(\\mathbf{Y}_{r+1:t}=\\mathbf{y}_{r+1:t},M_t^+=\\square\\mid S_r=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that since the last chunk in the multi-chunk must be closed at position $t$, we have\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:t}^\\mathtt{multi}(i,\\square) & \\doteq & P(M_t^+=\\square\\mid S_t=\\sigma_i)\n",
    "~=~\\tau_i^\\square\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, at the end of a complete sequence $\\mathbf{y}_{1:T}=\\langle y_1,\\ldots,y_T\\rangle$,\n",
    "we could instead use\n",
    "\\begin{eqnarray}\n",
    "\\beta_{T:T}^\\mathtt{multi}(i,\\triangleright) & \\doteq & P(M_T^+=\\triangleright\\mid S_T=\\sigma_i)\n",
    "~=~\\tau_i^\\triangleright\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "In general, for $r<t$ we suppose that either the last chunk in the multi-chunk was started at position $r+1$\n",
    "after the previous chunk was closed at position $r$, or else the last chunk extends back to include position $r$. Hence, we obtain the recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:t}^\\mathtt{multi}(i,\\square) & = & \n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\tilde{a}_{ij}\\,b_{j,y_{r+1}}\\,\\beta_{r+1:t}^\\mathtt{multi}(j,\\square)\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cec0f6",
   "metadata": {},
   "source": [
    "Finally, we note that occasionally (e.g. for\n",
    "[grammar estimation](#Grammar-estimation \"Section: Grammar estimation\")) \n",
    "we might need to complete a multi-chunk from an intermediate \n",
    "state $\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$ rather than from a leaf state $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$.\n",
    "When the context (i.e. leaf level versus intermediate level) is clear\n",
    "then we additionally define\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:t}^\\mathtt{int}(p,\\square) & \\doteq &\n",
    "P(\\mathbf{Y}_{r+1:t}=\\mathbf{y}_{r+1:t},M^+_t=\\square\\mid M^+_r=\\square, S_{*:r}=\\sigma_p)\n",
    "\\\\& = &\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "d_{pi}\\,b_{i,y_{r+1}}\\,\\beta_{r+1:T}^\\mathtt{multi}(i,\\square)\n",
    "\\end{eqnarray}\n",
    "for $r<t$, and\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:t}^\\mathtt{int}(p,\\square) & \\doteq &\n",
    "P(M^+_t=\\square\\mid M^+_t=\\square, S_{*:t}=\\sigma_p)\n",
    "~=~\n",
    "1\\,,\n",
    "\\end{eqnarray}\n",
    "for $r=t$. Once again, for the end of a sequence we may instead use\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:T}^\\mathtt{int}(p,\\triangleright) & \\doteq &\n",
    "P(\\mathbf{Y}_{r+1:T}=\\mathbf{y}_{r+1:T},M^+_T=\\triangleright\\mid M^+_r=\\square, S_{*:r}=\\sigma_p)\n",
    "\\\\& = &\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "d_{pi}\\,b_{i,y_{r+1}}\\,\\beta_{r+1:T}^\\mathtt{multi}(i,\\triangleright)\n",
    "\\end{eqnarray}\n",
    "for $r<t$, and\n",
    "\\begin{eqnarray}\n",
    "\\beta_{T:T}^\\mathtt{int}(p,\\triangleright) & \\doteq &\n",
    "P(M^+_T=\\triangleright\\mid M^+_T=\\triangleright, S_{*:T}=\\sigma_p)\n",
    "~=~\n",
    "1\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84868506",
   "metadata": {},
   "source": [
    "### Sequence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac97fa",
   "metadata": {},
   "source": [
    "We now have enough information for inference. In particular, the likelihood of a complete sequence\n",
    "$\\mathbf{y}_{1:T}=\\langle y_1,\\ldots,y_T\\rangle$ is\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}_{1:T}) & \\doteq &\n",
    "P(M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M^+_T=\\triangleright) \n",
    "\\\\& = &\n",
    "P(M^-_1=\\triangleleft)\\,\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{1:t}^\\mathtt{multi}(\\triangleleft,i)\\,\\beta_{t:T}^\\mathtt{leaf}(i,\\triangleright)\\,,\n",
    "\\end{eqnarray}\n",
    "for $t=1,2,\\ldots,T$.\n",
    "We typically assume that $P(M^-_1=\\triangleleft)=1$, \n",
    "i.e. that both the sequence and the first chunk must start at position 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1b102",
   "metadata": {},
   "source": [
    "For subsequences of tokens, the situation is more complex, since we have to allow for chunk boundaries. Of particular interest for sequence prediction is the incomplete subsequence \n",
    "$\\mathbf{y}_{1:t}=\\langle y_1,\\ldots,y_{t}]$ for $t<T$. \n",
    "If token $y_{t}$ has some leaf state $s_{t}=\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}$,\n",
    "then either position $t$ is the last position in its chunk with probability $\\tau^\\square_j$, or else\n",
    "the chunk remains open with probability $\\bar{\\tau}^\\square_j$. \n",
    "Since $\\tau^\\square_j+\\bar{\\tau}^\\square_j=1$, the probability of the subsequence is\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}_{1:t}) & \\doteq &\n",
    "P(M^-_1=\\triangleleft,\\mathbf{Y}_{1:t}=\\mathbf{y}_{1:t}) \n",
    "\\\\& = &\n",
    "P(M^-_1=\\triangleleft)\\,\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{1:t}^\\mathtt{multi}(\\triangleleft,i)\\,.\n",
    "\\end{eqnarray}\n",
    "One-step prediction for $t+1<T$ is then obtained via\n",
    "\\begin{eqnarray}\n",
    "P(y_{t+1}\\mid\\mathbf{y}_{1:t}) & \\doteq &\n",
    "\\frac{P(\\mathbf{y}_{1:t+1})}{P(\\mathbf{y}_{1:t})}\n",
    "~=~\n",
    "\\frac{\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{1:t+1}^\\mathtt{multi}(\\triangleleft,i)\n",
    "}{\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{1:t}^\\mathtt{multi}(\\triangleleft,i)\n",
    "}\\,.\n",
    "\\end{eqnarray}\n",
    "The remainder of the complete sequence is also predicted as\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}_{t+1:T}\\mid\\mathbf{y}_{1:t}) & \\doteq &\n",
    "P(\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},M^+_T=\\triangleright\n",
    "\\mid\\mathbf{Y}_{1:t}=\\mathbf{y}_{1:t},M^-_1=\\triangleleft) \n",
    "\\\\& = &\n",
    "\\frac{P(\\mathbf{y}_{1:T})}{P(\\mathbf{y}_{1:t})}\n",
    "~=~\n",
    "\\frac{\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{1:t}^\\mathtt{multi}(\\triangleleft,i)\\,\\beta_{t:T}^\\mathtt{multi}(i,\\triangleright)\n",
    "}{\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{1:t}^\\mathtt{multi}(\\triangleleft,i)\n",
    "}\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f89ff2",
   "metadata": {},
   "source": [
    "### Grammar estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee694b6",
   "metadata": {},
   "source": [
    "From [forward recursion](#Forward-chunk-recursion \"Section: Forward chunk recursion\") and\n",
    "[backward recursion](#Backward-chunk-recursion \"Section: Backward chunk recursion\"),\n",
    "we see that the stochastic nature of the chunking grammar $\\mathcal{G}$ is determined by a number of conditional probability tables (CPTs).\n",
    "Like a standard HMM, we require the *leaf-to-leaf* state transition matrix $\\mathbf{A}=[a_{ij}]$, the\n",
    "token *emission* matrix $\\mathbf{B}=[b_{im}]$, and the probability vector \n",
    "$\\boldsymbol{\\iota}^\\triangleleft=[\\iota^\\triangleleft_i]$ of *initial* states.\n",
    "Also, for complete sequences, we require the vector \n",
    "$\\boldsymbol{\\tau}^\\triangleright=[\\tau^\\triangleright_i]$ of sequence *termination* probabilities.\n",
    "Finally, for chunking we require the probability vector \n",
    "$\\boldsymbol{\\iota}^\\square=[\\iota^\\square_i]$ of *initial* chunk states and\n",
    "the vector \n",
    "$\\boldsymbol{\\tau}^\\square=[\\tau^\\square_i]$ of chunk *termination* probabilities, as well as\n",
    "the *leaf-to-intermediate* state transition matrix $\\mathbf{U}=[u_{ip}]$ and the\n",
    "*intermediate-to-leaf* state transition matrix $\\mathbf{D}=[d_{pi}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c926142",
   "metadata": {},
   "source": [
    "In analogy to the estimation process for a HMM, we assume that an expectation-maximisation (EM) formulation\n",
    "leads to a maximum likelihood (ML) estimate, by which the various probability vectors and matrices are\n",
    "simply normalised forms of vectors and matrices of various joint counts of interest. EM is an iterative process\n",
    "that starts with prior estimates, e.g. $\\mathbf{A}'$, $\\mathbf{B}'$, etc., and produces\n",
    "posterior re-estimates, e.g. $\\hat{\\mathbf{A}}$, $\\hat{\\mathbf{B}}$, et cetera. For notational convenience,\n",
    "we henceforth drop the prime from our prior estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd2c9e",
   "metadata": {},
   "source": [
    "In order to estimate $\\mathbf{A}$, we need to compute the number $N_{ij}^\\mathtt{leaf}$ of times that leaf state $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ has been immediately followed\n",
    "by leaf state $\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}$ within an open chunk. \n",
    "This is a stochastic value, and so we estimate the expected value $\\hat{N}_{ij}$ given the observed\n",
    "sequence $\\mathbf{y}_{1:T}$. The require computation is\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}_{ij}^\\mathtt{leaf} & \\doteq & \\mathbb{E}[N_{ij}^\\mathtt{leaf}\\mid\\mathbf{y}_{1:T}]\n",
    "\\\\& = &\n",
    "\\sum_{t=1}^{T-1}\n",
    "P(S_{t}=\\sigma_i,S_{t+1}=\\sigma_j,M^+_t=\\oplus\\mid M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M^+_T=\\triangleright)\n",
    "\\\\& = &\n",
    "\\frac{P(M^-_1=\\triangleleft)}{P(\\mathbf{y}_{1:T})}\n",
    "\\sum_{t=1}^{T-1}\n",
    "\\alpha_{1:t}^\\mathtt{multi}(\\triangleleft,i)\\,\\bar{\\tau}_i\\,\n",
    "a_{ij}\\,b_{j,y_{t+1}}\\,\n",
    "\\beta_{t+1:T}^\\mathtt{multi}(j,\\triangleright)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the latest estimate of the *leaf-to-leaf* state (or *within-chunk*) transition probability matrix $\\mathbf{A}=[a_{ij}]$\n",
    "is obtained via $\\hat{a}_{ij}\\doteq\\frac{\\hat{N}_{ij}^\\mathtt{leaf}}{\\hat{N}_{i\\cdot}^\\mathtt{leaf}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673f53d",
   "metadata": {},
   "source": [
    "Similarly, let $N_{ip}^\\mathtt{up}$ be the number of times that any chunk closes with leaf state \n",
    "$\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ and intermediate\n",
    "state $\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$. Then\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}_{ip}^\\mathtt{up} & \\doteq &\n",
    "\\mathbb{E}[N_{ip}^\\mathtt{up}\\mid\\mathbf{y}_{1:T}] \n",
    "\\\\& = &\n",
    "\\sum_{t=1}^{T-1}\n",
    "P(S_t=\\sigma_i,M^+_t=\\square,S_{*:t}=\\sigma_p\\mid \n",
    "M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M^+_T=\\triangleright)\n",
    "\\\\&=&\n",
    "\\frac{P(M^-_1=\\triangleleft)}{P(\\mathbf{y}_{1:T})}\\left\\{\n",
    "\\sum_{t=1}^{T-1}\n",
    "\\alpha_{1:t}^\\mathtt{multi}(\\triangleleft,i)\\,\\tau^\\square_i\\,u_{ip}\\,\n",
    "\\beta_{t+1:T}^\\mathtt{int}(p,\\triangleright)\n",
    "+\\alpha_{1:T}^\\mathtt{multi}(\\triangleleft,i)\\,\\tau^\\triangleright_i\n",
    "\\,u_{ip}\n",
    "\\right\\}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and the *leaf-to-intermediate* state (or *end-chunk*) transition probability matrix \n",
    "$\\mathbf{U}=[u_{ip}]$ is re-estimated via\n",
    "$\\hat{u}_{ip}\\doteq\\frac{\\hat{N}_{ip}^\\mathtt{up}}{\\hat{N}^\\mathtt{up}_{i\\cdot}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d994e1",
   "metadata": {},
   "source": [
    "Additionally, let $N_{pi}^\\mathtt{down}$ be the number of times that a chunk closes\n",
    "with intermediate state $\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$ \n",
    "and the next chunk  opens with leaf state \n",
    "$\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$. \n",
    "Then\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}_{pi}^\\mathtt{down} & \\doteq &\n",
    "\\mathbb{E}[N_{pi}^\\mathtt{down}\\mid\\mathbf{y}_{1:T}] \n",
    "\\\\& = &\n",
    "\\sum_{t=1}^{T-1}\n",
    "P(M^+_t=\\square,S_{*:t}=\\sigma_p,S_{t+1}=\\sigma_i\\mid M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M^+_T=\\triangleright)\n",
    "\\\\&=&\n",
    "\\frac{P(M^-_1=\\triangleleft)}{P(\\mathbf{y}_{1:T})}\n",
    "\\sum_{t=1}^{T-1}\n",
    "\\alpha_{1:t}^\\mathtt{int}(\\triangleleft,p)\n",
    "\\,d_{pi}\\,\\breve{b}_{i,t+1}\\,\\beta_{t+1:T}^\\mathtt{multi}(i,\\triangleright)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and the *intermediate-to-leaf* state (or *start-chunk*) transition probability matrix $\\mathbf{D}=[d_{pi}]$ is re-estimated via\n",
    "$\\hat{d}_{pi}\\doteq\\frac{\\hat{N}_{pi}^\\mathtt{down}}{\\hat{N}^\\mathtt{down}_{p\\cdot}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0393d3",
   "metadata": {},
   "source": [
    "Finally, we want to re-estimate the *initial* state probability vectors, namely\n",
    "$\\boldsymbol{\\iota}^\\square=[\\iota_i^\\square]$ for the start of chunks, and\n",
    "$\\boldsymbol{\\iota}^\\triangleleft=[\\iota_i^\\triangleleft]$ for the start of sequences.\n",
    "Likewise, we want to re-estimate the state *termination* probabilities, namely\n",
    "$\\boldsymbol{\\tau}^\\square=[\\tau_i^\\square]$ for the end of chunks, and\n",
    "$\\boldsymbol{\\tau}^\\triangleright=[\\tau_i^\\triangleright]$ for the end of sequences.\n",
    "\n",
    "We consider the start and end of a sequence first.\n",
    "The *initial* sequence probabilities are given by\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\iota}^\\triangleleft_i & \\doteq &\n",
    "P(S_1=\\sigma_i\\mid M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M^+_T=\\triangleright)\n",
    "~=~\n",
    "\\frac{P(M^-_1=\\triangleleft)}{P(\\mathbf{y}_{1:T})}\n",
    "\\iota^\\triangleleft_i\\,b_{i,y_1}\\,\\beta^\\mathtt{multi}_{1:T}(i,\\triangleright)\\,.\n",
    "\\end{eqnarray}\n",
    "However, the *terminal* sequence probabilities are more difficult. We note that the posterior\n",
    "we want cannot be computed directly due to the Markov nature of the model, since\n",
    "\\begin{eqnarray}\n",
    "P(M^+_T=\\triangleright\\mid S_T=\\sigma_i,M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T})\n",
    "& = &\n",
    "\\frac{\n",
    "P(M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_T=\\sigma_i,M^+_T=\\triangleright)\n",
    "}{\n",
    "P(M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_T=\\sigma_i)\n",
    "}\n",
    "\\\\& = &\n",
    "\\frac{\n",
    "P(M^-_1=\\triangleleft)\\,\\alpha_{1:T}^\\mathtt{multi}(\\triangleleft,i)\\,\\tau^\\triangleright_i\n",
    "}{\n",
    "P(M^-_1=\\triangleleft)\\,\\alpha_{1:T}^\\mathtt{multi}(\\triangleleft,i)\n",
    "}\n",
    "~=~\\tau^\\triangleright_i\\,.\n",
    "\\end{eqnarray}\n",
    "Instead, we note that only the last leaf state $s_T$ in a complete sequence contributes to \n",
    "sequence termination, and all previous the leaf states $s_t$ for $t<T$ contribute to non-termination.\n",
    "Hence, we define the per-token leaf state posterior\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^\\mathtt{token}_{it} & \\doteq &\n",
    "P(S_t=\\sigma_i\\mid M^-_1=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}M^+_T=\\triangleright)\n",
    "\\\\& = &\n",
    "\\frac{P(M^-_1=\\triangleleft)\\,\n",
    "\\alpha^\\mathtt{multi}_{1:t}(\\triangleleft,i)\\,\\beta^\\mathtt{multi}_{t:T}(i,\\triangleright)\n",
    "}{P(\\mathbf{y}_{1:T})}\\,,\n",
    "\\end{eqnarray}\n",
    "and re-estimate \n",
    "$\\hat{\\tau}^\\triangleright_i\\doteq\\frac{\\hat{N}^\\mathtt{token}_{iT}}{\\hat{N}^\\mathtt{token}_{i\\cdot}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198faa0e",
   "metadata": {},
   "source": [
    "Lastly, we want re-estimate the *start-chunk* (initial) probability vector\n",
    "$\\boldsymbol{\\iota}^\\square=[\\iota^\\square_i]$, and the *end-chunk* (terminal) probability vector\n",
    "$\\boldsymbol{\\tau}^\\square=[\\tau_i^\\square]$.\n",
    "Note that the end of every chunk is followed by a *leaf-to-intermediate* state transition, which we have counted\n",
    "via $\\hat{N}^\\mathtt{up}_{ip}$. Additionally, within every chunk we have counted the *leaf-to-leaf* state\n",
    "transitions, namely $\\hat{N}^\\mathtt{leaf}_{ij}$, which do not terminate the chunk.\n",
    "Hence, the *terminal* chunk probabilities are re-estimated as\n",
    "$\\hat{\\tau}^\\square_i\\doteq\n",
    "\\frac{\\hat{N}^\\mathtt{up}_{i\\cdot}}\n",
    "{\\hat{N}^\\mathtt{up}_{i\\cdot}+\\hat{N}^\\mathtt{leaf}_{i\\cdot}}$.\n",
    "\n",
    "Similarly, $\\hat{N}^\\mathtt{down}_{pi}$ counts the expected number of \n",
    "*intermediate-to-leaf* state transitions that start each chunk except the first chunk of a sequence.\n",
    "The initial states of the first chunk have already been estimated via $\\hat{\\iota}^\\triangleleft_i$.\n",
    "Consequently, the *initial* chunk probabilities are re-estimated as\n",
    "$\\hat{\\iota}^\\square_i\\doteq\n",
    "\\frac{\\hat{\\iota}^\\square_i+\\hat{N}^\\mathtt{down}_{\\cdot i}}\n",
    "{\\hat{\\iota}^\\square_\\cdot+\\hat{N}^\\mathtt{down}_{\\cdot\\cdot}}$.\n",
    "\n",
    "we want re-estimate the *start-chunk* (initial) probability vector\n",
    "$\\boldsymbol{\\iota}=[\\iota_i]$, and the *end-chunk* (terminal) probability vector\n",
    "$\\boldsymbol{\\tau}=[\\tau_i]$.\n",
    "For the former quantity, recall that $\\hat{N}^\\mathtt{down}_{pi}$ counts the joint occurrences of \n",
    "leaf state \n",
    "$\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ at the start of a chunk and intermediate state \n",
    "$\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$ at the end of the previous chunk. Hence, it follows that\n",
    "$\\hat{\\iota}_i\\doteq\\frac{\\hat{N}^\\mathtt{down}_{\\cdot i}}{\\hat{N}^\\mathtt{down}_{\\cdot\\cdot}}$.\n",
    "Similarly, for the latter quantity, recall that $\\hat{N}_{ij}^\\mathtt{leaf}$ counts\n",
    "every non-terminating transition, and $\\hat{N}_{ip}^\\mathtt{up}$ counts every terminating transition.\n",
    "Hence, $\\hat{\\tau}_i\\doteq\\frac{\\hat{N}_{i\\cdot}^\\mathtt{up}}{\\hat{N}_{i\\cdot}^\\mathtt{leaf}+\\hat{N}_{i\\cdot}^\\mathtt{up}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b7a66",
   "metadata": {},
   "source": [
    "## Simplified Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b16f1",
   "metadata": {},
   "source": [
    "Having gone through the complicated details and assumptions of chunking in the\n",
    "[previous](#Chunking \"Section: Chunking\") section, let us now revisit the key ideas with the aim of simplifying the grammar $\\mathcal{G}$ still further.\n",
    "As before, we consider a finite set $\\mathcal{Y}=\\{\\nu_1,\\nu_2,\\ldots\\}$ of terminal tokens,\n",
    "and a finite set $\\mathcal{S}=\\{\\sigma_1,\\sigma_2,\\ldots\\}$ of non-terminal states.\n",
    "We also retain the set $\\mathcal{M}=\\{\\triangleleft,\\oplus,\\square,\\triangleright\\}$ of markers that denote the internal context of the process. \n",
    "\n",
    "However, we now simplify the sequence generation process as follows. For each complete sequence, let the process always start in context\n",
    "$M_0=\\triangleleft$ with probability $P(M_0=\\triangleleft)=1$. Let this context correspond to the\n",
    "chunking symbols $C_0=\\langle($, which means that the start of a sequence also opens the first chunk.\n",
    "Next, the process chooses some state $S_1=s_1$, generates token $Y_1=y_1$, and then transitions\n",
    "to context $M_1=m_1$. Iteratively, the process has context $M_{t-1}=m_{t-1}$, chooses state\n",
    "$S_t=s_t$, generates token $Y_t=y_t$, and then transitions to context $M_t=m_t$.\n",
    "Finally, for a complete sequence of length $|\\mathbf{y}|=T$, the process terminates with\n",
    "context $M_T=\\triangleright$. This corresponds to the chunking symbols $C_T=)\\rangle$, which means that the \n",
    "last chunk is closed at the end of a sequence.\n",
    "\n",
    "In the interior of a sequence, for $t=1,2,\\ldots,T-1$, the process has a choice of context, namely\n",
    "$M_t=\\oplus$ or $M_t=\\square$. The former context indicates that both the sequence and the current chunk will\n",
    "continue to the next token, with corresponding chunking symbols $C_t=][$. The latter context indicates that the \n",
    "current chunk will be closed and the sequence will continue to the next token in a new chunk, with\n",
    "corresponding chunking symbols $C_t=)($."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6df01",
   "metadata": {},
   "source": [
    "The chunking grammar $\\mathcal{G}$ is expressed by a Markov process that generates\n",
    "an arbitrary-length (but non-empty) complete sequence $\\mathbf{Y}_{1:T}=\\langle Y_1,\\ldots,Y_T\\rangle$, driven by the dependencies\n",
    "$\\overset{M_0}{\\rightarrow} S_1\\overset{M_1}{\\rightarrow} S_2\n",
    "\\overset{M_2}{\\rightarrow}\\cdots\\overset{M_{T-1}}{\\rightarrow}S_{T}\n",
    "\\overset{M_T}{\\rightarrow}$,\n",
    "with hidden states $\\mathbf{S}_{1:T}=(S_1,\\ldots,S_T)$ and hidden contexts\n",
    "$\\mathbf{M}_{0:T}=(M_0,\\ldots,M_{T})$. \n",
    "The grammar is thus comprised of distinct types of rules. For token $\\nu_m\\in\\mathcal{Y}$,\n",
    "context $\\kappa\\in\\mathcal{M}$, and states $\\sigma_i,\\sigma_j\\in\\mathcal{S}$, the types of rules are:\n",
    "\n",
    "1. *Context-to-state* transition rules of the form $\\overset{\\kappa}{\\rightarrow}\\sigma_i$ with probability\n",
    "$P(S_t=\\sigma_i\\mid M_{t-1}=\\kappa)\\doteq\\iota^\\kappa_i$.\n",
    "\n",
    "1. *State-to-context* transition rules of the form $\\sigma_i\\overset{\\kappa}{\\rightarrow}$\n",
    "with probability $P(M_t=\\kappa\\mid S_t=\\sigma_i)\\doteq\\tau^\\kappa_i$.\n",
    "\n",
    "1. *Token* generation rules of the form $\\sigma_i\\rightarrow\\nu_m$ with probability \n",
    "$P(Y_t=\\nu_m\\mid S_t=\\sigma_i)\\doteq b_{im}$.\n",
    "\n",
    "1. *State-to-state* transition rules of the form $\\sigma_i\\overset{\\kappa}{\\rightarrow}\\sigma_j$\n",
    "with probability \n",
    "$P(S_{t+1}=\\sigma_j,M_t=\\kappa\\mid S_t=\\sigma_i)=\n",
    "P(M_t=\\kappa\\mid S_t=\\sigma_i)\\,P(S_{t+1}=\\sigma_j\\mid S_t=\\sigma_i,M_t=\\kappa)$,\n",
    "where $P(S_{t+1}=\\sigma_j\\mid S_t=\\sigma_i,M_t=\\kappa)\\doteq a^\\kappa_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786665ed",
   "metadata": {},
   "source": [
    "Let $\\mathcal{R}$ be the set of rules in the grammar $\\mathcal{G}$, over all contexts\n",
    "$\\kappa\\in\\mathcal{M}$, states $\\sigma_i,\\sigma_j\\in\\mathcal{S}$, and tokens $\\nu_m\\in\\mathcal{Y}$.\n",
    "Why do we need so many rules? In practice, although the process always generates complete sequences, we might not observe the entire sequence $\\langle y_1,\\ldots,y_T\\rangle=\\langle Y_1,\\ldots,Y_T\\rangle$. Instead, we might have observed an incomplete sequence, e.g. $\\langle y_1,\\ldots,y_t]=\\langle Y_1,\\ldots,Y_t]$ or\n",
    "$[y_1,\\ldots,y_t\\rangle=[Y_{T-t+1},\\ldots,Y_T\\rangle$, or even a subsequence, e.g.\n",
    "$[y_1,\\ldots,y_t]=[Y_r,\\ldots,Y_{t+r-1}]$. Hence, we re-index the stochastic process above to locally match\n",
    "the observed sequence, rather than the true (but unknown) process. Consequently, we now have a choice of starting context $M_0$, internal context $M_t$, and ending context $M_T$, depending upon what we know of the observation process.\n",
    "\n",
    "However, note that some contexts do not make sense. Thus, at the start of an observed sequence we set\n",
    "$P(M_0=\\triangleright)=0$, since the current sequence will not have been observed if the process terminated\n",
    "beforehand. Similarly, the process cannot restart during a sequence, such that\n",
    "$P(M_t=\\triangleleft\\mid S_t)=0$.\n",
    "Additionally, only the contexts $M_t\\in\\mathcal{M}^+\\doteq\\{\\oplus,\\square\\}$ designate the \n",
    "continuation of a sequence, and thus\n",
    "we set $P(S_{t+1}\\mid S_t,M_t)=0$ for $M_t\\in\\mathcal{M}^-\\doteq\\{\\triangleleft,\\triangleright\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f1424",
   "metadata": {},
   "source": [
    "The joint model of the local process is therefore\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{s},\\mathbf{m},\\mathbf{y}\\mid\\mathcal{G}) & = &\n",
    "P(M_0=m_0)\\,P(S_1=s_1\\mid M_0=m_0)\\,\n",
    "\\\\&&{}\\times\n",
    "\\prod_{t=1}^{T-1}\\left\\{P(Y_t=y_t\\mid S_t=s_t)\\,P(M_t=m_t\\mid S_t=s_t)\\,\n",
    "P(S_{t+1}=s_{t+1}\\mid S_t=s_t,M_t=m_t)\\right\\}\\,\n",
    "\\\\&&{}\\times\n",
    "P(M_T=m_T\\mid S_T=s_T)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and the marginal probability of the sequence is\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}\\mid\\mathcal{G}) & = &\n",
    "\\sum_{\\mathbf{s}\\in\\mathcal{S}^T}\n",
    "\\sum_{\\mathbf{m}\\in\\mathcal{M}^{T+1}}\n",
    "P(\\mathbf{s},\\mathbf{m},\\mathbf{y}\\mid\\mathcal{G})\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4d9b7",
   "metadata": {},
   "source": [
    "### Forward-Backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581dfe10",
   "metadata": {},
   "source": [
    "In the [previous](#Chunking \"Section: Chunking\") formulation,\n",
    "we had difficulty succinctly expressing the difference between the start of a sequence\n",
    "with initial states $\\boldsymbol{\\iota}^\\triangleleft$,\n",
    "and the start of a chunk with initial states $\\boldsymbol{\\iota}^\\square$.\n",
    "Similarly, there was confusion between the end of a sequence with terminal probabilities\n",
    "$\\boldsymbol{\\tau}^\\triangleright$, and the end of a chunk with terminal probabilities $\\boldsymbol{\\tau}^\\square$.\n",
    "For convenience, let us now define a new, polymorphic marker symbol '$\\diamond$', which denotes\n",
    "$M_0=\\triangleleft$ at the start of a complete sequence $\\mathbf{y}_{1:T}$, \n",
    "$M_T=\\triangleright$ at the end of the sequence, and\n",
    "$M_t=\\square$ internally within the sequence. This correspondence is deterministic, and depends only upon the\n",
    "starting and ending positions of any chosen subsequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd045a",
   "metadata": {},
   "source": [
    "As shown in a [previous](#Chunks-and-multi-chunks \"Section: Chunks and multi-chunks\")\n",
    "section, the process permits a HMM-like view of a sequence by making chunks implicit.\n",
    "Hence, for an open multi-chunk that starts at position $r$, the *forward* probabilities are given by the definition\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}(i) & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},S_t=\\sigma_i\\mid M_{r-1}=\\diamond)\\,,\n",
    "\\end{eqnarray}\n",
    "which for $r=t$ reduces to\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{t:t}(i) & = &\n",
    "P(Y_t=y_t,S_t=\\sigma_i\\mid M_{t-1}=\\diamond)~=~\\iota^\\diamond\\,\\breve{b}_{it}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and for $r<t$ gives the recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}(i) & = &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}}\n",
    "\\alpha_{r:t-1}(j)\\,\\tilde{a}_{ji}\\,\\breve{b}_{it}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\tilde{a}_{ij} & \\doteq & \\sum_{m\\in\\mathcal{M}^+}\\tau^m_i\\,a^m_{ij}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac12a3",
   "metadata": {},
   "source": [
    "Likewise, for an open multi-chunk that ends at position $t$, the *backward* probabilities are given by the definition\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:t}(i) & \\doteq &\n",
    "P(\\mathbf{Y}_{r+1:t}=\\mathbf{y}_{r+1:t},M_t=\\diamond\\mid S_r=\\sigma_i)\\,,\n",
    "\\end{eqnarray}\n",
    "which for $r=t$ reduces to\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:t}(i) & = &\n",
    "P(M_t=\\diamond\\mid S_r=\\sigma_i)~=~\\tau^\\diamond\\,,\n",
    "\\end{eqnarray}\n",
    "and for $r<t$ gives the recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:t}(i) & = &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}}\n",
    "\\tilde{a}_{ij}\\,\\breve{b}_{j,r+1}\\,\\beta_{r+1:t}(j)\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f12ab",
   "metadata": {},
   "source": [
    "Note that if we neglect internal subsequences spanning $r:t$, i.e. we insist that either $r=1$ or $t=T$, then this formulation reduces to the standard forward-backward algorithm. Hence, the probability of a complete sequence $\\mathbf{y}=\\langle y_1,\\ldots,y_T\\rangle$ is given by\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}) & = & \n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}}\n",
    "\\alpha_{1:t}(i)\\,\\beta_{t:T}(i)\\,,\n",
    "\\end{eqnarray}\n",
    "for any $t=1,2,\\ldots,T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386d17f",
   "metadata": {},
   "source": [
    "### Inside-Outside algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59760d03",
   "metadata": {},
   "source": [
    "Whereas the forward-backward algorithm of the\n",
    "[previous](#Forward-Backward-algorithm \"Section: Forward-Backward algorithm\") section\n",
    "deliberately obscures the start and end of chunks, here we want to explicitly handle chunks, or at least multi-chunks.\n",
    "Hence, if $\\alpha_{r:t}(i)$ is the probability of an open multi-chunk starting at position $r$, then\n",
    "the corresponding probability of a closed multi-chunk is given by\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\alpha}_{r:t} & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M_t=\\diamond\\mid M_{r-1}=\\diamond)\n",
    "~=~\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}}\\alpha_{r:t}(i)\\,\\tau^\\diamond_i\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Recall that, by construction, an arbitrary chunk or multi-chunk has only a known start, and that by assumption\n",
    "the last state of the previous closed chunk is unknown. Hence, under our simplified model, once a multi-chunk is permanently closed, its the terminal state is of no further relevance to the adjacent multi-chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093cbf0",
   "metadata": {},
   "source": [
    "Note that since $\\bar{\\alpha}_{r:t}$ spans tokens $\\mathbf{y}_{r:t}$, these define *inner* probabilities, and their computation over all $1\\le r\\le t\\le T$ forms the *inside* pass.\n",
    "Consequently, the *outside* pass corresponds to computing the *outer* probabilities \n",
    "$\\bar{\\beta}_{r:t}$ that complete the rest of the sequence. We therefore define\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\beta}_{r:t} & \\doteq &\n",
    "P(M_0=\\triangleleft,\\mathbf{Y}_{1:r-1}=\\mathbf{y}_{1:r-1},M_{r-1}=\\diamond,\n",
    "\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},M_T=\\triangleright\n",
    "\\mid M_t=\\diamond)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that on the left for $r=1$ this reduces to\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\beta}_{1:t} & \\doteq &\n",
    "P(M_0=\\triangleleft,\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},M_T=\\triangleright\\mid M_t=\\diamond)\\,,\n",
    "\\end{eqnarray}\n",
    "and on the right for $t=T$ becomes\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\beta}_{r:T} & \\doteq &\n",
    "P(M_0=\\triangleleft,\\mathbf{Y}_{1:r-1}=\\mathbf{y}_{1:r-1},M_{r-1}=\\diamond\\mid \n",
    "M_T=\\triangleright)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c29860",
   "metadata": {},
   "source": [
    "Now, if $r>1$ then there is room on the left of the current multi-chunk to place an adjacent multi-chunk. Hence, for some position $s<r$, we adjoin the closed multi-chunk $\\bar{\\alpha}_{s:r-1}$, and what remains forms the outer probability $\\bar{\\beta}_{s:t}$.\n",
    "Likewise, if $t<T$ then there is room on the right of the current multi-chunk to place an adjacent multi-chunk. Hence, for some position $s>t$, we adjoin the closed multi-chunk $\\bar{\\alpha}_{t+1:s}$, and what remains forms the outer probability $\\bar{\\beta}_{r:s}$. Summing over all such adjacent multi-chunks gives rise to the\n",
    "recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\beta}_{r:t} & \\doteq &\n",
    "\\sum_{s=1}^{r-1}\\bar{\\alpha}_{s:r-1}\\,\\bar{\\beta}_{s:t}\n",
    "+\\sum_{s=t+1}^{T}\\bar{\\alpha}_{t+1:s}\\,\\bar{\\beta}_{t:s}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that now $\\bar{\\alpha}_{r:t}\\,\\bar{\\beta}_{r:t}$ gives the joint probability of the token sequence\n",
    "**and** the fact that a closed multi-chunk spans positions $r:t$, since\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\alpha}_{r:t}\\,\\bar{\\beta}_{r:t} & = &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M_t=\\diamond\\mid M_{r-1}=\\diamond)\\,\n",
    "P(M_0=\\triangleleft,\\mathbf{Y}_{1:r-1}=\\mathbf{y}_{1:r-1},M_{r-1}=\\diamond,\n",
    "\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},M_T=\\triangleright\n",
    "\\mid M_t=\\diamond)\n",
    "\\\\& = &\n",
    "P(M_0=\\triangleleft,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M_T=\\triangleright,M_{r-1}=\\diamond,M_t=\\diamond)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The marginal probability of the complete sequence $\\mathbf{y}_{1:T}$ is\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{y}_{1:T}) & \\doteq & \\bar{\\alpha}_{1:T}\\,\\bar{\\beta}_{1:T}\\,,\n",
    "\\end{eqnarray}\n",
    "due to the polymorphic nature of the marker '$\\diamond'$, and the fact that a complete sequence can always be partitioned into closed chunks.\n",
    "For incomplete sequences, one may replace\n",
    "'$\\triangleleft$' and/or '$\\triangleright$' by '$\\square$', as necessary, in the definition of \n",
    "$\\bar{\\beta}_{r:t}$. However, this assumes that the incomplete sequence can also be chunked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2cbbc",
   "metadata": {},
   "source": [
    "### An alternative formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21d0c4",
   "metadata": {},
   "source": [
    "In terms of the process depicted [earlier](#Simplified-Chunking \"Section: Simplified Chunking\"),\n",
    "note that each *complete-data* case $(\\mathbf{s},\\mathbf{m},\\mathbf{y})$ corresponds to a structure $T$, where,\n",
    "in grammatical terms, $T$ is a derivation or parse of the token sequence $\\mathbf{y}$.\n",
    "Hence, we restrict our attention to the set $\\mathcal{T}=\\mathcal{T}(\\mathbf{y})$\n",
    "of all such parses that are consistent with $\\mathbf{y}$.\n",
    "\n",
    "Next, we note that the Markov process takes the form of a graph,\n",
    "which can be re-expressed as the Bayesian network shown below.\n",
    "\n",
    "<img src=\"simple_chunking_grammar.png\" \n",
    "     title=\"Simplified chunking model of a sentence with context\" \n",
    "     width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796601e",
   "metadata": {},
   "source": [
    "Thus, each parse $T\\in\\mathcal{T}$ has a structural interpretation as a network (or graph) $G(T)$ of \n",
    "nodes (or vertices), where each \n",
    "node $v\\in G(T)$ has some designated context, state or token that conditionally depends on the current or\n",
    "previous context and/or state, denoted by $\\boldsymbol{\\pi}(v,T)$.\n",
    "Consequently, the conditional model has the form\n",
    "\\begin{eqnarray}\n",
    "P(T\\mid\\mathbf{y}) & = & \n",
    "\\frac{P(\\mathbf{s},\\mathbf{m},\\mathbf{y}\\mid\\mathcal{G})}{P(\\mathbf{y}\\mid\\mathcal{G})}\n",
    "~=~\\frac{1}{Z}\\prod_{v\\in G(T)} P(v\\mid\\boldsymbol{\\pi}(v,T))\\,,\n",
    "\\end{eqnarray}\n",
    "where we have normalised the distribution\n",
    "via the partition function $Z=Z(\\mathbf{y})\\doteq P(\\mathbf{y}\\mid\\mathcal{G})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697f616",
   "metadata": {},
   "source": [
    "Now, following [(Eisner)](#References \"Reference [4]: Inside-Outside and Forward-Backward algorithms are just backprop\"), for every rule $R\\in\\mathcal{R}$ we define $\\theta_R\\doteq\\ln P(R)$, such that these \n",
    "log-probabilities parameterise the grammar $\\mathcal{G}$. Next, we introduce the\n",
    "feature function $f_R:\\mathcal{T}\\mapsto\\mathbb{N}$ that counts the number of occurrences of rule\n",
    "$R$ in parse $T$. Hence, the conditional model now becomes\n",
    "\\begin{eqnarray}\n",
    "P(T\\mid\\mathbf{y}) & = & \\frac{1}{Z}\\prod_{R\\in\\mathcal{R}} P(R)^{f_R(T)}\n",
    "~=~\\frac{1}{Z}\\exp\\left\\{\\sum_{R\\in\\mathcal{R}}f_R(T)\\,\\theta_R\\right\\}\\,,\n",
    "\\end{eqnarray}\n",
    "with normaliser\n",
    "\\begin{eqnarray}\n",
    "Z & = & \\sum_{T\\in\\mathcal{T}}\\exp\\left\\{\\sum_{R\\in\\mathcal{R}}f_R(T)\\,\\theta_R\\right\\}\\,.\n",
    "\\end{eqnarray}\n",
    "It follows that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\ln Z}{\\partial\\theta_R} & = &\n",
    "\\frac{1}{Z}\\frac{\\partial Z}{\\partial\\theta_R}\n",
    "~=~\\frac{1}{Z}\\sum_{T\\in\\mathcal{T}}f_R(T)\\,\\exp\\left\\{\\sum_{R'\\in\\mathcal{R}}f_{R'}(T)\\,\\theta_{R'}\\right\\}\n",
    "\\\\& = & \\sum_{T\\in\\mathcal{T}}f_R(T)\\,P(T\\mid\\mathbf{y})\n",
    "~=~\\mathbb{E}_\\mathcal{T}[f_R(T)\\mid\\mathbf{y}]\\,.\n",
    "\\end{eqnarray}\n",
    "The last term gives the expected count $\\hat{N}_R$ of the number of times rule $R$ can appear across all possible parses of sequence $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1da3c5",
   "metadata": {},
   "source": [
    "Given this relation, \n",
    "(Eisner) goes on to show how automatic differentiation of $\\ln Z$ provides the update equations for\n",
    "computing $\\theta_R$. This is demonstrated by applying back-propagation to the inside algorithm to \n",
    "efficiently obtain the both the outside algorithm and rule count estimation. \n",
    "In particular, we have\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}_R & \\doteq & \\frac{\\partial\\ln Z}{\\partial\\theta_R}\n",
    "~=~\\frac{\\partial\\ln Z}{\\partial Z}\\,\\frac{\\partial Z}{\\partial P(R)}\\,\n",
    "\\frac{\\partial P(R)}{\\partial\\theta_R}~=~\\frac{P(R)}{Z}\\,\\frac{\\partial Z}{\\partial P(R)}\\,.\n",
    "\\end{eqnarray}\n",
    "Now, recall that the probability $P(R)$ of rule $R$ is assumed to be invariant to the substructure in which\n",
    "it occurs.\n",
    "Consequently, the back-propagation gradient $\\frac{\\partial Z}{\\partial P(R)}$ represents\n",
    "the marginalisation over all possible substructures in which rule $R$ can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3094d",
   "metadata": {},
   "source": [
    "## Hierarchical Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b9f85",
   "metadata": {},
   "source": [
    "Now that we have looked at [leaf-level](#Simplified-Chunking \"Section: Simplified Chunking\")\n",
    "chunking in detail, it is time to revisit \n",
    "[hierarchical](#Hierarchical-chunking \"Section: Hierarchical chunking\") \n",
    "chunking. This means we need once again to distinguish between leaf states $\\mathcal{S}_\\mathtt{leaf}$\n",
    "and intermediate states $\\mathcal{S}_\\mathtt{int}$.\n",
    "Essentially, each leaf-level chunk will be assigned an intermediate state, and \n",
    "the chunks will be combined by higher-level rules, according to the grammar $\\mathcal{G}$.\n",
    "\n",
    "[Previously](#Two-level-chunking \"Section: Two-level chunking\"), the intermediate state was assigned at the end of a chunk. However, this meant that the initial states of arbitrarily-placed chunks (as opposed to adjacent chunks) all shared the same fixed distribution. A viable alternative, therefore, is to assign the intermediate state\n",
    "at the start of a chunk and to condition the initial leaf state of each chunk on the chunk's intermediate state.\n",
    "As a consequence, we no longer need to distinguish between the start of the sequence and the start of a chunk\n",
    "at the leaf level, since this distinction will be handled by the higher-level grammar.\n",
    "This process is demonstrated by the example shown in the figure below.\n",
    "\n",
    "<img src=\"hierarchical_chunking.png\" \n",
    "     title=\"Top-down, hierarchical chunking model with sequential dependencies\" \n",
    "     width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3eb4c",
   "metadata": {},
   "source": [
    "In abbreviated form, the probability of this derivation is\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "P(\\triangleleft\\texttt{S}\\triangleright)\\,P(\\square\\texttt{NP},\\square\\texttt{VP}\\mid\\texttt{S})\\,\n",
    "\\\\&&\n",
    "P(\\texttt{D}\\mid\\square\\texttt{NP})\\,P(\\textit{The}\\mid\\texttt{D})\\,\n",
    "P(\\oplus\\texttt{J}\\mid\\texttt{D})\\,\\,P(\\textit{black}\\mid\\texttt{J})\\,\n",
    "P(\\oplus\\texttt{N}\\mid\\texttt{J})\\,\\,P(\\textit{cat}\\mid\\texttt{N})\\,P(\\square\\mid\\texttt{N})\\,\n",
    "\\\\&&\n",
    "P(\\texttt{V}\\mid\\square\\texttt{VP})\\,P(\\textit{purred}\\mid\\texttt{V})\\,P(\\square\\mid\\texttt{V})\\,\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fae321",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08618197",
   "metadata": {},
   "source": [
    "We now consider an open chunk that starts at position $r$ with intermediate state \n",
    "$S_{r:*}=\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$,\n",
    "and 'ends' at position $t$ with leaf state $S_t=\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$.\n",
    "The probability of this chunk is\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t}(p,i) & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},S_t=\\sigma_i\\mid M_{r-1}=\\square,S_{r:*}=\\sigma_p)\\,.\n",
    "\\end{eqnarray}\n",
    "The initial leaf state of the chunk is determined via\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{t:t}(p,i) & \\doteq &\n",
    "P(Y_t=y_t,S_t=\\sigma_i\\mid M_{t-1}=\\square,S_{t:*}=\\sigma_p)\n",
    "~=~d_{pi}\\,\\breve{b}_{it}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where now\n",
    "\\begin{eqnarray}\n",
    "d_{pi} & \\doteq & P(S_t=\\sigma_i\\mid M_{t-1}=\\square, S_{t:*}=\\sigma_p)\\,.\n",
    "\\end{eqnarray}\n",
    "The open chunk is then continued at the leaf level via the recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{r:t+1}(p,i) & \\doteq &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t}(p,j)\\,\\bar{\\tau}_j\\,c_{ji}\\,\\breve{b}_{i,t+1}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where once again $\\tau_i\\doteq\\tau^\\square_i$ and\n",
    "$\\bar{\\tau}_i=1-\\tau_i\\doteq\\tau^\\oplus_i$ with position-invariant probability\n",
    "\\begin{eqnarray}\n",
    "P(M_t=\\square\\mid S_t=\\sigma_i) & \\doteq & \\tau_i\\,,\n",
    "\\end{eqnarray}\n",
    "and now $c_{ij}\\doteq a^\\oplus_{ij}$ with position-invariant probability\n",
    "\\begin{eqnarray}\n",
    "P(S_{t+1}=\\sigma_j\\mid S_t=\\sigma_i,M_t=\\oplus) & \\doteq & c_{ij}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that all $\\alpha_{r:t}(p,i)$ comprise the *forward* probabilities of the *foward* pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edad17b",
   "metadata": {},
   "source": [
    "Eventually, every open chunk must be closed. The probability of a closed chunk is simply\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{r:t}(p) & \\doteq &\n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M_t=\\square\\mid M_{r-1}=\\square,S_{r:t}=\\sigma_p)\n",
    "~=~\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\\alpha_{r:t}(p,i)\\,\\tau_i\\,.\n",
    "\\end{eqnarray}\n",
    "Note that after closure of the chunk, the sequence generation process notionally returns to the (end of the)\n",
    "chunk's intermediate state. Hence, we no longer need to know the terminal leaf state of the chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55294ea",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e20b6",
   "metadata": {},
   "source": [
    "The converse of the [forward](#Forward-pass \"Section: Forward pass\")\n",
    "pass is a *backward* pass.\n",
    "Whereas the forward pass extends an open chunk on the right with some leaf state, say \n",
    "$\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$, the backward pass extends an open chunk on the left\n",
    "given the previous state $\\sigma_i$.\n",
    "Consequently, we define\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:t}(p,i) & \\doteq &\n",
    "P(\\mathbf{Y}_{r+1:t}=\\mathbf{y}_{r+1:t},M_t=\\square\\mid S_r=\\sigma_i,S_{*:t}=\\sigma_p)\\,,\n",
    "\\end{eqnarray}\n",
    "with \n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:t}(p,i) & = & P(M_t=\\square\\mid S_t=\\sigma_i) ~=~ \\tau_i\\,.\n",
    "\\end{eqnarray}\n",
    "For $r<t$, the recurrence relation is\n",
    "\\begin{eqnarray}\n",
    "\\beta_{r:t}(p,i) & \\doteq &\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\bar{\\tau}_i\\,c_{ij}\\,\\breve{b}_{j,r+1}\\,\\beta_{r+1:t}(p,j)\\,.\n",
    "\\end{eqnarray}\n",
    "Eventually, this open chunk will become closed on the left\n",
    "at some position $s\\le r$ with probability $\\alpha_{s:r}(p,i)$, and hence\n",
    "the general probability of a closed chunk is\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{s:t}(p) & \\doteq &\n",
    "P(\\mathbf{Y}_{s:t}=\\mathbf{y}_{s:t},M_t=\\square\\mid M_{s-1}=\\square,S_{s:t}=\\sigma_p)\n",
    "\\\\& = &\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{s:r}(p,i)\\,\\beta_{r:t}(p,i)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for every $s\\le r\\le t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862d41",
   "metadata": {},
   "source": [
    "### Inside pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f99a42",
   "metadata": {},
   "source": [
    "We recall that the (closed) chunk with state $S_{r:s}=\\sigma_p\\in\\mathcal{S}_\\mathtt{int}$ that spans tokens $\\mathbf{y}_{r:s}$ has *inner* probability $\\gamma_{r:s}(p)$. Furthermore, if that chunk is followed by an adjacent chunk, say\n",
    "$\\gamma_{s+1:t}(q)$ with intermediate state $\\sigma_q\\in\\mathcal{S}_\\mathtt{int}$, then the two chunks may be combined (via high-level rules) into a multi-chunk with state $S_{r:t}=\\sigma_w\\in\\mathcal{S}_\\mathtt{int}$, say, with probability $\\bar{\\gamma}_{r:t}(w)$, where the bar indicates a (closed) multi-chunk of one or more (closed) chunks.\n",
    "We define the probability of a multi-chunk to be\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\gamma}_{r:t}(w) & = & \n",
    "P(\\mathbf{Y}_{r:t}=\\mathbf{y}_{r:t},M_t=\\square\\mid M_{r-1}=\\square,S_{r:t}=\\sigma_w)\\,,\n",
    "\\end{eqnarray}\n",
    "which, ambiguously, has the same form as for a single chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b34862",
   "metadata": {},
   "source": [
    "In principle, there are many ways of defining how chunks may be combined. \n",
    "For example, we could define a *head-driven* \n",
    "grammar such that exactly one of states, $S_{r:s}=\\sigma_p$ or $S_{s+1:t}=\\sigma_q$, would be chosen as the overall *head* state $S_{r:t}$ of the combination. Each such combination would therefore represent a *dependency* where either *satellite* chunk $S_{r:s}$ attaches to head chunk $S_{s+1:t}$ on its right, or satellite chunk $S_{s+1:t}$ attaches to head chunk $S_{r:s}$ on its left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06ebb0",
   "metadata": {},
   "source": [
    "Alternatively, we could allow production rules, such as $n$-ary rules of the form $\\mathcal{S}_\\mathtt{int}\\rightarrow\\mathcal{S}_\\mathtt{int}^n$. \n",
    "For simplicity, and consistency with the usual context-free grammar in Chomksy normal form (CNF),\n",
    "we utilise binary rules of the form\n",
    "$\\mathcal{S}_\\mathtt{int}\\rightarrow\\mathcal{S}_\\mathtt{int}\\oplus\\mathcal{S}_\\mathtt{int}$,\n",
    "and unary rules of the form $\\mathcal{S}_\\mathtt{leaf}\\rightarrow\\mathcal{Y}$.\n",
    "However, we now need to also include additional unary rules of the form\n",
    "$\\mathcal{S}_\\mathtt{int}\\rightarrow\\mathcal{S}_\\mathtt{leaf}$ and\n",
    "$\\mathcal{S}_\\mathtt{leaf}\\rightarrow\\mathcal{S}_\\mathtt{leaf}$, \n",
    "such that the grammar $\\mathcal{G}$ no longer has the CNF property but\n",
    "is still context-free. Note that these latter chunking rules implicitly correspond to unconstrained \n",
    "$n$-ary rules of the form $\\mathcal{S}_\\mathtt{int}\\rightarrow\\mathcal{S}_\\mathtt{leaf}^n$\n",
    "that provide the additional sequential dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce45702",
   "metadata": {},
   "source": [
    "Consequently, if a multi-chunk spanning tokens $\\mathbf{y}_{r:t}$ has intermediate state $\\sigma_w\\in\\mathcal{S}_\\mathtt{int}$, then any dichotomous partitioning of the multi-chunk via\n",
    "some binary rule $\\sigma_w\\rightarrow\\sigma_p\\oplus\\sigma_q$ has position-invariant probability\n",
    "\\begin{eqnarray}\n",
    "P(S_{r:s}=\\sigma_p,S_{s+1:t}=\\sigma_q\\mid S_{r:t}=\\sigma_w)\n",
    "& \\doteq & P(\\sigma_w\\rightarrow\\sigma_p\\oplus\\sigma_q) ~\\doteq~ a_{wpq}\\,,\n",
    "\\end{eqnarray}\n",
    "for every $s=r,\\ldots,t-1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e53632",
   "metadata": {},
   "source": [
    "In general, we do not care about the internal structure of a multi-chunk. Hence, the *inside* pass\n",
    "sums over the probabilities of all internal chunking of a multi-chunk.\n",
    "Thus, appropriately modifying the model from a \n",
    "[previous](#Chunks-and-multi-chunks \"Section: Chunks and multi-chunks\")\n",
    "section, the *inner* probability of a multi-chunk is given by the recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\gamma}_{r:t}(w) & = & \n",
    "\\gamma_{r:t}(w) +\n",
    "\\sum_{s=r}^{t-1}\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\sum_{\\sigma_q\\in\\mathcal{S}_\\mathtt{int}}\n",
    "a_{wpq}\\,\n",
    "\\bar{\\gamma}_{r:s}(p)\\,\\gamma_{s+1:t}(q)\\,,\n",
    "\\end{eqnarray}\n",
    "for $r<t$, with\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\gamma}_{t:t}(w) & = & \\gamma_{t:t}(w)\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "This is very similar to the standard inside pass (with both $\\gamma$ and $\\bar{\\gamma}$ replaced by $\\beta$), except that whereas there two adjoining subparses make a bigger subparse, here two adjoining chunks make a multi-chunk, not a single chunk.\n",
    "Also note that here we are explcitly combining a multi-chunk with a single chunk (to its right), rather than a multi-chunk with a multi-chunk, since (as noted previously) marginalising over the latter would count some internal chunks multiple times.\n",
    "The extra leading term in the recurrence relation comes from the fact that a multi-chunk may also be comprised of a single chunk.\n",
    "\n",
    "Due to the similarity with the standard inside pass, we choose to also denote the probability of a multi-chunk via $\\bar{\\beta}_{r:t}(p)\\doteq\\bar{\\gamma}_{r:t}(p)$. This is not to be confused with the \n",
    "[backward](#Backward-pass \"Section: Backward pass\") \n",
    "probability $\\beta_{r:t}(p,i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10431659",
   "metadata": {},
   "source": [
    "As a quick example of the difference between a chunk and a multi-chunk, consider\n",
    "the sentence \"*The cat sat on the mat.*\", with possible\n",
    "chunking $[\\textit{The}\\oplus\\textit{cat}]_\\texttt{NP}[\\textit{sat}]_\\texttt{VP}[\\textit{on}\\oplus\\textit{the}\\oplus\\textit{mat}]_\\texttt{PP}$. However,\n",
    "an alternative chunking might be\n",
    "$[\\textit{The}\\oplus\\textit{cat}]_\\texttt{NP}[\\textit{sat}\\oplus\\textit{on}\\oplus\\textit{the}\\oplus\\textit{mat}]_\\texttt{VP}$.\n",
    "The single chunk $[\\textit{sat}\\oplus\\textit{on}\\oplus\\textit{the}\\oplus\\textit{mat}]_\\texttt{VP}$ \n",
    "is not the same as the multi-chunk\n",
    "$\\{[\\textit{sat}]_\\texttt{VP}[\\textit{on}\\oplus\\textit{the}\\oplus\\textit{mat}]_\\texttt{PP}\\}_\\texttt{VP}$,\n",
    "even though both span the same tokens, and have the same intermediate state. \n",
    "Furthermore,\n",
    "the probability $P(\\{[\\textit{sat}]_\\texttt{VP}[\\textit{on}\\oplus\\textit{the}\\oplus\\textit{mat}]_\\texttt{PP}\\}_\\texttt{VP})$ is only one\n",
    "way of contributing to the total multi-chunk probability $\\bar{\\bar{\\alpha}}_{3:6}(\\texttt{VP})$.\n",
    "However, the single chunk has probability $\\bar{\\alpha}_{3:6}(\\texttt{VP})=\n",
    "P([\\textit{sat}\\oplus\\textit{on}\\oplus\\textit{the}\\oplus\\textit{mat}]_\\texttt{VP})$ exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf565a0",
   "metadata": {},
   "source": [
    "### Outside pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993ae6e",
   "metadata": {},
   "source": [
    "Now, recall from the [previous](#Inside-pass \"Section: Inside pass\") section that\n",
    "a multi-chunk spanning tokens $\\mathbf{y}_{r:t}$ has inside probability $\\bar{\\beta}_{r:t}(p)$,\n",
    "marginalising over all inner structure.\n",
    "Hence, the remainder of the derivation of the sequence $\\mathbf{y}_{1:T}$ forms the *outer* structure with outer probability $\\bar{\\alpha}_{r:t}(p)$, defined as\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\alpha}_{r:t}(p) & \\doteq &\n",
    "P(\\mathbf{Y}_{1:r-1}=\\mathbf{y}_{1:r-1},M_{r-1}=\\square,S_{r:t}=\\sigma_p,\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "This is not to be confused with the [forward](#Forward-pass \"Section: Forward pass\") probability\n",
    "$\\alpha_{r:t}(p,i)$. As a direct consequence of this definition, we now obtain\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M_{r-1}=\\square,M_t=\\square) & = &\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\\bar{\\alpha}_{r:t}(p)\\,\\bar{\\beta}_{r:t}(p)\\,,\n",
    "\\end{eqnarray}\n",
    "and thus\n",
    "\\begin{eqnarray}\n",
    "P(\\bar{\\mathbf{y}}_{1:T}) & \\doteq & P(M_{0}=\\square,\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},M_T=\\square)\n",
    "~=~\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\\bar{\\alpha}_{1:T}(p)\\,\\bar{\\beta}_{1:T}(p)\\,,\n",
    "\\end{eqnarray}\n",
    "where we have now defined the logical proposition\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\mathbf{y}}_{1:T} & \\doteq & \n",
    "M_{0}=\\square\\wedge\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}\\wedge M_T=\\square\n",
    "\\end{eqnarray}\n",
    "for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167525e9",
   "metadata": {},
   "source": [
    "We now suppose that the entire sequence has some over-arching root state $\\sigma_p\\in\\mathcal{S}_\\mathtt{root}$\n",
    "with probability\n",
    "\\begin{eqnarray}\n",
    " P(S_{1:T}=\\sigma_p\\mid M_0=\\square) & \\doteq & \\iota_p\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the outside pass commences with the edge case\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\alpha}_{1:T}(p) & \\doteq & P(M_0=\\square,S_{1:T}=\\sigma_p)~=~\n",
    "P(M_0=\\square)\\,\\iota_p\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "We take $P(M_0=\\square)=1$ on the basis that we have observed either a complete sequence or at least\n",
    "a closed multi-chunk, since otherwise chunking a partial multi-chunk would be very difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbd2b7",
   "metadata": {},
   "source": [
    "The recurrence relation for the outside probabilities is derived by following the usual reasoning\n",
    "for the [inside-outside](#Inside-Outside-algorithm \"Section: Inside-Outside algorithm\") algorithm.\n",
    "In particular,\n",
    "for $r>1$ there exists some position $1\\le s<r$ giving rise to a closed chunk \n",
    "spanning tokens $\\mathbf{y}_{s:r-1}$ with probability $\\gamma_{s:r-1}(q)$.\n",
    "The chunk and the multi-chunk can now be combined via binary rules of the form\n",
    "$\\sigma_w\\rightarrow\\sigma_q\\oplus\\sigma_p$ to form a larger multi-chunk \n",
    "spanning tokens $\\mathbf{y}_{s:t}$ with inner probability\n",
    "$\\bar{\\beta}_{s:t}(w)$. Consequently, what remains is a smaller outer structure with\n",
    "outer probability $\\bar{\\alpha}_{s:t}(w)$.\n",
    "\n",
    "Similarly, for $t<T$ there exists some position $t<s\\le T$ leading to a closed chunk spanning tokens \n",
    "$\\mathbf{y}_{t+1:s}$ with probability $\\gamma_{t+1:s}(q)$.\n",
    "Hence, the multi-chunk and the chunk may be combined via binary rules of the form\n",
    "$\\sigma_w\\rightarrow\\sigma_p\\oplus\\sigma_q$ into a larger multi-chunk spanning tokens\n",
    "$\\mathbf{y}_{r:s}$ with inner probability $\\bar{\\beta}_{r:s}(w)$.\n",
    "What remains forms a smaller outer structure with probability $\\bar{\\alpha}_{r:s}(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d1645",
   "metadata": {},
   "source": [
    "Consequently, marginalising over all the possible ways of expanding a multi-chunk to either the left or to the right,\n",
    "the outer probabilities are computed via the recurrence relation\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\alpha}_{r:t}(p) & = & \n",
    "\\sum_{s=1}^{r-1}\n",
    "\\sum_{\\sigma_q\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\sum_{\\sigma_w\\in\\mathcal{S}_\\mathtt{int}}\n",
    "a_{wqp}\\,\\gamma_{s:r-1}(q)\\,\\bar{\\alpha}_{s:t}(w)\n",
    "+\n",
    "\\sum_{s=t+1}^{T}\n",
    "\\sum_{\\sigma_q\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\sum_{\\sigma_w\\in\\mathcal{S}_\\mathtt{int}}\n",
    "a_{wpq}\\,\\gamma_{t+1:s}(q)\\,\\bar{\\alpha}_{r:s}(w)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Once again, this resembles the standard outside algorithm (with $\\alpha$ instead of $\\bar{\\alpha}$,\n",
    "and $\\beta$ instead of $\\gamma$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65ee64",
   "metadata": {},
   "source": [
    "### Grammatical restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e755593",
   "metadata": {},
   "source": [
    "We noted via an example at the end of a [previous](#Inside-pass \"Section: Inside pass\") section that \n",
    "chunking ambiguity may arise due to the existence of nested rules, such as $\\texttt{VP}\\rightarrow\\texttt{VP}\\oplus\\texttt{PP}$. One possible way of avoiding such situations is to label\n",
    "each state with an explicit role, e.g. $\\texttt{VP}_\\mathtt{bin}\\rightarrow\\texttt{VP}_\\mathtt{chunk}\\oplus\\texttt{PP}_\\mathtt{chunk}$,\n",
    "such that $\\texttt{VP}_\\mathtt{bin}\\neq\\texttt{VP}_\\mathtt{chunk}$.\n",
    "More generally, such role labelling corresponds to the separation of intermediate states \n",
    "$\\mathcal{S}_\\mathtt{int}$ \n",
    "into states $\\mathcal{S}_\\mathtt{bin}$ that may appear at the head of binary rules,\n",
    "and other states $\\mathcal{S}_\\mathtt{chunk}$ that may produce leaf states $\\mathcal{S}_\\mathtt{leaf}$,\n",
    "such that $\\mathcal{S}_\\mathtt{int}=\\mathcal{S}_\\mathtt{bin}\\cup\\mathcal{S}_\\mathtt{chunk}$.\n",
    "The binary rules would therefore take the form $\\mathcal{S}_\\mathtt{bin}\\rightarrow\\left(\\mathcal{S}_\\mathtt{bin}\\cup\\mathcal{S}_\\mathtt{chunk}\\right)^2$,\n",
    "and the non-token unary rules would take the form $\\mathcal{S}_\\mathtt{chunk}\\rightarrow\\mathcal{S}_\\mathtt{leaf}$,\n",
    "e.g. $\\texttt{VP}_\\mathtt{chunk}\\rightarrow\\texttt{V}_\\mathtt{leaf}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb23827",
   "metadata": {},
   "source": [
    "Note that we do not necessarily require\n",
    "that $\\mathcal{S}_\\mathtt{bin}\\cap\\mathcal{S}_\\mathtt{chunk}=\\emptyset$,\n",
    "just as we do not require\n",
    "that $\\mathcal{S}_\\mathtt{int}\\cap\\mathcal{S}_\\mathtt{leaf}=\\emptyset$.\n",
    "However, the existence of unary rules of the form\n",
    "$\\mathcal{S}_\\mathtt{chunk}\\rightarrow\\mathcal{S}_\\mathtt{leaf}$ in the grammar $\\mathcal{G}$\n",
    "implies a degree of separation between leaf states and intermediate states, such that\n",
    "the grammatical restriction $\\mathcal{S}_\\mathtt{int}\\cap\\mathcal{S}_\\mathtt{leaf}=\\emptyset$\n",
    "would be justified. Note, however, that the existence of single-token sequences precludes\n",
    "the exclusion $\\mathcal{S}_\\mathtt{root}\\cap\\mathcal{S}_\\mathtt{int}=\\emptyset$, although it\n",
    "does not prevent choosing $\\left|\\mathcal{S}_\\mathtt{root}\\right|=1$.\n",
    "\n",
    "Such restrictions on the grammar, specifically mutual exclusions between subsets of states, would typically be imposed by the explcit setting of zero-valued\n",
    "probabilities (known as *structural zeros*) within the corresponding conditional probability tables.\n",
    "Structural zeros are distinct from *estimated zeros* caused by a lack of training data, and hence\n",
    "[parameter estimation](#Parameter-estimation \"Section: Parameter estimation\")\n",
    "should typically allow non-zero prior probabilites at all places other than structural zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01407c3",
   "metadata": {},
   "source": [
    "### Parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a90f84",
   "metadata": {},
   "source": [
    "The hierarchical chunking grammar $\\mathcal{G}$ is now parameterised by a collection of conditional probability tables. The *chunk combination* rules are specified by the tensor $\\mathbf{A}=[a_{wpq}]$, for\n",
    "$\\sigma_w\\in\\mathcal{S}_\\mathtt{bin}$ and $\\sigma_p,\\sigma_q\\in\\mathcal{S}_\\mathtt{bin}\\cup\\mathcal{S}_\\mathtt{chunk}$.\n",
    "The *token generation* rules are specified by the matrix $\\mathbf{B}=[b_{im}]$, for\n",
    "$\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$ and $\\nu_m\\in\\mathcal{Y}$.\n",
    "The *chunk transition* rules are specified by the matrix $\\mathbf{C}=[c_{ij}]$, for\n",
    "$\\sigma_i,\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}$.\n",
    "The *chunk initiation* rules are specified by the matrix $\\mathbf{D}=[d_{pi}]$, for\n",
    "$\\sigma_p\\in\\mathcal{S}_\\mathtt{chunk}$ and $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$.\n",
    "Finally, the *chunk termination* rules are specified by the vector $\\boldsymbol{\\tau}=[\\tau_i]$, for\n",
    "$\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$, and the *sequence initiation* rules are specified by the vector\n",
    "$\\boldsymbol{\\iota}=[\\iota_p]$, for $\\sigma_p\\in\\mathcal{S}_\\mathtt{root}$.\n",
    "\n",
    "The maximum likelihood (ML) estimates of these probabilities are obtained via iterations of the\n",
    "expectation-maximisation (EM) procedure. The individual estimates of the rule probabilities are obtained as normalisations of the expected joint counts of each rule, namely\n",
    "\\begin{eqnarray}\n",
    "&&\n",
    "\\hat{a}_{wpq}~=~\\frac{\\hat{N}^A_{wpq}}{\\hat{N}^A_{w\\cdot\\cdot}}\\,,\n",
    "\\hat{b}_{im}~=~\\frac{\\hat{N}^B_{im}}{\\hat{N}^B_{i\\cdot}}\\,,\n",
    "\\hat{c}_{ij}~=~\\frac{\\hat{N}^C_{ij}}{\\hat{N}^C_{i\\cdot}}\\,,\n",
    "\\hat{d}_{pi}~=~\\frac{\\hat{N}^D_{pi}}{\\hat{N}^D_{p\\cdot}}\\,,\n",
    "\\hat{\\tau}_{i}~=~\\frac{\\hat{N}^\\square_{i}}{\\hat{N}^\\square_i+\\hat{N}^\\oplus_i}\\,,\n",
    "\\hat{\\iota}_{p}~=~\\frac{\\hat{N}^\\triangleleft_{p}}{\\hat{N}^\\triangleleft_{\\cdot}}\\,,\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that, in general, these counts may be summed over all sequences in the training corpus, and may also\n",
    "be initialised with prior counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010c840",
   "metadata": {},
   "source": [
    "As explained briefly in a\n",
    "[previous](#An-alternative-formulation \"Section: An alternative formulation\") section,\n",
    "to compute the expected count $\\hat{N}_R$ of each rule $R\\in\\mathcal{R}$ for a given sequence $\\mathbf{y}$, we\n",
    "essentially count the number $f_R(T)$ of times rule $R$ appears in each parse $T$, \n",
    "weight this count by the conditional probability $P(T\\mid\\mathbf{y})$ of the parse, and sum these weighted counts over every possible parse $T\\in\\mathcal{T}(\\mathbf{y})$. \n",
    "More traditionally, we may (loosely speaking) enumerate each distinct parse structure $S$ (which does not specify the states of the nodes), and for each such $S$ compute the conditonal probability $P(S,R\\mid\\mathbf{y})$ of the rule $R$ (which does specify the states) occuring within that structure. The sum of these conditonal probabilities over all structures then gives the expected count as $\\hat{N}_R=P(R\\mid\\mathbf{y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efb69b",
   "metadata": {},
   "source": [
    "Thus, for the chunk combination rule $R^A_{wpq}: \\sigma_w\\rightarrow\\sigma_p\\oplus\\sigma_q$ we have\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^A_{wpq} & = &\n",
    "\\frac{1}{Z}\n",
    "\\sum_{r=1}^{T-1}\\sum_{t=r+1}^{T}\\sum_{s=r}^{t-1}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},S_{r:t}=\\sigma_w,S_{r:s}=\\sigma_p,S_{s+1:t}=\\sigma_q)\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "Z & \\doteq & P(\\bar{\\mathbf{y}}_{1:T})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Now, from the [inside](#Inside-pass \"Section: Inside pass\") pass, the innards of a multi-chunk comprised of\n",
    "two or more chunks may be exposed via\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\beta}_{r:t}(w) - \\gamma_{r:t}(w) & = &\n",
    "\\sum_{s=r}^{t-1}\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\sum_{\\sigma_q\\in\\mathcal{S}_\\mathtt{int}}\n",
    "a_{wpq}\\,\n",
    "\\bar{\\beta}_{r:s}(p)\\,\\gamma_{s+1:t}(q)\\,,\n",
    "\\end{eqnarray}\n",
    "Furthermore, the inner probability $\\bar{\\beta}_{r:t}(w)$ has corresponding outer probability \n",
    "$\\bar{\\alpha}_{r:t}(w)$ that completes the derivation.\n",
    "Consequently, the joint probability factors as\n",
    "\\begin{eqnarray}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},S_{r:t}=\\sigma_w,S_{r:s}=\\sigma_p,S_{s+1:t}=\\sigma_q)\n",
    "& = &\n",
    "a_{wpq}\\,\\bar{\\beta}_{r:s}(p)\\,\\gamma_{s+1:t}(q)\\,\\bar{\\alpha}_{r:t}(w)\\,,\n",
    "\\end{eqnarray}\n",
    "giving\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^A_{wpq} & = &\n",
    "\\frac{a_{wpq}}{Z}\n",
    "\\sum_{r=1}^{T-1}\\sum_{t=r+1}^{T}\\sum_{s=r}^{t-1}\n",
    "\\bar{\\beta}_{r:s}(p)\\,\\gamma_{s+1:t}(q)\\,\\bar{\\alpha}_{r:t}(w)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f9667",
   "metadata": {},
   "source": [
    "Similarly, the within-chunk transition rule $R^C_{ij}:\\sigma_i\\overset{\\tiny\\oplus}{\\rightarrow}\\sigma_j$ has expected count\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^C_{ij} & = & \\frac{1}{Z}\n",
    "\\sum_{t=1}^{T-1}\n",
    "P(\\bar{\\mathbf{y}},S_t=\\sigma_i,M_t=\\oplus,S_{t+1}=\\sigma_j)\\,.\n",
    "\\end{eqnarray}\n",
    "Now, from the [backward](#Backward-pass \"Section: Backward pass\") pass, we may expose the innards of a closed chunk via\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{r:s}(p) & = &\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t}(p,i)\\,\\bar{\\tau}_i\\,c_{ij}\\,\\breve{b}_{j,t+1}\\,\\beta_{t+1:s}(p,j)\\,,\n",
    "\\end{eqnarray}\n",
    "for $r<s$. Next, since the chunk determines an inner probability, we close the derivation with outer\n",
    "probability $\\bar{\\alpha}_{r:s}(p)$, and then marginalise across the structure of the chunk.\n",
    "Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^C_{ij} & = & \\frac{c_{ij}}{Z}\n",
    "\\sum_{r=1}^{T-1}\n",
    "\\sum_{s=r+1}^{T}\n",
    "\\sum_{t=r}^{s-1}\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\alpha_{r:t}(p,i)\\,\\bar{\\tau}_i\\,\\breve{b}_{j,t+1}\\,\\beta_{t+1:s}(p,j)\\,\\bar{\\alpha}_{r:s}(p)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d79af3",
   "metadata": {},
   "source": [
    "Next, the chunk initiation rule $R^D_{pi}: \\sigma_p\\overset{\\square}{\\rightarrow}\\sigma_i$\n",
    "has expected count\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^D_{pi} & = & \\frac{1}{Z}\n",
    "\\sum_{t=1}^{T}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},M_{t-1}=\\square,S_{t:*}=\\sigma_p,S_t=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "Now, from the both the [forward](#Forward-pass \"Section: Forward pass\")\n",
    "and  [backward](#Backward-pass \"Section: Backward pass\") \n",
    "passes, we may expose the start of a closed chunk via\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{t:s}(p) & = & \n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "d_{pi}\\,\\breve{b}_{it}\\,\\beta_{t:s}(p,i)\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, we obtain the expected count\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^D_{pi} & = & \\frac{d_{pi}}{Z}\n",
    "\\sum_{t=1}^{T}\n",
    "\\sum_{s=t}^{T}\n",
    "\\breve{b}_{it}\\,\\beta_{t:s}(p,i)\\,\\bar{\\alpha}_{t:s}(p)\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e273e3",
   "metadata": {},
   "source": [
    "Similarly, the token generation rule $R^B_{im}: \\sigma_i\\rightarrow\\nu_m$ has expected count\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^B_{im} & = & \\frac{1}{Z}\n",
    "\\sum_{t=1}^{T}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},S_t=\\sigma_i,Y_t=\\nu_m)\n",
    "~=~\\frac{1}{Z}\\sum_{t=1}^{T}\\delta(y_t=\\nu_m)\\,P(\\bar{\\mathbf{y}}_{1:T},S_t=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "Once again, the leaf state $S_t$ occurs within an arbitrary chunk with span $r\\le t\\le s$.\n",
    "For $r=t$, we have\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{t:s}(p) & = & \n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "d_{pi}\\,\\breve{b}_{it}\\,\\beta_{t:s}(p,i)\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, for $r<t$, we\n",
    "have\n",
    "\\begin{eqnarray}\n",
    "\\gamma_{r:s}(p) & = & \n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t-1}(p,j)\\,\\bar{\\tau}_j\\,c_{ji}\\,\\breve{b}_{it}\\,\\beta_{t:s}(p,i)\n",
    "\\\\& = &\n",
    "\\sum_{\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\frac{\\alpha_{r:t}(p,i)}{\\breve{b}_{it}}\\,\\breve{b}_{it}\\,\\beta_{t:s}(p,i)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the expected count is\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^B_{im} & = & \\frac{b_{im}}{Z}\n",
    "\\sum_{t=1}^{T}\n",
    "\\delta(y_t=\\nu_m)\n",
    "\\sum_{s=t}^{T}\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\left\\{\n",
    "d_{pi}\\,\\bar{\\alpha}_{t:s}(p)\n",
    "+\\sum_{r=1}^{t-1}\n",
    "\\frac{\\alpha_{r:t}(p,i)}{\\breve{b}_{it}}\n",
    "\\,\\bar{\\alpha}_{r:s}(p)\n",
    "\\right\\}\n",
    "\\,\\beta_{t:s}(p,i)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, we may simply use the fact that\n",
    "\\begin{eqnarray}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},S_t=\\sigma_i) & = &\n",
    "\\sum_{r=1}^{t}\n",
    "\\sum_{s=t}^T\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\alpha_{r:t}(p,i)\\,\\beta_{t:s}(p,i)\\,\\bar{\\alpha}_{r:s}(p)\\,,\n",
    "\\end{eqnarray}\n",
    "which does not expose the prior probability $b_{im}$ as a back-propagation factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b645a3c0",
   "metadata": {},
   "source": [
    "The chunk termination rule $R^\\square_{i}:\\sigma_i\\overset{\\square}{\\rightarrow}$ requires expected counts\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^\\square_{i} & = & \\frac{1}{Z}\n",
    "\\sum_{t=1}^{T}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},S_t=\\sigma_i,M_t=\\square)\n",
    "\\\\& = &\n",
    "\\frac{\\tau_i}{Z}\n",
    "\\sum_{r=1}^{T}\\sum_{t=r}^{T}\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\alpha_{r:t}(p,i)\\,\\bar{\\alpha}_{r:t}(p)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^\\oplus_{i} & = & \\frac{1}{Z}\n",
    "\\sum_{t=1}^{T-1}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},S_t=\\sigma_i,M_t=\\oplus)\n",
    "\\\\& = &\n",
    "\\frac{\\bar{\\tau}_i}{Z}\n",
    "\\sum_{r=1}^{T-1}\n",
    "\\sum_{s=r+1}^{T}\n",
    "\\sum_{t=r}^{s-1}\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\sum_{\\sigma_j\\in\\mathcal{S}_\\mathtt{leaf}}\n",
    "\\alpha_{r:t}(p,i)\\,c_{ij}\\,\\breve{b}_{j,t+1}\\,\\beta_{t+1:s}(p,j)\\,\n",
    "\\bar{\\alpha}_{r:s}(p)\n",
    "\\\\& = &\n",
    "\\frac{1}{Z}\n",
    "\\sum_{r=1}^{T-1}\n",
    "\\sum_{s=r+1}^{T}\n",
    "\\sum_{t=r}^{s-1}\n",
    "\\sum_{\\sigma_p\\in\\mathcal{S}_\\mathtt{int}}\n",
    "\\alpha_{r:t}(p,i)\\,\\beta_{t:s}(p,i)\\,\n",
    "\\bar{\\alpha}_{r:s}(p)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where the last expression does not expose the prior porability $\\bar{\\tau}_i$ as a\n",
    "back-propagation term.\n",
    "Alternatively, we recall that\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^C_{ij} & = & \\frac{1}{Z}\n",
    "\\sum_{t=1}^{T-1}\n",
    "P(\\bar{\\mathbf{y}},S_t=\\sigma_i,M_t=\\oplus,S_{t+1}=\\sigma_j)\\,,\n",
    "\\end{eqnarray}\n",
    "and thus $\\hat{N}^\\oplus_{i}=\\hat{N}^C_{i\\cdot}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c0af6",
   "metadata": {},
   "source": [
    "Finally, the sequence initiation rule $R^\\triangleleft_{p}:\\overset{\\triangleleft}{\\rightarrow}\\sigma_p$ \n",
    "has expected count\n",
    "\\begin{eqnarray}\n",
    "\\hat{N}^\\triangleleft_{p} & = & \\frac{1}{Z}\n",
    "P(\\bar{\\mathbf{y}}_{1:T},S_{1:T}=\\sigma_p)\n",
    "~=~\\frac{\\iota_p}{Z}P(M_0=\\square)\\,\\bar{\\beta}_{1:T}(p)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499bee1",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6b8f2",
   "metadata": {},
   "source": [
    "[1a] J. Kupiec (1992): \"*Robust part-of-speech tagging using a hidden Markov model*\", Computer speech & language 6(3): 225242.\n",
    "\n",
    "[1b] J. Kupiec (1992): \"*An Algorithm for Estimating the Parameters of Unrestricted Hidden Stochastic Context-Free Grammars*\", COLING 1992 Vol. 1.\n",
    "\n",
    "[2] S. Fine, Y. Singer, and N. Tishby (1998) \"*The Hierarchical Hidden Markov Model: Analysis and Applications*\", Machine Learning 32. \n",
    "[(PDF)](https://link.springer.com/content/pdf/10.1023/A:1007469218079.pdf \"springer.com\")\n",
    "\n",
    "[3] H.H. Bui, Q. Phung and S. Venkatesh (2004) \"*Hierarchical Hidden Markov Models with General State Hierarchy*\", AAAI-04 (National Conference on Artificial Intelligence).\n",
    "[(PDF)](https://www.aaai.org/Papers/AAAI/2004/AAAI04-052.pdf \"aaai.org\")\n",
    "\n",
    "[4] J. Eisner (2016): \"*Inside-Outside and Forward-Backward algorithms are just backprop*\",\n",
    "Proc. Workshop on Structured Prediction for NLP.\n",
    "[(PDF)](https://aclanthology.org/W16-5901.pdf \"aclanthology.org\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
