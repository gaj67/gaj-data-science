{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddd0507",
   "metadata": {},
   "source": [
    "# Appendix C: Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f7904",
   "metadata": {},
   "source": [
    "The purpose of this appendix is to provide an introduction from first principles to the mechanics of generalised linear models (GLMs) for regression,\n",
    "first introduced by Nelder and Wedderburn [[1]](#Citations \"Citation [1]: Generalized Linear Models\"). \n",
    "However, note that traditional GLM theory was derived for distributions of a single parameter (possibly with a constant \n",
    "hyper-parameter defining variance or dispersion).\n",
    "For distributions of multiple parameters, more careful handling is required, which we derive here.\n",
    "We also consider generalised nonlinear regression and its relationship to least-squares fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d2be0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588e123",
   "metadata": {},
   "source": [
    "### Probability distribution functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8caa0",
   "metadata": {},
   "source": [
    "We consider either a multi-dimensional or a scalar (i.e. uni-dimensional) variate $X$ on domain $\\mathcal{X}$. Let $X$ have an underlying probability distribution function (PDF) $p(X\\mid\\boldsymbol{\\theta})$ governed by a scalar or vector parameter $\\boldsymbol{\\theta}$. Then $p$ must satisfy the constraints of non-negativity:\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})~\\ge~0 && \\forall\\mathbf{x}\\in\\mathcal{X}\\,,\n",
    "\\end{eqnarray}\n",
    "and a total probability of unity:\n",
    "\\begin{eqnarray}\n",
    "\\int_\\mathcal{X}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}| & = & 1\\,,\n",
    "\\end{eqnarray}\n",
    "where $|d\\mathbf{x}|$ is taken to be an infinitesimal volume or length in $\\mathcal{X}$. Note that for discrete variates the constraint\n",
    "is instead\n",
    "\\begin{eqnarray}\n",
    "\\sum_{\\mathbf{x}\\in\\mathcal{X}}p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) & = & 1\\,.\n",
    "\\end{eqnarray}\n",
    "We shall henceforth assume continuous variates for convenience, but the resulting derivations will also hold \n",
    "in discretes case by replacing integration with summation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dd798",
   "metadata": {},
   "source": [
    "When considered as general functions, PDFs have a variety of additional properties and constraints. For instance, for a continuous distribution, the integral represents the area under the curve. As a consequence of the non-negativity constraint, this places limits on the values of $p(\\mathbf{x}\\mid\\boldsymbol{\\theta})$. For example, if the domain $\\mathcal{X}$ has no finite upper bound, then $p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) \\rightarrow 0$ as $\\mathbf{x}\\rightarrow\\infty$. Similarly,\n",
    "$p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) \\rightarrow 0$ as $\\mathbf{x}\\rightarrow -\\infty$ if $\\mathcal{X}$\n",
    "does not have a finite lower bound. Similar conditions hold for spatial derivatives with respect to $\\mathbf{x}$ at the extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c11338",
   "metadata": {},
   "source": [
    "Consider now derivatives with respect to the parameter $\\boldsymbol{\\theta}$, denoted by the gradient vector operator\n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\doteq\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}$, which we take to be a column vector. Similarly, second derivatives are denoted by the *Hessian* matrix operator\n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}\\doteq\\frac{\\partial^2}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}^{T}}$.\n",
    "[Later](#Parameter-transformations \"Section: Parameter transformations\") \n",
    "we shall also require the first and second derivatives with respect to some transformation\n",
    "$\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$ of the parameters, denoted\n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}$ and \n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}$, respectively.\n",
    "In general, unless we specifically need to distinguish between these two cases, we may drop the subscript and assume the results hold for all parameterisations.\n",
    "\n",
    "Now, considering the total probability constraint above, since the derivatives are with respect to\n",
    "$\\boldsymbol{\\theta}$ or $\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$ and not $\\mathbf{x}$, it follows that\n",
    "taking first derivatives of both sides gives\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\int_\\mathcal{X}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}| & = &\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla} p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\\mathbf{0}\\,,\n",
    "\\end{eqnarray}\n",
    "and taking second derivatives gives\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}\\int_\\mathcal{X}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}| & = &\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T} p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\\mathbf{O}\\,,\n",
    "\\end{eqnarray}\n",
    "where we let $\\mathbf{O}$ denote an appropriately dimensioned zero matrix, as distinct from the zero (column) vector\n",
    "denoted by $\\mathbf{0}$.\n",
    "We shall use these results in the\n",
    "[next](#Expectations-and-log-likelihoods \"Section: Log-likelihoods and expectations\") section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619a3cb",
   "metadata": {},
   "source": [
    "Note that there are various fields of stochatic modelling, such as Maximum Entropy, that require use of the general properties of PDFs in order to construct specific PDFs that fit given theoretical or practical requirements. For the rest of this doccument, however, we shall assume that the form of the PDF has been specified in advance. The properties we require then relate instead to fitting the PDF to observed data, e.g. via regression modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe3393",
   "metadata": {},
   "source": [
    "### Expectations and log-likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dee07",
   "metadata": {},
   "source": [
    "We assume the \n",
    "[*law of the unconscious statistician*](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician \"Wikipedia: LOTUS\"), and take the expectation of an arbitrary function $\\mathbf{f}(X, \\boldsymbol{\\theta})$ to be given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_X\\left[\\mathbf{f}(X, \\boldsymbol{\\theta})\\mid\\boldsymbol{\\theta}\\right] & \\doteq & \n",
    "\\int_\\mathcal{X}\\mathbf{f}(\\mathbf{x}, \\boldsymbol{\\theta})\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that in general we may drop the subscript when it is clear with respect to which variate we are taking the expectation.\n",
    "Also note that since the expectation is the weighted mean of the function $\\mathbf{f}$, we often denote this for convenience  as\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_\\mathbf{f}(\\boldsymbol{\\theta}) & \\doteq & \\mathbb{E}\\left[\\mathbf{f}\\mid\\boldsymbol{\\theta}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, note that function $\\mathbf{f}$ may in general be scalar, vector, matrix or even tensor valued.\n",
    "However, we shall typically assume, without loss of generality, some scalar function $f$ (unless otherwise stated), since the expectation of a vector is a vector of scalar expectations, and likewise for matrices and tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2650fd3",
   "metadata": {},
   "source": [
    "Thus, taking the gradient of the expectation, we see that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\,\\mathbb{E}\\left[f\\mid\\boldsymbol{\\theta}\\right] & = & \n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "+\n",
    "\\int_\\mathcal{X}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\& = &\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "+\n",
    "\\int_\\mathcal{X}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}\\ln p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}f\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\n",
    "\\mathbb{E}\\left[f\\,\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where, for convenience, we have defined the log-likelihood $L$ as\n",
    "\\begin{eqnarray}\n",
    "L(\\boldsymbol{\\theta}; X) & \\doteq & \\ln p(X\\mid\\boldsymbol{\\theta})\\,.\n",
    "\\end{eqnarray}\n",
    "Note that if we instead used a vector function $\\mathbf{f}$, then we would have a choice of either the scalar gradient\n",
    "$\\boldsymbol{\\nabla}^T\\mathbf{f}$ or the matrix gradient $\\boldsymbol{\\nabla}\\mathbf{f}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4e016",
   "metadata": {},
   "source": [
    "Suppose now, as an example, that we take the constant function \n",
    "$f\\equiv 1~\\Rightarrow\\boldsymbol{\\nabla}f\\equiv\\mathbf{0}$.\n",
    "Then we immediately deduce that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right] & = & \\mathbf{0}\\,.\n",
    "\\end{eqnarray}\n",
    "This is one of the useful\n",
    " results from Kendall and Stuart [[2]](#Citations \"Citation [2]: The Advanced Theory of Statistics\").\n",
    "It is of interest to derive this result direcly.\n",
    "We begin by taking the gradient of $L$, namely\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}L & = & \n",
    "\\boldsymbol{\\nabla}\\ln p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "~=~\\frac{\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, taking the expectation of the gradient, we therefore obtain\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\int_\\mathcal{X}\n",
    "\\frac{\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\\mathbf{0}\\,,\n",
    "\\end{eqnarray}\n",
    "as before. The last part follows from the\n",
    "[previous](#Probability-distribution-functions \"Section: Probability distribution functions\") section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3d9f8",
   "metadata": {},
   "source": [
    "Similarly, taking the Hessian of $L$ gives\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L & = & \n",
    "\\boldsymbol{\\nabla}\\left\\{\n",
    "\\frac{\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\right\\}\n",
    "~=~\\frac{\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "}\n",
    "{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})^2}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, taking the expectation results in\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\int_\\mathcal{X}\n",
    "\\frac{\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "}\n",
    "{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})^2}\n",
    "\\,\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\\n",
    "& = & \n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "- \\int_\\mathcal{X}\n",
    "\\frac{\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,\\frac{\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\\n",
    "& = &\n",
    "\\mathbf{O}-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which again follows from the \n",
    "[previous](#Probability-distribution-functions \"Section: Probability distribution functions\") section. We shall require these results later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56266a0c",
   "metadata": {},
   "source": [
    "Finally, we note that the expected value of the log-likelihood itself is given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[L\\mid\\boldsymbol{\\theta}\\right] & = & \n",
    "\\int_\\mathcal{X} p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\ln p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "\\,|d\\mathbf{x}|~\\doteq~ -H(X\\mid\\boldsymbol{\\theta})\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $H(X\\mid\\boldsymbol{\\theta})$ is just the information-theoretic entropy of the distribution measured in *nats*. Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}H(X\\mid\\boldsymbol{\\theta}) & = &\n",
    "-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "-\n",
    "\\mathbb{E}\\left[L\\,\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~ -\\mathbb{E}\\left[L\\,\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which follows from the derivation above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fed659",
   "metadata": {},
   "source": [
    "### Parameter transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907b4e6",
   "metadata": {},
   "source": [
    "Suppose now that we wish to take derivatives, not with respect to the \n",
    "[PDF](#Probability-distribution-functions \"Section: Probability distribution functions\") \n",
    "parameter $\\boldsymbol{\\theta}$, but with respect to some other reparameterisation, say $\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\theta)$.\n",
    "For this purpose, we consider the chain rules, namely that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}~=~\n",
    "\\frac{\\partial\\boldsymbol{\\eta}^T}{\\partial\\boldsymbol{\\theta}}\\frac{\\partial}{\\partial\\boldsymbol{\\eta}}\\,,\n",
    "& \\;\\;\\;\\mbox{and}~ &\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\eta}}~=~\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}\\,.\n",
    "\\end{eqnarray}\n",
    "For convenience, we define \n",
    "$\\mathbf{J}_\\boldsymbol{\\eta}\\doteq\\frac{\\partial\\boldsymbol{\\eta}^T}{\\partial\\boldsymbol{\\theta}}$\n",
    "to be the *Jacobian* matrix of the transformation $\\boldsymbol{\\eta}(\\theta)$. It then follows from the first chain rule that the gradients are related via\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}~=~\\mathbf{J}_\\boldsymbol{\\eta}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\n",
    "& ~\\Rightarrow~ &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}~=~\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\,,\n",
    "\\end{eqnarray}\n",
    "and thus we deduce from the second chain rule that $\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\doteq\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}$. Note that $\\mathbf{J}_\\boldsymbol{\\eta}$ and $\\mathbf{J}_\\boldsymbol{\\eta}^{-1}$ are only truly matrix inverses of each other if both\n",
    "$\\boldsymbol{\\theta}$ and $\\boldsymbol{\\eta}$ have the same dimensions, otherwise we shall treat them symbolically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d02617",
   "metadata": {},
   "source": [
    "The relationship between the Hessians is more involved. Firstly, we take the transpose of the second chain rule  to obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\eta}^T} & = &\n",
    "\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\\,,\n",
    "\\end{eqnarray}\n",
    "where the symbol \"$\\cdot$\" explicitly indicates the position of the argument.\n",
    "Next, we apply the second chain rule directly to this result, thereby obtaining\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial^2}{\\partial\\boldsymbol{\\eta}\\,\\partial\\boldsymbol{\\eta}^T} & = &\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}\n",
    "\\left\\{\n",
    "\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "\\right\\}\n",
    "\\\\& = &\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\n",
    "\\left\\{\n",
    "\\frac{\\partial^2\\,\\cdot}{\\partial\\boldsymbol{\\theta}\\,\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "+\\left(\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}\\right)\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "\\right\\}\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, we replace the last derivative\n",
    "$\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}$\n",
    "via the first chain rule, to obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial^2}{\\partial\\boldsymbol{\\eta}\\,\\partial\\boldsymbol{\\eta}^T} & = &\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\n",
    "\\left\\{\n",
    "\\frac{\\partial^2\\,\\cdot}{\\partial\\boldsymbol{\\theta}\\,\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "+\\left(\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\eta}^T}{\\partial\\boldsymbol{\\theta}}\\right)\\odot\n",
    "\\frac{\\partial^2\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}\\,\\partial\\boldsymbol{\\eta}^T}\n",
    "\\right\\}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the last term is the dot product \"$\\odot$\" of a (row) vector with a *tensor*, i.e. a column \"vector\" in which each element is \n",
    "itself a matrix. \n",
    "Consequently, in terms of the gradient operators and Jacobian matrices, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T} & = &\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}(\\cdot)\\,\n",
    "\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\n",
    "+\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\left(\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^T(\\cdot)\\,\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}\\right)\\odot\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\\,.\n",
    "\\end{eqnarray}\n",
    "Also note that various specialisations of this relationship occur depending upon both the parameters and the reparameterisation.\n",
    "For example, in the case of scalar $\\theta$ and scalar $\\eta$, the identity simplifies to\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial^2}{\\partial\\eta^2} & = & \\left(\\frac{\\partial\\theta}{\\partial\\eta}\\right)^2\n",
    "\\frac{\\partial^2}{\\partial\\theta^2} +\n",
    "\\frac{\\partial^2\\theta}{\\partial\\eta^2}\\,\\frac{\\partial}{\\partial\\theta}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1408a",
   "metadata": {},
   "source": [
    "Now, the general identity will be a bit complex to apply in practice.\n",
    "However, we find that there is a simpler approximation when we specifically consider the log-likelihood $L$, such that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L & = &\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\n",
    "+\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\left(\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\\,\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}\\right)\\odot\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\boldsymbol{\\theta}\\,.\n",
    "\\end{eqnarray}\n",
    "In particular, taking the expectation of both sides, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\n",
    "& = &\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\,\n",
    "\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\n",
    "+\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\left(\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\,\\mathbf{J}_\\boldsymbol{\\eta}\\right)\\odot\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\boldsymbol{\\theta}\n",
    "\\\\&=&\n",
    "-\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^{T}}\n",
    "~=~ -\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which follows from a [previous](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\") section,\n",
    "where we derived that $\\mathbf{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]=\\mathbf{0}$\n",
    "and\n",
    "$\\mathbb{E}\\left[\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]=\n",
    "-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\n",
    "\\boldsymbol{\\nabla}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]$.\n",
    "This approximation to the Hessian of the log-likelihood will be used in the\n",
    "[next](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da760047",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a1a17",
   "metadata": {},
   "source": [
    "Consider a stochastic sampling process that produces an (arbitrary) length-$n$ sequence of independent, identically distributed variables,\n",
    "$X_1, X_2, \\ldots, X_n$. Then the sample average is defined as\n",
    "\\begin{eqnarray}\n",
    "\\langle X\\rangle ~\\doteq~\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}X_i\\,.\n",
    "\\end{eqnarray}\n",
    "More generally, the sample average of an arbitrary function $\\mathbf{f}(X,\\ldots)$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mathbf{f}\\rangle(\\ldots) & \\doteq &\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{f}( X_i,\\ldots)\\,,\n",
    "\\end{eqnarray}\n",
    "where the ellipsis \"$\\ldots$\" represents arbitrary parameters that do not vary with $X$.\n",
    "Due to the linearity of the operator $\\langle\\cdot\\rangle$, we henceforth typically consider a scalar function $f$\n",
    "(unless otherwise stated), since the sample mean of a vector is a vector of scalar sample means, and likewise for\n",
    "matrices and tensors.\n",
    "Similarly, it also follows from the linearity of the various other operators\n",
    "that the parameter gradient and Hessian of the sample mean obey\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\langle f\\rangle ~=~\n",
    "\\left\\langle\\boldsymbol{\\nabla}f\\right\\rangle &\\;\\;\\mbox{and}\\;&\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}\\langle f\\rangle ~=~\n",
    "\\left\\langle\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}f\\right\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "respectively, and the expectation obeys\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\langle f\\rangle\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\left\\langle\\mathbb{E}\\left[f\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\n",
    "~=~\\mathbb{E}\\left[f\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since the variates $X_i$ are here assumed to be independent and identically distributed (IID).\n",
    "We shall relax this last restriction [later](#Regression-modelling \"Section: Regression modelling\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8b768",
   "metadata": {},
   "source": [
    "We therefore see that the sample-mean log-likelihood $\\langle L\\rangle$ satisfies\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}\\langle L\\rangle\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\n",
    "~=~\\mathbf{0}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with the last result obtained from a [previous](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\") section.\n",
    "This result motivates the maximum likelihood approach, which is to determine the estimate $\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$ that satisfies\n",
    "$\\langle\\boldsymbol{\\nabla}L\\rangle(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})=\\mathbf{0}$,\n",
    "if such a solution exists.\n",
    "Under mild conditions of convexity,\n",
    "$\\langle L\\rangle(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$ is a local maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dae87e",
   "metadata": {},
   "source": [
    "The maximum-likelihood parameter value\n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$ is usually found iteratively via an update scheme of the form\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta}' & = & \\boldsymbol{\\theta}+\\Delta\\boldsymbol{\\theta}\\,,\n",
    "\\end{eqnarray}\n",
    "where all requisite quantities are evaluated at the current estimate $\\boldsymbol{\\theta}$.\n",
    "Then the parameter increment $\\Delta\\boldsymbol{\\theta}$ itself is usually computed either via\n",
    "a direct gradient method, e.g. gradient ascent\n",
    "\\begin{eqnarray}\n",
    "\\Delta\\boldsymbol{\\theta} & \\doteq & \\rho\\,\\boldsymbol{\\nabla}\\langle L\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "or via a modified gradient method, e.g. the Newton-Raphson method\n",
    "\\begin{eqnarray}\n",
    "\\Delta\\boldsymbol{\\theta} & \\doteq & \n",
    "-\\left[\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}\\langle L\\rangle\\right]^{-1}\\,\n",
    "\\boldsymbol{\\nabla}\\langle L\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that, in either case, the iterations will halt when \n",
    "$\\Delta\\boldsymbol{\\theta}=\\mathbf{0}$, which occurs when\n",
    "the gradient of the sample-mean log-likelihood vanishes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45cdcf",
   "metadata": {},
   "source": [
    "In practice, we usually apply the Newton-Raphson scheme by solving the linear equation\n",
    "\\begin{eqnarray}\n",
    "-\\langle\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}L\\rangle\\,\\Delta\\boldsymbol{\\theta} & = & \n",
    "\\langle\\boldsymbol{\\nabla}L\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "However, the Hessian is often difficult to compute, and so an approximation is typically used.\n",
    "Hence, following the reasoning from the\n",
    "[previous](#Parameter-transformations \"Section: Parameter transformations\") section,\n",
    "we take the expectation of the left-hand side only (since the expectation of the right-hand side is always zero).\n",
    "This allows us to compute an approximate update as the solution to\n",
    "\\begin{eqnarray}\n",
    "-\\left\\langle\\mathbb{E}\\left[\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\\,\\Delta\\boldsymbol{\\theta} \n",
    "& = & \n",
    "\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\n",
    "\\,\\Delta\\boldsymbol{\\theta} ~=~\n",
    "\\langle\\boldsymbol{\\nabla}L\\rangle\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which has the advantage of only requiring knowledge about the gradient of the log-likelihood.\n",
    "Note that some other gradient update schemes also use approximations to the Hessian. For example, the LBFGS algorithm computes an approximate Hessian matrix from (multiple) previous estimates of the gradient,\n",
    "effectively approximating the expectation itself by a temporal average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710b12b",
   "metadata": {},
   "source": [
    "Finally, note that we might more generally consider some \n",
    "[parameter transformation](#Parameter-transformations \"Section: Parameter transformations\") \n",
    "$\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$, and therefore the transformed parameter update\n",
    "$\\Delta\\boldsymbol{\\eta}$ would follow from the above formulae by\n",
    "taking all gradients and Hessians with respect to $\\boldsymbol{\\eta}$ rather than $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea62110",
   "metadata": {},
   "source": [
    "## Exponential families"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4f8db",
   "metadata": {},
   "source": [
    "Since [PDFs](#Probability-distribution-functions \"Section: Probability distribution functions\") are required to be\n",
    "non-negative, it follows that they may be expressed as exponentials. Different exponential forms lead to different families of distributions.\n",
    "Of particular interest is a [general family](#General-form \"Section: General form\")\n",
    "of distributions having linearly-additive log-likelihoods, and also a more \n",
    "[specialised family](#Seperable-dependencies \"Section: Seperable dependencies\"),\n",
    "misleadingly called \"**the**\" exponential family,\n",
    "having bilinear or separable dependencies between the variates and the parameters. These distributions are discussed in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ce121",
   "metadata": {},
   "source": [
    "### General form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7b31",
   "metadata": {},
   "source": [
    "Clearly, a [PDF](#Probability-distribution-functions \"Section: Probability distribution functions\")\n",
    "$p(\\mathbf{x}\\mid\\boldsymbol{\\theta})$ must be the exponential of its log-likelihood\n",
    "$L(\\boldsymbol{\\theta};\\,\\mathbf{x})$. Considered as an additive model with variate $X$, the log-likelihood will in general include: constant terms; terms in $X$ but not $\\boldsymbol{\\theta}$; terms in $\\boldsymbol{\\theta}$ but not $X$;\n",
    "and terms containing interactions between $X$ and $\\boldsymbol{\\theta}$. Hence, the general log-likelihood takes the form\n",
    "\\begin{eqnarray}\n",
    "L(\\boldsymbol{\\theta};\\,X) & = & \\ln h(X)-\\ln Ƶ(\\boldsymbol{\\theta})+s(X,\\boldsymbol{\\theta})\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where any constant terms may be placed in either or both of $h(X)$ or $Ƶ(\\boldsymbol{\\theta})$, but the\n",
    "interaction function $s(X,\\boldsymbol{\\theta})$ may contain neither constant terms, nor terms only in $X$,\n",
    "nor terms only in $\\boldsymbol{\\theta}$.\n",
    "Consequently, the general probability distribution is specified by\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) & = & \n",
    "\\frac{h(\\mathbf{x})\\,e^{s(\\mathbf{x},\\boldsymbol{\\theta})}}\n",
    "     {Ƶ(\\boldsymbol{\\theta})}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $Ƶ(\\boldsymbol{\\theta})$ is now seen to be the normalising *partition* function defined by\n",
    "\\begin{eqnarray}\n",
    "Ƶ(\\boldsymbol{\\theta}) & \\doteq & \n",
    "\\int_\\mathcal{X}h(\\mathbf{x})\\,e^{s(\\mathbf{x},\\boldsymbol{\\theta})}\n",
    "\\,|d\\mathbf{x}|\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ec89a",
   "metadata": {},
   "source": [
    "It now [follows](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\") \n",
    "from the parameter gradient of the log-likelihood $L$ that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}L(\\boldsymbol{\\theta}; X) & = & \\boldsymbol{\\nabla}s(X,\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}\\ln Ƶ(\\boldsymbol{\\theta})\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right] & = & \n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "-\\boldsymbol{\\nabla}\\ln Ƶ~=~\\mathbf{0}\n",
    "\\\\\n",
    "\\Rightarrow \n",
    "\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s} & ~\\doteq~ &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}\\ln Ƶ\n",
    "\\\\\n",
    "\\Rightarrow \\boldsymbol{\\nabla}L & = & \\boldsymbol{\\nabla}s-\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, it [follows](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\")\n",
    "from the Hessian of the log-likelihood that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}L(\\boldsymbol{\\theta}; X) & = & \n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s(X,\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ(\\boldsymbol{\\theta})\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "-\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ\n",
    "& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\\mathbb{E}\\left[\\left(\\boldsymbol{\\nabla}s-\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s}\\right)\\,\n",
    "\\left(\\boldsymbol{\\nabla}s-\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s}\\right)^T\n",
    "\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "We observe that the last term on the right-hand side is just the variance of $\\boldsymbol{\\nabla}s$, such that\n",
    "we may define\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_{\\small\\boldsymbol{\\nabla}s}\n",
    "& ~\\doteq~ &\n",
    "\\mathbb{V}\\left[\\boldsymbol{\\nabla}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ\n",
    "-\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the variance operator $\\mathbb{V}[\\cdot]$ is also often denoted as $\\texttt{Var}[\\cdot]$ instead.\n",
    "We shall use the $\\texttt{Var}[\\cdot]$ form when it is also useful to consider the covariance operator\n",
    "$\\texttt{Cov}[\\cdot,\\cdot]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8544eff",
   "metadata": {},
   "source": [
    "These identities hold for gradients and Hessians with respect to both the distributional parameter\n",
    "$\\boldsymbol{\\theta}$ and also any arbitrary\n",
    "[reparameterisation](#Parameter-transformations \"Section: Parameter transformations\")\n",
    "$\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$.\n",
    "Consequently, we may always define a new variate of the form \n",
    "$Y\\doteq\\boldsymbol{\\nabla}s(X,\\boldsymbol{\\theta})$, such that\n",
    "its mean is given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_{Y} & ~=~ &\n",
    "\\mathbb{E}\\left[Y\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}\\ln Ƶ\\,,\n",
    "\\end{eqnarray}\n",
    "and its variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_Y\n",
    "& ~=~ &\n",
    "\\mathbb{V}\\left[Y\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\mu}_{Y}^{T}\n",
    "-\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}Y^T\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "estimate $\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$\n",
    "may therefore be obtained iteratively via the approximate Newton-Raphson update\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\\,\\Delta\\boldsymbol{\\theta} ~=~\n",
    "\\langle\\boldsymbol{\\nabla}L\\rangle\n",
    "& ~~~\\Rightarrow~~~ &\n",
    "\\left\\langle\\boldsymbol{\\Sigma}_Y\\right\\rangle\\,\\Delta\\boldsymbol{\\theta}\n",
    "~=~\n",
    "\\left\\langle Y-\\boldsymbol{\\mu}_Y\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The iterations halt when $\\Delta\\boldsymbol{\\theta}=\\mathbf{0}$, at which point\n",
    "the sample mean $\\left\\langle Y\\right\\rangle$ equals the expected mean\n",
    "$\\hat{\\boldsymbol{\\mu}}_Y=\\boldsymbol{\\mu}_Y(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$.\n",
    "Note that under the transformation $\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$,\n",
    "we may also obtain updates for $\\boldsymbol{\\eta}$ via $\\Delta\\boldsymbol{\\eta}$,\n",
    "culminating in the maximum-likelihood etimate\n",
    "$\\hat{\\boldsymbol{\\eta}}_\\texttt{ML}=\\boldsymbol{\\eta}(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e3e49",
   "metadata": {},
   "source": [
    "### Seperable dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1b191",
   "metadata": {},
   "source": [
    "We now consider a specialisation of the [general form](#General-form \"Section: General form\") which had a (somewhat) arbitrary,\n",
    "scalar interaction term $s(X,\\boldsymbol{\\theta})$.\n",
    "The essential idea is that all interactions between the variate $X$ and the\n",
    "parameter $\\boldsymbol{\\theta}$ are now multiplicatively separable, i.e. specified via one or more product terms. \n",
    "The simplest such product form is \n",
    "$s(X,\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^{T}X$. However, more generally the nonlinear product\n",
    "$s(X,\\boldsymbol{\\theta})=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})^{T}\\mathbf{u}(X)$ is also valid. \n",
    "Note that the vector function $\\boldsymbol{\\eta}(\\cdot)$ may be thought of as defining the\n",
    "*natural* parameterisation $\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$ of the distribution.\n",
    "Also, note that we now have the special property that $\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,s(X,\\boldsymbol{\\theta})=\\mathbf{u}(X)$,\n",
    "such that $Y_\\boldsymbol{\\eta}\\doteq\\mathbf{u}(X)$ may be thought of as the natural *variates*\n",
    "of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8661fdf",
   "metadata": {},
   "source": [
    "Note that distributions having the form $s(\\mathbf{x},\\boldsymbol{\\theta})=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})^{T}\\mathbf{u}(\\mathbf{x})$ are regarded as belonging to **the** *exponential family*. This is a somewhat misleading and presumptuous term, given that other forms of $s(\\mathbf{x},\\boldsymbol{\\theta})$ exist, and other forms of log-likelihood not in the general form also exist, i.e. members of an even more general exponential family that are not in \"the\" exponential family.\n",
    "Also note that distributions having the bilinear form $s(\\mathbf{x},\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^{T}\\mathbf{x}$\n",
    "are regarded as members of the *natural* exponential family, since the natural parameter $\\boldsymbol{\\eta}$\n",
    "is identically the ordinary parameter $\\boldsymbol{\\theta}$. The other stipluation is \n",
    "([apparently](https://en.wikipedia.org/wiki/Natural_exponential_family \"Wikipedia: Natural exponential family\")) \n",
    "that we also must have the identity function\n",
    "$\\mathbf{u}(\\mathbf{x})=\\mathbf{x}$ to be in the natural exponential family. It is unclear what categorisation should be given to distributions for which the parameters are already *natural*, but for which $\\mathbf{u}(\\mathbf{x})\\neq\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e46a7",
   "metadata": {},
   "source": [
    "As noted above, the special property of \"the\" exponential family is that, taking gradients with respect to\n",
    "the natural parameter\n",
    "$\\boldsymbol{\\eta}$, we have\n",
    "\\begin{eqnarray}\n",
    "Y_\\boldsymbol{\\eta}~=~\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,s(X,\\boldsymbol{\\theta})~=~\\mathbf{u}(X)\n",
    "& ~\\Rightarrow~ &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,Y_\\boldsymbol{\\eta}^{T}~=~\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\,s~=~\\mathbf{O}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "It then follows from the [previous](#General-form \"Section: General form\") section that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta}) & ~=~ & \n",
    "\\mathbb{E}\\left[Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right] ~=~ \n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\ln Z(\\boldsymbol{\\theta})\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta}) & ~=~ &\n",
    "\\mathbb{V}\\left[ Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\ln Z(\\boldsymbol{\\theta})\n",
    "~=~\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}^{T}(\\boldsymbol{\\theta})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "As a simplification, we may use the [results](#Parameter-transformations \"Section: Parameter transformations\") that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}~=~\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\ln Z\\,, \n",
    "& \\;\\;~\\mbox{and}~\\;\\; &\n",
    "\\boldsymbol{\\Sigma}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta})~=~\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\,\n",
    "~\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\,\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}^{T}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the **exact** \n",
    "[Newton-Raphson](#Maximum-likelihood-estimation \"Section: #Maximum likelihood estimation\") update\n",
    "of the natural parameter $\\boldsymbol{\\eta}$ is now given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta})\\,\\Delta\\boldsymbol{\\eta} & = & \n",
    "\\langle Y_\\boldsymbol{\\eta}\\rangle-\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta})\\,,\n",
    "\\end{eqnarray}\n",
    "such that the \n",
    " [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "estimate $\\hat{\\boldsymbol{\\eta}}_\\texttt{ML}=\\boldsymbol{\\eta}(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$\n",
    "satisfies $\\hat{\\boldsymbol{\\mu}}_{\\small Y_\\boldsymbol{\\eta}}\n",
    "=\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})=\\langle Y_\\boldsymbol{\\eta}\\rangle$.\n",
    " We shall see from a later \n",
    "[example](#Beta-distribution \"Section: Beta distribution\") that $\\langle Y_\\boldsymbol{\\eta}\\rangle$ are the sufficient statistics for \"the\" exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e430e",
   "metadata": {},
   "source": [
    "### Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611f0f5",
   "metadata": {},
   "source": [
    "Consider a match between two teams, say team A and team B. Suppose further that, after considering all the evidence, our model proposes a probability $\\theta$ of team A winning. In practice, team A may either win or lose the match, or even draw the match, which we shall deal with later. Hence, we let $X=1$ indicate that team A actually won the match, and let $X=0$ indicate that team A lost the match. The variate $X$ then follows the Bernoulli distribution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & = & \\theta^{x}\\,(1-\\theta)^{1-x}\n",
    "~=~\\frac{e^{x\\ln\\frac{\\theta}{1-\\theta}}}{(1-\\theta)^{-1}}\\,.\n",
    "\\end{eqnarray}\n",
    "We therefore observe that the Bernoulli distribution is a member of \"the\" \n",
    "[exponential family](#Seperable-dependencies \"Section: Seperable dependencies\") \n",
    "with natural parameter\n",
    "\\begin{eqnarray}\n",
    "\\eta & \\doteq & \\ln\\frac{\\theta}{1-\\theta}~\\doteq~\\sigma^{-1}(\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\sigma^{-1}(\\cdot)$ is the *logit* function, and its inverse is the  *logistic* (sigmoid)\n",
    "function $\\sigma(\\eta)\\doteq(1+e^{-\\eta})^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20438b",
   "metadata": {},
   "source": [
    "We also see that the utility function is just the identity, $u(x)=x$, \n",
    "such that $X$ is the natural variate.\n",
    "Lastly, observe that the partition function is given by\n",
    "\\begin{eqnarray}\n",
    "Ƶ(\\theta)~=~(1-\\theta)^{-1} & ~\\Rightarrow~ & \\nabla_\\theta\\ln Ƶ~=~\\frac{1}{1-\\theta}\\,.\n",
    "\\end{eqnarray}\n",
    "Given the reparameterisation $\\eta=\\sigma^{-1}(\\theta)$, we also note that the\n",
    "[Jacobian](#Parameter-transformations \"Section: Parameter transformations\") of this transformation is given by\n",
    "\\begin{eqnarray}\n",
    "J_\\eta & = & \\nabla_\\theta\\,\\eta~=~\\frac{1}{\\theta\\,(1-\\theta)}\n",
    "\\\\\n",
    "\\Rightarrow \\nabla_\\eta\\ln Ƶ & = & J_\\eta^{-1}\\nabla_\\theta\\ln Ƶ~=~\\theta\\,,\n",
    "\\end{eqnarray}\n",
    "from which it [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that\n",
    "the distributional mean is given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & = & \\mathbb{E}[X\\mid\\theta]~=~\\nabla_\\eta\\ln Ƶ~=~\\theta\\,.\n",
    "\\end{eqnarray}\n",
    "Taking $\\mu\\doteq\\mu_X$ for convenience,\n",
    "we now see that the logit function $\\sigma^{-1}(\\cdot)$ is the\n",
    "natural [link function](#Regression-modelling \"Section: Regression modelling\")\n",
    "for the Bernoulli distribution, since $\\eta=\\sigma^{-1}(\\mu)$.\n",
    "This choice of link function is also justified in a \n",
    "[later](#Bernoulli-regression-(again) \"Section: Bernoulli regression (again)\") section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9479744",
   "metadata": {},
   "source": [
    "Similarly, we observe that\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\eta^2\\ln Ƶ(\\theta)~=~\\nabla_\\eta\\,\\theta~=~J_\\eta^{-1}~=~\\theta\\,(1-\\theta)\\,.\n",
    "\\end{eqnarray}\n",
    "It therefore [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that\n",
    "the distributional variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_X^2 & = & \\mathbb{V}[X\\mid\\theta]~=~\\theta\\,(1-\\theta)~=~\\mu\\,(1-\\mu)\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the Bernoulli distribution (or its variate $X$) is *heteroscedastic*, since the variance is not constant\n",
    "but is instead a function of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f416143",
   "metadata": {},
   "source": [
    "It also [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that the\n",
    "[maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate of the mean\n",
    "is given by \n",
    "\\begin{eqnarray}\n",
    "\\hat{\\mu}_\\texttt{ML} & = & \\hat{\\theta}_\\texttt{ML}~=~\\langle X\\rangle\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ef957",
   "metadata": {},
   "source": [
    "Finally, we make use of the fact that $1-\\sigma(\\eta)=\\sigma(-\\eta)$, and therefore observe that we may reparameterise the Bernoulli distribution \n",
    "in terms of its natural parameter $\\eta$ as\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\eta) & = & \\sigma(-\\eta)\\,e^{\\eta x}\\,,\n",
    "\\end{eqnarray}\n",
    "which puts it into the natural exponential family. Consequently, it appears that the membership (or non-membership) of a probability distribution in a given exponential sub-family is largely determined by its parameterisation.\n",
    "Also note that under this reparameterisation, the mean and variance are now given by\n",
    "\\begin{eqnarray}\n",
    "\\mu~=~\\mathbb{E}[X\\mid\\eta]~=~\\sigma(\\eta)\\,,\n",
    "&\\;\\;\\;& \n",
    "\\sigma^2_X~=~\\mathbb{V}[X\\mid\\eta]~=~\\sigma(\\eta)\\,\\sigma(-\\eta)\\,,\n",
    "\\end{eqnarray}\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab90079",
   "metadata": {},
   "source": [
    "### Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5facea20",
   "metadata": {},
   "source": [
    "It is feasible that team scores from some sporting games might be represented as Possion variables.\n",
    "We suppose that a scoring event occurs at an average rate of $\\lambda$ units per unit time (e.g. per match).\n",
    "Thus, let $X\\in\\mathbb{Z}^{\\ge 0}$ follow the Possion distribution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\lambda) & = & e^{-\\lambda}\\,\\frac{\\lambda^x}{x!}\n",
    "~=~\\frac{(x!)^{-1}\\,e^{x\\ln\\lambda}}{e^\\lambda}\\,,\n",
    "\\end{eqnarray}\n",
    "which is in \"the\" [exponential family](#Seperable-dependencies \"Section: Seperable dependencies\").\n",
    "The natural variate is therefore just $X$, but the natural parameter is $\\eta\\doteq\\ln\\lambda$.\n",
    "The partition function is\n",
    "\\begin{eqnarray}\n",
    "Ƶ(\\lambda)~=~e^\\lambda & ~~~\\Rightarrow~~~ & \\ln Ƶ~=~\\lambda~=~e^\\eta\\,,\n",
    "\\end{eqnarray}\n",
    "[whereupon](#Seperable-dependencies \"Section: Seperable dependencies\")\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & ~=~ & \\mathbb{E}[X\\mid\\lambda]~=~\\nabla_\\eta\\ln Ƶ~=~e^\\eta~=~\\lambda\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X & ~=~ & \\mathbb{V}[X\\mid\\lambda]~=~\\nabla^2_\\eta\\ln Ƶ~=~e^\\eta~=~\\lambda\\,.\n",
    "\\end{eqnarray}\n",
    "The [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate of\n",
    "the rate $\\lambda$ is therefore just $\\hat{\\lambda}_\\texttt{ML}=\\hat{\\mu}_\\texttt{ML}=\\langle X\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d692fd",
   "metadata": {},
   "source": [
    "### Beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771a982",
   "metadata": {},
   "source": [
    "In a [previous](#Bernoulli-distribution \"Section: Bernoulli distribution\") section, we\n",
    "considered the situation where a match between teams A and B might have a fixed probability $\\theta$ of team A winning.\n",
    "In contrast, we now suppose that this\n",
    "probability is denoted by a variate $X$, which is itself sampled from another distribution. For example, $X$ might be drawn from the Beta distribution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{x^{\\alpha-1}\\,(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\n",
    "~=~\\frac{[x(1-x)]^{-1}\\,e^{\\alpha\\ln x+\\beta\\ln(1-x)}}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "This distribution is also in \"the\"\n",
    "[exponential family](#Seperable-dependencies \"Section: Seperable dependencies\"), with natural parameters \n",
    "$\\boldsymbol{\\theta}=(\\alpha,\\beta)$, \n",
    "natural variates\n",
    "$Y_\\alpha=\\ln X$ and $Y_\\beta=\\ln (1-X)$, and\n",
    "partition function $Ƶ(\\boldsymbol{\\theta})$ given by\n",
    "\\begin{eqnarray}\n",
    "B(\\alpha,\\beta) & ~\\doteq~ & \\frac{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\Gamma(\\cdot)$ is the *gamma* function.\n",
    "It therefore [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that\n",
    "the mean of $Y_\\alpha=\\ln X$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{\\small Y_\\alpha} & ~=~ & \\mathbb{E}\\left[\\ln X\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial}{\\partial\\alpha}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi(\\alpha)-\\psi(\\alpha+\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\psi(\\cdot)$ is the *digamma* function given by \n",
    "\\begin{eqnarray}\n",
    "\\psi(z) & ~\\doteq~ & \\frac{d}{dz}\\ln\\Gamma(z)~=~\\frac{\\Gamma'(z)}{\\Gamma(z)}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, the mean of $Y_\\beta=\\ln (1-X)$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{\\small Y_\\beta} & ~=~ & \\mathbb{E}\\left[\\ln (1-X)\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial}{\\partial\\beta}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi(\\beta)-\\psi(\\alpha+\\beta)\\,.\n",
    "\\end{eqnarray}\n",
    "For interest sake, note that if we let $Y\\doteq\\ln\\frac{X}{1-X}=\\sigma^{-1}(X)$, then we deduce that\n",
    "\\begin{eqnarray}\n",
    "\\mu_Y & ~=~ & \\mathbb{E}\\left[Y\\mid\\alpha,\\beta\\right]~=~\n",
    "\\mu_{\\small Y_\\alpha}-\\mu_{\\small Y_\\beta} \n",
    "~=~\\psi(\\alpha)-\\psi(\\beta)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04864273",
   "metadata": {},
   "source": [
    "Similarly, we find that the variance of $Y_\\alpha$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{\\small Y_\\alpha} & ~=~ & \\texttt{Var}\\left[\\ln X\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial^2}{\\partial\\alpha^2}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi'(\\alpha)-\\psi'(\\alpha+\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\psi'(\\cdot)\\doteq\\psi_1(\\cdot)$ is the *trigamma* function given by\n",
    "\\begin{eqnarray}\n",
    "\\psi_1(z) & \\doteq & \\frac{\\Gamma(z)\\,\\Gamma''(z)-\\Gamma'(z)^2}{\\Gamma(z)^2}\\,.\n",
    "\\end{eqnarray}\n",
    "Likewise, the variance of $Y_\\beta$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{\\small Y_\\beta} & ~=~ & \\texttt{Var}\\left[\\ln(1-X)\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial^2}{\\partial\\beta^2}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi'(\\beta)-\\psi'(\\alpha+\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "and the covariance between $Y_\\alpha$ and $Y_\\beta$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\alpha,Y_\\beta} & ~=~ & \\texttt{Cov}\\left[\\ln X,\\,\\ln(1-X)\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial^2}{\\partial\\alpha\\partial\\beta}\\ln B(\\alpha,\\beta)\n",
    "~=~-\\psi'(\\alpha+\\beta)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88911d0",
   "metadata": {},
   "source": [
    "It might seem somewhat surprising that following the defined procedure does not immediately give us the mean and variance of the variate $X$, but instead gives the mean and variance of $\\mathbf{u}(X)$. \n",
    "In fact, it turns out that $\\langle\\ln X\\rangle$ and $\\langle\\ln(1-X)\\rangle$ provide the sufficient statistics for the \n",
    "[Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution \"Wikipedia: Beta distribution\"), \n",
    "and not $\\langle X\\rangle$. \n",
    "The mean and variance of $X$ are actually given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & ~=~ & \\mathbb{E}[X\\mid\\alpha,\\beta] ~=~\\frac{\\alpha}{\\alpha+\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X & ~=~ & \\texttt{Var}[X\\mid\\alpha,\\beta] ~=~\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "Note that we can also reparameterise the Beta distribution in another way. If we define $\\nu\\doteq\\alpha+\\beta$, then we see that\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X ~=~ \\frac{\\mu_X\\,(1-\\mu_X)}{\\nu+1} & \\;\\;\\Rightarrow\\;\\; &\n",
    "\\nu ~=~ \\frac{\\mu_X\\,(1-\\mu_X)}{\\sigma^2_X}-1\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\alpha~=~\\mu_X\\,\\nu\\,, & \\;\\;\\;\\; & \\beta=(1-\\mu_X)\\,\\nu\\,.\n",
    "\\end{eqnarray}\n",
    "The distribution is therefore heteroscedastic, with the variance $\\sigma_X^2$ clearly being a function of \n",
    "the mean $\\mu_X$ and hyper-parameter $\\nu$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959a457",
   "metadata": {},
   "source": [
    "Yet another reparametersiation is to retain $\\alpha$ and $\\nu$, such that the distribution becomes\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\nu) & ~=~ & \n",
    "\\frac{[x(1-x)]^{-1}\\,e^{\\alpha\\ln\\frac{x}{1-x}+\\nu\\ln(1-x)}}{B(\\alpha,\\nu-\\alpha)}\\,,\n",
    "\\end{eqnarray}\n",
    "where the natural variates are now $\\mathbf{u}(X)=[Y, \\ln(1-X)]^{T}$. We therefore obtain the mean of variate $Y$ as\n",
    "\\begin{eqnarray}\n",
    "\\mu_Y & ~=~ & \\frac{\\partial}{\\partial\\alpha}\\ln B(\\alpha,\\nu-\\alpha)~=~\\psi(\\alpha)-\\psi(\\nu-\\alpha)\\,,\n",
    "\\end{eqnarray}\n",
    "as before, and the variance of $Y$ is now also obtained as\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_Y & ~=~ & \\frac{\\partial^2}{\\partial\\alpha^2}\\ln B(\\alpha,\\nu-\\alpha)~=~\\psi'(\\alpha)+\\psi'(\\nu-\\alpha)\\,.\n",
    "\\end{eqnarray}\n",
    "This should come as no surprise, since $Y=\\ln X-\\ln(1-X)$, such that\n",
    "\\begin{eqnarray}\n",
    "\\texttt{Var}[Y\\mid\\alpha,\\beta] & ~=~ & \n",
    "\\texttt{Var}[\\ln X\\mid\\alpha,\\beta]+\\texttt{Var}[\\ln(1-X)\\mid\\alpha,\\beta]-2\\,\\texttt{Cov}[\\ln X,\\,\\ln(1-X)\\mid\\alpha,\\beta]\n",
    "\\\\& ~=~ &\n",
    "\\sigma^2_{Y_\\alpha}+\\sigma^2_{Y_\\beta}-2\\,\\sigma_{Y_\\alpha, Y_\\beta}~=~\\psi'(\\alpha)+\\psi'(\\beta)\\,.\n",
    "\\end{eqnarray}\n",
    "This little exercise demonstrates that although different parameterisations might make the various calculations either easier\n",
    "or harder to obtain, they cannot alter the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f1bc2",
   "metadata": {},
   "source": [
    "Finally, we observe that the [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "solution \n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}=(\\hat{\\alpha}_\\texttt{ML},\\hat{\\beta}_\\texttt{ML})$\n",
    "satisfies the nonlinear system of equations\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\\ln X\\right\\rangle & = & \n",
    "\\psi(\\hat{\\alpha}_\\texttt{ML})-\\psi(\\hat{\\alpha}_\\texttt{ML}+\\hat{\\beta}_\\texttt{ML})\\,,\n",
    "\\\\\n",
    "\\left\\langle\\ln(1-X)\\right\\rangle & = & \n",
    "\\psi(\\hat{\\beta}_\\texttt{ML})-\\psi(\\hat{\\alpha}_\\texttt{ML}+\\hat{\\beta}_\\texttt{ML})\\,.\n",
    "\\end{eqnarray}\n",
    "This may be solved numerically using the Newton-Raphson method with iterative parameter updates of the form\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{}\\Delta\\alpha\\\\\\Delta\\beta\\end{array}\\right]\n",
    "& = &\n",
    "\\left[\\begin{array}{}\n",
    "\\psi'(\\alpha)-\\psi'(\\alpha+\\beta) & -\\psi'(\\alpha+\\beta)\\\\\n",
    "-\\psi'(\\alpha+\\beta) & \\psi'(\\beta)-\\psi'(\\alpha+\\beta)\n",
    "\\end{array}\\right]^{-1}\\,\n",
    "\\left[\\begin{array}{}\n",
    "\\left\\langle\\ln X\\right\\rangle - \\psi(\\alpha)+\\psi(\\alpha+\\beta)\\\\\n",
    "\\left\\langle\\ln(1-X)\\right\\rangle - \\psi(\\beta)+\\psi(\\alpha+\\beta)\n",
    "\\end{array}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c1271",
   "metadata": {},
   "source": [
    "The final issue remains about what sample values of the variate $X$ are being observed in practice? We can no longer use the\n",
    "[Bernoulli](#Bernoulli-distribution \"Section: Bernoulli distribution\") values of $X=1$ for a  win and $X=0$ for a loss,\n",
    "since now $X$ is a probability. One possibility is to note that after a match between team A and team B, we might have observed team A's score $S_A$ and team B's score $S_B$. Hence, we could use the proportion $X=\\frac{S_A}{S_A+S_B}$ as a proxy measure of the probability of team A winning a similar match against team B in the future.\n",
    "\n",
    "Note that, in general, we should not expect $S_A$ and $S_B$ to be independent, since $S_A$ should increase with team A's offensive strength, and decrease with team B's defensive strength. For example, we might expect both scores to be higher in a match between poor defenders than in a match between strong defenders. However, if we do assume independence, then taking\n",
    "$X\\sim\\texttt{Beta}(\\alpha,\\beta)$ [follows](https://en.wikipedia.org/wiki/Beta_distribution \"Wikipedia: Beta distribution\")\n",
    "from $S_A\\sim\\texttt{Gamma}(\\alpha,\\gamma)$ and $S_B\\sim\\texttt{Gamma}(\\beta,\\gamma)$,\n",
    "where $\\gamma$ is a parameter common across all teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063dea1d",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee171e1",
   "metadata": {},
   "source": [
    "We now consider the combined situation where each match has a \n",
    "[Bernoulli-distributed](#Bernoulli-distribution \"Section: Bernoulli distribution\") outcome, $X\\mid\\theta\\sim\\texttt{Bern}(\\theta)$, but where the probability $\\theta$ iteslf is [Beta-distributed](#Beta-distribution \"Section: Beta distribution\"),\n",
    "with $\\theta\\sim\\texttt{Beta}(\\alpha,\\beta)$. It may be \n",
    "[shown](#D_distributions.ipynb#Beta-Bernoulli-mixture-distribution \"Appendix D: Beta-Bernoulli mixture distribution\") \n",
    "that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & = & \\frac{\\alpha^x\\,\\beta^{1-x}}{\\alpha+\\beta}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36f749",
   "metadata": {},
   "source": [
    "We may now rewrite the distribution in the form\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{\\beta\\,\\left(\\frac{\\alpha}{\\beta}\\right)^x}{\\alpha+\\beta}\n",
    "~=~\\frac{e^{x\\ln\\frac{\\alpha}{\\beta}}}{1+\\frac{\\alpha}{\\beta}}\\,,\n",
    "\\end{eqnarray}\n",
    "which is in \"the\" [exponential family](#Seperable-dependencies \"Section: Seperable dependencies\") with natural parameter\n",
    "$\\eta=\\ln\\frac{\\alpha}{\\beta}$. Consequently, the reparameterised distribution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\eta) & \\doteq & \\frac{e^{\\eta x}}{1+e^\\eta}\\,,\n",
    "\\end{eqnarray}\n",
    "is thus in the natural exponential family, and has mean\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & \\mathbb{E}[X\\mid\\eta]~=~\\frac{d}{d\\eta}\\ln(1+e^\\eta)\n",
    "~=~\\frac{e^\\eta}{1+e^\\eta}~=~\\frac{1}{1+e^{-\\eta}}\\,.\n",
    "\\end{eqnarray}\n",
    "This is just the logistic function $\\sigma(\\cdot)$, such that\n",
    "\\begin{eqnarray}\n",
    "\\mu~=~\\sigma(\\eta) & ~\\Rightarrow~ & \\eta~=~\\sigma^{-1}(\\mu)\\,.\n",
    "\\end{eqnarray}\n",
    "The [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate\n",
    "$\\hat{\\eta}_\\texttt{ML}$ therefore satisfies\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\mu}_\\texttt{ML} & ~=~ & \\sigma(\\hat{\\eta}_\\texttt{ML})=\\langle X\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, the variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_X^2 & ~=~ & \\texttt{Var}[X\\mid\\eta]~=~\\frac{d}{d\\eta}\\frac{1}{1+e^{-\\eta}}\n",
    "~=~\\frac{e^{-\\eta}}{(1+e^{-\\eta})^2}~=~\\sigma(\\eta)\\,\\sigma(-\\eta)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fa2c6",
   "metadata": {},
   "source": [
    "In terms of the original parameters $\\alpha$ and $\\beta$, substitution of $\\eta=\\ln\\frac{\\alpha}{\\beta}$ into the above results gives\n",
    "\\begin{eqnarray}\n",
    "\\mu_X~=~\\frac{\\alpha}{\\alpha+\\beta}\\,, & \\;\\;\\; & \\sigma^2_X~=~\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}\\,.\n",
    "\\end{eqnarray}\n",
    "However, it must be noted that the natural exponential form of the distribution above required only a single parameter, namely $\\eta$, instead of the two parameters specified here, namely $\\alpha$ and $\\beta$.\n",
    "In order to help explain this discrepancy, consider the log-likelihood\n",
    "\\begin{eqnarray}\n",
    "L(\\alpha,\\beta;X) & ~=~ & X\\,(\\ln\\alpha-\\ln\\beta)+\\ln\\beta-\\ln(\\alpha+\\beta)\\,. \n",
    "\\end{eqnarray}\n",
    "Taking first derivatives with respect to $\\alpha$ and $\\beta$ then gives\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial\\alpha} & ~=~ & \\frac{X}{\\alpha}-\\frac{1}{\\alpha+\\beta}~\\doteq~Y_\\alpha-\\mu_{Y_\\alpha}\\,,\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial\\beta} & ~=~ & -\\frac{X}{\\beta}+\\frac{\\alpha}{\\beta(\\alpha+\\beta)}\n",
    "~\\doteq~Y_\\beta-\\mu_{Y_\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "from which it follows that\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\alpha} & ~\\doteq~ & \\mathtt{Var}[Y_\\alpha\\mid\\alpha,\\beta]~=~\\frac{\\sigma^2_X}{\\alpha^2}\n",
    "~=~\\frac{\\beta}{\\alpha(\\alpha+\\beta)^2}\\,,\n",
    "\\\\\n",
    "\\sigma^2_{Y_\\beta} & ~\\doteq~ & \\mathtt{Var}[Y_\\beta\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\sigma^2_X}{\\beta^2}~=~\\frac{\\alpha}{\\beta(\\alpha+\\beta)^2}\\,,\n",
    "\\\\\n",
    "\\sigma_{Y_\\alpha,Y_\\beta} & ~\\doteq~ & \\mathtt{Cov}[Y_\\alpha,Y_\\beta\\mid\\alpha,\\beta]\n",
    "~=~-\\frac{\\sigma^2_X}{\\alpha\\beta}~=~-\\frac{1}{(\\alpha+\\beta)^2}\\,.\n",
    "\\end{eqnarray}\n",
    "In terms of the vector parameter $\\boldsymbol{\\theta}=(\\alpha,\\beta)$ with vector variate \n",
    "$Y_\\boldsymbol{\\theta}=(Y_\\alpha,Y_\\beta)$,\n",
    "the covariance matrix is therefore given by\n",
    "\\begin{eqnarray}\n",
    "\\Sigma_{Y_\\boldsymbol{\\theta}} & ~\\doteq~ & \\left[\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\beta}{\\alpha(\\alpha+\\beta)^2} & -\\frac{1}{(\\alpha+\\beta)^2}\n",
    "\\\\\n",
    "-\\frac{1}{(\\alpha+\\beta)^2} & \\frac{\\alpha}{\\beta(\\alpha+\\beta)^2}\n",
    "\\end{array}\n",
    "\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "We observe that this covariance matrix is singular, which prevents us from using the\n",
    "[Newton-Raphson](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "approach to estimating $\\alpha$ and $\\beta$. We conclude that we cannot simultaneously estimate\n",
    "both $\\alpha$ and $\\beta$.\n",
    "\n",
    "Part of the problem is that we have no natural scale for $\\alpha$ and $\\beta$.\n",
    "Observe that rescaling both $\\alpha$ and $\\beta$ by the same factor affects neither the\n",
    "mean $\\mu_X$ nor the variance $\\sigma^2_X$. Consequently, we may set either of $\\alpha$ or $\\beta$ to a constant,\n",
    "which is equivalent to pre-defining the scale.\n",
    "For example, we could set $\\alpha=1$ and then estimate $\\beta=e^{-\\eta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da2e2a",
   "metadata": {},
   "source": [
    "The problem, fundamentally, is that the variates $Y_\\alpha$ and $Y_\\beta$ are not independent,\n",
    "since $\\alpha Y_\\alpha+\\beta Y_\\beta=0$. Furthermore,\n",
    "given $\\alpha$ and $\\beta$, both $Y_\\alpha$ and $Y_\\beta$ provide *identical* information about \n",
    "the response variate $X$. This is what makes the covariance matrix $\\Sigma_{Y_\\boldsymbol{\\theta}}$ singular.\n",
    "However, we can create an approximate estimation scheme by *assuming* that these variates *are* in fact independent. This amounts to artificially setting the covariance $\\sigma_{Y_\\alpha,Y_\\beta}$ to zero, such that the\n",
    "approximate covariance matrix is no longer singular. This approach appears to work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1df3e",
   "metadata": {},
   "source": [
    "### Gamma distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9dcaaa",
   "metadata": {},
   "source": [
    "As was noted [previously](#Beta-distribution \"Section: Beta distribution\"), in a match between team A and team B, we might observe scores $S_A$ and $S_B$, respectively. These scores are non-negative and usually, but not necessarily,\n",
    "integer valued. Let $X$ be the variate denoting a team's score.\n",
    "Then given appropriate assumptions of independence between the teams' scores, $X$ might follow the Gamma distribution:\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,x^{\\alpha-1}\\,e^{-\\beta x}\n",
    "~=~\\frac{x^{-1}e^{\\alpha\\ln x-\\beta x}}{\\beta^{-\\alpha}\\,\\Gamma(\\alpha)}\\,.\n",
    "\\end{eqnarray}\n",
    "This [PDF](#Probability-distribution-functions \"Section: Probability distribution functions\") \n",
    "is in \"the\" [exponential family](#Seperable-dependencies \"Section: Seperable dependencies\") with natural parameters\n",
    "$\\boldsymbol{\\theta}=(\\alpha,\\beta)$, natural variates \n",
    "$Y_\\alpha=\\ln X$ and $Y_\\beta=-X$, and\n",
    "partition function $Ƶ(\\boldsymbol{\\theta})$ that obeys\n",
    "\\begin{eqnarray}\n",
    "\\ln Ƶ(\\boldsymbol{\\theta}) & = & -\\alpha\\ln\\beta+\\ln\\Gamma(\\alpha)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184e95a",
   "metadata": {},
   "source": [
    "It therefore [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that the means are given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\alpha} & ~=~ & \\mathbb{E}[\\ln X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial}{\\partial\\alpha}\\ln Ƶ\n",
    "~=~-\\ln\\beta+\\psi(\\alpha)\\,,\n",
    "\\\\\n",
    "\\mu_{Y_\\beta} & ~=~ & \\mathbb{E}[-X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial}{\\partial\\beta}\\ln Ƶ\n",
    "~=~-\\frac{\\alpha}{\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\psi(\\cdot)$ is (as before) the *digamma* function. We note that the distributional mean is thus given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & ~=~ & \\mathbb{E}[X\\mid\\alpha,\\beta]~=~\\frac{\\alpha}{\\beta}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, the variances are given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\alpha} & ~=~ & \\texttt{Var}[\\ln X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial^2}{\\partial\\alpha^2}\\ln Ƶ\n",
    "~=~\\psi'(\\alpha)\\,,\n",
    "\\\\\n",
    "\\sigma^2_{Y_\\beta} & ~=~ & \\texttt{Var}[-X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial^2}{\\partial\\beta^2}\\ln Ƶ\n",
    "~=~\\frac{\\alpha}{\\beta^2}\\,,\n",
    "\\end{eqnarray}\n",
    "and the covariance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\alpha,Y_\\beta} & ~=~ & \\texttt{Cov}[\\ln X,-X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial^2}{\\partial\\alpha\\partial\\beta}\\ln Ƶ\n",
    "~=~-\\frac{1}{\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "such that the distributional variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X & ~=~ & \\texttt{Var}[X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\alpha}{\\beta^2}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c3246",
   "metadata": {},
   "source": [
    "The [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate\n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$ therefore satistifies the equations\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle -X\\right\\rangle~=~-\\frac{\\hat{\\alpha}_\\texttt{ML}}{\\hat{\\beta}_\\texttt{ML}}\n",
    "& ~~~\\Rightarrow~~~ & \\hat{\\beta}_\\texttt{ML}~=~\\frac{\\hat{\\alpha}_\\texttt{ML}}{\\left\\langle X\\right\\rangle}\n",
    "\\,,\n",
    "\\\\\n",
    "\\left\\langle\\ln X\\right\\rangle~=~-\\ln\\hat{\\beta}_\\texttt{ML}+\\psi(\\hat{\\alpha}_\\texttt{ML})\n",
    "& ~\\Rightarrow~ & \\ln\\left\\langle X\\right\\rangle-\\left\\langle\\ln X\\right\\rangle\n",
    "~=~\\ln\\hat{\\alpha}_\\texttt{ML}-\\psi(\\hat{\\alpha}_\\texttt{ML})\\,.\n",
    "\\end{eqnarray}\n",
    "[Apparently](https://en.wikipedia.org/wiki/Gamma_distribution \"Wikipedia: Gamma distribution\"), a good initial\n",
    "estimate of $\\hat{\\alpha}_\\texttt{ML}$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\alpha}_0 & ~=~ & \\frac{3-s+\\sqrt{(s-3)^2+24s}}{12s}\\,,\n",
    "\\end{eqnarray}\n",
    "where $s=\\ln\\left\\langle X\\right\\rangle-\\left\\langle\\ln X\\right\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f6468",
   "metadata": {},
   "source": [
    "### Negative binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1068c1c",
   "metadata": {},
   "source": [
    "The [negative binomial distribution](D_distributions.ipynb#Negative-binomial-distribution\n",
    "\"Appendix D: Negative binomial distribution\") may be derived as a continuous mixture of a Poisson distribution with a gamma prior. This distribution takes the form\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid\\alpha,p) & ~=~ & \n",
    "\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,(1-p)^k\\,p^\\alpha\\,,\n",
    "\\end{eqnarray}\n",
    "where $p$ is the probability of a notional *stopping* event and the sequence of trials terminates immediately after observing\n",
    "$\\alpha$ such stopping events. The variate $X\\in\\mathbb{Z}^{>0}$ counts the number of non-stopping events in a terminated sequence.\n",
    "The mean and variance of the distribution are given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & ~=~ & \\mathbb{E}[X\\mid\\alpha,p]~=~\\frac{\\alpha\\,(1-p)}{p}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X & ~=~ & \\mathbb{V}[X\\mid\\alpha,p]~=~\\frac{\\alpha\\,(1-p)}{p^2}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "Note that when $\\alpha$ is real-valued we have the *Polya* distribution, whereas when $\\alpha$ is\n",
    "integer-valued we have the *Pascal* distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249b15c",
   "metadata": {},
   "source": [
    "We may rewrite this distribution in the [general form](#General-form \"Section: General form\") as\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid\\alpha,p) & ~=~ & \\frac{(k!)^{-1}\\,e^{\\ln\\Gamma(\\alpha+k)+k\\ln(1-p)}}{\\Gamma(\\alpha)\\,p^{-\\alpha}}\\,.\n",
    "\\end{eqnarray}\n",
    "Denoting the parameters by $\\boldsymbol{\\theta}\\doteq(\\alpha,p)$, the interaction between the variate $X$ and the parameters\n",
    "takes the form\n",
    "\\begin{eqnarray}\n",
    "s(X,\\boldsymbol{\\theta}) & ~\\doteq~ & \\ln\\Gamma(\\alpha+X)+X\\ln(1-p)\\,.\n",
    "\\end{eqnarray}\n",
    "Due to the nonlinear interaction of $\\Gamma(\\alpha+X)$, this is **not**\n",
    "in \"the\" [exponential family](#Seperable-dependencies \"Section: Seperable dependencies\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa6a3f",
   "metadata": {},
   "source": [
    "Clearly, the variate $X$ has the corresponding natural parameter $\\eta\\doteq\\ln(1-p)$, whereupon\n",
    "$p=1-e^\\eta$.\n",
    "Given the partition function\n",
    "\\begin{eqnarray}\n",
    "Ƶ(\\boldsymbol{\\theta}) & ~=~ & \\Gamma(\\alpha)\\,p^{-\\alpha}\\,,\n",
    "\\end{eqnarray}\n",
    "we deduce that\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\eta\\ln Ƶ & ~=~ & \n",
    "-\\frac{\\partial p}{\\partial\\eta}\\,\\frac{\\partial \\alpha\\ln p}{\\partial p}\n",
    "~=~(1-p)\\,\\frac{\\alpha}{p}\\,,\n",
    "\\end{eqnarray}\n",
    "which agrees with $\\mu_X$. Proceding to take the another derivative then gives\n",
    "\\begin{eqnarray}\n",
    "\\nabla^2_\\eta\\ln Ƶ & ~=~ & \n",
    "\\frac{\\partial p}{\\partial\\eta}\\,\\frac{\\partial}{\\partial p}\\left\\{\\frac{\\alpha}{p}-\\alpha\\right\\}\n",
    "~=~(1-p)\\,\\frac{\\alpha}{p^2}\\,,\n",
    "\\end{eqnarray}\n",
    "which agrees with $\\sigma^2_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed74fbd",
   "metadata": {},
   "source": [
    "For convenience, let us now consider $\\alpha$ as another 'natural' parameter, despite its nonlinear co-dependence with $X$.\n",
    "We then define its corresponding variate to be\n",
    "\\begin{eqnarray}\n",
    "Y_\\alpha & ~\\doteq~ & \\nabla_\\alpha\\,s(X,\\boldsymbol{\\theta})\n",
    "~=~\\psi(\\alpha+X)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\psi(\\cdot)$ is the *digamma* function. From the partition function we then derive that\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\alpha} & ~\\doteq~ & \\mathbb{E}[Y_\\alpha\\mid\\boldsymbol{\\theta}]\n",
    "~=~\\nabla_\\alpha\\ln Ƶ(\\boldsymbol{\\theta})\n",
    "\\\\& = &\n",
    "\\frac{\\partial}{\\partial\\alpha}\\left\\{\\ln\\Gamma(\\alpha)-\\alpha\\ln p\\right\\}\n",
    "~=~\\psi(\\alpha)-\\ln p\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "It then [follows](#General-form \"Section: General form\") that  the variance of $Y_\\alpha$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\alpha} & ~\\doteq~ & \\mathbb{V}[Y_\\alpha\\mid\\boldsymbol{\\theta}]\n",
    "~=~\\nabla^2_\\alpha\\ln Ƶ(\\boldsymbol{\\theta})-\\mathbb{E}[\\nabla^2_\\alpha\\,s(X,\\boldsymbol{\\theta})\\mid\\boldsymbol{\\theta}]\n",
    "\\\\& = &\n",
    "\\psi'(\\alpha)-\\mathbb{E}[\\psi'(\\alpha+X)\\mid\\boldsymbol{\\theta}]\\,,\n",
    "\\end{eqnarray}\n",
    "and the covariance between $Y_\\alpha$ and $Y_\\eta\\doteq\\nabla_\\eta\\,s=X$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\alpha,X} & ~\\doteq~ & \\mathtt{Cov}[Y_\\alpha,X\\mid\\boldsymbol{\\theta}]\n",
    "~=~\\nabla_\\alpha\\nabla_\\eta\\ln Ƶ\n",
    "~=~\\frac{1-p}{p}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7169a9",
   "metadata": {},
   "source": [
    "The theoretical expectation \n",
    "$\\mathbb{E}[\\psi'(\\alpha+X)\\mid\\boldsymbol{\\theta}]$ above is likely to be intractible. However, if we consider a sequence of $n$\n",
    "[*unconditionally independent*](D_distributions.ipynb#Negative-binomial-distribution\n",
    "\"Appendix D: Negative binomial distribution\") observations, then the\n",
    "Hessian of the\n",
    "[*average* data log-likelihood](D_distributions.ipynb#Maximum-likelihood-estimation\n",
    "\"Appendix D: Maximum likelihood estimation\") \n",
    "contains the analogous term\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}\\psi'(\\alpha+X_i)$.\n",
    "Consequently, we replace the exact variance $\\sigma^2_{Y_\\alpha}$\n",
    "by its empirical estimate\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\sigma}^2_{Y_\\alpha} & ~\\doteq~ &\n",
    "\\psi'(\\alpha)-\\langle\\psi'(\\alpha+X)\\rangle\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5524c8",
   "metadata": {},
   "source": [
    "Taking the reparameterisation\n",
    "$\\boldsymbol{\\theta}'\\doteq(\\eta,\\alpha)$,\n",
    "the [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate \n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}'$ is then (hopefully) reached\n",
    "via the empirical iterative update\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{}\\Delta\\eta\\\\\\Delta\\alpha\\end{array}\\right]\n",
    "& = &\n",
    "\\left[\\begin{array}{}\n",
    "\\sigma_X^2 & \\sigma_{X,Y_\\alpha}\\\\\n",
    "\\sigma_{X,Y_\\alpha} & \\hat{\\sigma}_{Y_\\alpha}^2\n",
    "\\end{array}\\right]^{-1}\\,\n",
    "\\left[\\begin{array}{}\n",
    "\\left\\langle X\\right\\rangle - \\mu_X\\\\\n",
    "\\left\\langle Y_\\alpha\\right\\rangle - \\mu_{Y_\\alpha}\n",
    "\\end{array}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In particular, letting $q\\doteq 1-p$ and $\\bar{X}\\doteq\\langle X\\rangle$,\n",
    "the update for $\\hat{\\alpha}_\\texttt{ML}$ takes the form\n",
    "\\begin{eqnarray}\n",
    "\\Delta\\alpha & ~=~ &\n",
    "\\frac{\\sigma_X^2\\,(\\langle Y_\\alpha\\rangle-\\mu_{Y_\\alpha})\n",
    "-\\sigma_{X,Y_\\alpha}\\,(\\langle X\\rangle-\\mu_X)}\n",
    "{\\sigma_X^2\\,\\hat{\\sigma}_{Y_\\alpha}^2-\\sigma_{X,Y_\\alpha}^2}\n",
    "\\\\& = &\n",
    "\\frac{\\frac{\\alpha q}{p^2}\\,(\\langle\\psi(\\alpha+X)\\rangle-\\psi(\\alpha)+\\ln p)-\\frac{q}{p}\\,(\\bar{X}-\\frac{\\alpha q}{p})}\n",
    "{\\frac{\\alpha q}{p^2}\\,(\\psi'(\\alpha)-\\langle\\psi'(\\alpha+X)\\rangle)-\n",
    "\\frac{q^2}{p^2}}\n",
    "\\\\& = &\n",
    "\\frac{\\langle\\psi(\\alpha+X)\\rangle-\\psi(\\alpha)+\\ln p\n",
    "+\\left(q-\\frac{p\\bar{X}}{\\alpha}\\right)}\n",
    "{\\psi'(\\alpha)-\\langle\\psi'(\\alpha+X)\\rangle-\\frac{q}{\\alpha}}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa78de5",
   "metadata": {},
   "source": [
    "It turns out that the last term in the numerator vanishes under the\n",
    "constraint that\n",
    "\\begin{eqnarray}\n",
    "p~\\doteq~\\frac{\\alpha}{\\alpha+\\bar{X}}\n",
    "& ~~~\\Rightarrow~~~ &\n",
    "\\frac{\\partial}{\\partial\\alpha}\\ln p~=~\\frac{q}{\\alpha}\\,,\n",
    "\\end{eqnarray}\n",
    "whereupon the denominator is recognised as the negative gradient of the\n",
    "numerator with respect to $\\alpha$. Hence, this Newton-Raphson update is \n",
    "[**identical**](D_distributions.ipynb#Maximum-likelihood-estimation\n",
    "\"Appendix D: Maximum likelihood estimation\")\n",
    " to an update via Newton's method of the maximum likelihood\n",
    "estimate $\\hat{\\alpha}_\\texttt{ML}$ subject to the constraint that\n",
    "\\begin{eqnarray}\n",
    "\\langle X\\rangle-\\mu_X(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})~=~0 & ~~~\\Rightarrow~~~ &\n",
    "\\hat{p}_\\texttt{ML}~=~ \n",
    "\\frac{\\hat{\\alpha}_\\texttt{ML}}\n",
    "{\\hat{\\alpha}_\\texttt{ML}+\\langle X\\rangle}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that, subject to the usual constraints on convergence,\n",
    "the empirical approximation of the variance works here because\n",
    "\\begin{eqnarray}\n",
    "\\langle Y_\\alpha\\rangle\\left(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}\\right)\n",
    "~=~\\mathbb{E}[Y_\\alpha\\mid\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}]\n",
    "& ~~~\\Rightarrow~~~ &\n",
    "\\sigma^2_{Y_\\alpha}\\left(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}\\right)~=~\\hat{\\sigma}^2_{Y_\\alpha}\\left(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "from the [definition](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "of the maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c0c8f",
   "metadata": {},
   "source": [
    "## Regression modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729a18e",
   "metadata": {},
   "source": [
    "For the case of regression modelling, we now suppose that the *response* (or *dependent*) variate $X$ is *explained* by one or more\n",
    "exogenous (or *independent*) covariates, collectively denoted by $Z$, via some regression function $\\mathbf{f}(Z,\\boldsymbol{\\phi})$\n",
    "with regression parameter $\\boldsymbol{\\phi}$. \n",
    "In particular, the *unconditional* \n",
    "[distribution](#Probability-distribution-functions \"Section: Probability distribution functions\") \n",
    "$p(X\\mid\\boldsymbol{\\theta})$ is now replaced by a\n",
    "*conditional* distribtution $p(X\\mid\\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})$.\n",
    "\n",
    "The usual [mean regression](#Mean-regression \"Section: Mean regression\") \n",
    "model is to fit $\\mathbf{f}$ to the mean $\\boldsymbol{\\mu}=\\mathbb{E}[X\\mid\\boldsymbol{\\theta}]$\n",
    "of the unconditional distribution $p(X\\mid\\boldsymbol{\\theta})$.\n",
    "In essence, this assumes that the conditional distribution\n",
    "$p(X\\mid\\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})$ takes the same form as the unconditional distribution.\n",
    "We may represent this symbollically by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "The traditional approach is then to estimate the regression parameter $\\boldsymbol{\\phi}$ from the observed data using\n",
    "some form of [least-squares](#Least-squares-regression \"Section: Least-squares regression\") approach that minimises the square\n",
    "error of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250c6c9",
   "metadata": {},
   "source": [
    "One of the issues with such an approach is that the range of the regression function $\\mathbf{f}$ is usually unconstrained, especially for \n",
    "[linear models](#Generalised-linear-models \"Section: Generalised linear models\") \n",
    "like $\\mathbf{f}(\\mathbf{z},\\boldsymbol{\\Phi})=\\boldsymbol{\\Phi}^{T}\\mathbf{z}\\in\\mathbb{R}^d$.\n",
    "However, the permissible values of the mean $\\boldsymbol{\\mu}$ are usually proscribed by the PDF, for example if $X$ represents a proportion.\n",
    "The key innovation of generalised linear modelling (GLM) introduced by\n",
    " Nelder and Wedderburn [[1]](#Citations \"Citation [1]: Generalized Linear Models\") was to regress, not on the mean\n",
    " $\\boldsymbol{\\mu}$ itself, but instead on another parameter $\\boldsymbol{\\eta}$ related by a *link* function $\\mathbf{g}$ to the mean\n",
    "via $\\boldsymbol{\\eta}=\\mathbf{g}(\\boldsymbol{\\mu})$. We shall henceforth refer to $\\boldsymbol{\\eta}$ as the *link parameter*,\n",
    "although this is not standard terminology. This new relationship corresponds to the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xrightarrow{\\mathbf{g}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "Now, for $\\mathbf{f}$ to explain $\\boldsymbol{\\mu}$, we require that the link function $\\mathbf{g}$ be invertible,\n",
    "such that the inverse relationship may be represented by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "In terms of mean regression, we could also collapse this model to become\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}\\circ\\,\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "which corresponds to nonlinear regression, even with a linear regression function $\\mathbf{f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1259b81",
   "metadata": {},
   "source": [
    "We now turn to the issue of how to explain the PDF parameter $\\boldsymbol{\\theta}$ in terms of the mean $\\boldsymbol{\\mu}$, regardless of whether the mean is directly or indirectly obtained from \n",
    "$\\mathbf{f}(Z,\\boldsymbol{\\phi})$.\n",
    "Now, for some simple PDFs, such as the \n",
    "[Bernoulli distribution](#Bernoulli-distribution \"Section: Bernoulii distribution\"),\n",
    "the relationship between the distribution parameter $\\boldsymbol{\\theta}$ and the mean $\\boldsymbol{\\mu}$ is invertible,\n",
    "and we may therefore deduce $\\boldsymbol{\\theta}$ from\n",
    "knowledge of $\\boldsymbol{\\mu}$. In general, however, knowing $\\boldsymbol{\\mu}$ might only give us partial\n",
    "knowledge of $\\boldsymbol{\\theta}$, as is the case, for example, with the \n",
    "[Beta distribution](#Beta-distribution \"Section: Beta distribution\").\n",
    "We therefore propose that the parameter $\\boldsymbol{\\theta}$ may be partitioned into two parts,\n",
    "namely $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$, \n",
    "such that implicitly $\\boldsymbol{\\mu}=\\boldsymbol{\\mu}(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$.\n",
    "Conversely, however, we now suppose that the *independent* parameter $\\boldsymbol{\\psi}$ is not obtainable from\n",
    "$\\boldsymbol{\\mu}$ and so must be estimated separately, but that the *dependent* parameter \n",
    "$\\boldsymbol{\\varphi}$ is obtainable from $\\boldsymbol{\\mu}$, given $\\boldsymbol{\\psi}$, via some implicit\n",
    "function $\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi}, \\boldsymbol{\\mu})$,\n",
    "such that now $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu}))$.\n",
    "This inverted relationship may be represented by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\varphi} & \\xleftarrow{\\boldsymbol{\\iota}^{-1}_\\boldsymbol{\\psi}} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\iota_\\boldsymbol{\\psi}(\\cdot)$ denotes some implicit, quasi-invertible function\n",
    "such that\n",
    "$\\boldsymbol{\\mu}=\\boldsymbol{\\iota}_\\boldsymbol{\\psi}(\\boldsymbol{\\varphi})=\\boldsymbol{\\mu}(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$\n",
    "and \n",
    "$\\boldsymbol{\\varphi}=\\boldsymbol{\\iota}^{-1}_\\boldsymbol{\\psi}(\\boldsymbol{\\mu})=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42934a01",
   "metadata": {},
   "source": [
    "In summary, the generalised modelling approach may be boiled down to three essential requirements:\n",
    "\n",
    "1. The mean $\\boldsymbol{\\mu}$ of the variate $X$ is a function of the\n",
    "[PDF](#Probability-distribution-functions \"Section: Probability distribution functions\")\n",
    "parameter $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$ via the expectation\n",
    "$\\boldsymbol{\\mu}=\\mathbb{E}[X\\mid\\boldsymbol{\\theta}]$, with partial inversion given implicitly via\n",
    "$\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu})$.\n",
    "2. There exists an explicitly invertable *link* function $\\mathbf{g}$ that maps $\\boldsymbol{\\mu}$ into a more convenient *link* parameter\n",
    "$\\boldsymbol{\\eta}$ via $\\boldsymbol{\\eta}=\\mathbf{g}(\\boldsymbol{\\mu})$.\n",
    "3. Each value of the *response* variate $X$ is sampled from a distribution for which the corresponding link parameter $\\boldsymbol{\\eta}$ is determined by a parameterised regression function \n",
    "$\\mathbf{f}(Z,\\boldsymbol{\\phi})$ of exogenous covariates $Z$. \n",
    "\n",
    "We now provide brief explanations of [mean regression](#Mean-regression \"Section: Mean regression\") \n",
    " and\n",
    "[least-squares regression](#Least-squares-regression \"Section: Least-squares regression\"),\n",
    "and then go on to expand upon the above points to derive the \n",
    "[general regression](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\")\n",
    "model, and then discuss its specialisation to a \n",
    "[linear regression](#Generalised-linear-models \"Section: Generalised linear models\")\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb855f7d",
   "metadata": {},
   "source": [
    "### Mean regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081eb27",
   "metadata": {},
   "source": [
    "Consider [again](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") \n",
    "the stochastic sampling process that produces an arbitrary length-$n$ sequence of independent variables,\n",
    "$X_1, X_2, \\ldots, X_n$. However, now we drop the requirement that these variables are identically distributed.\n",
    "Instead, we suppose that (the value of) each $X_i$ is drawn from the same \n",
    "[PDF](#Probability-distribution-functions \"Section: Probability distribution functions\")\n",
    "but with  potentially different parameters $\\boldsymbol{\\theta}_i$, with individual means\n",
    "$\\boldsymbol{\\mu}_{X_i}\\doteq\\mathbb{E}\\left[X_i\\mid\\boldsymbol{\\theta}_i\\right]$\n",
    "and variances $\\boldsymbol{\\Sigma}_{X_i}\\doteq\\texttt{Var}\\left[X_i\\mid\\boldsymbol{\\theta}_i\\right]$.\n",
    "\n",
    "\n",
    "Now, if we were allowed to sample the value of variate $X_i$ multiple times, then we would expect the values\n",
    "to be displaced about the mean $\\boldsymbol{\\mu}_i$ according to\n",
    "\\begin{eqnarray}\n",
    "X_i ~=~ \\boldsymbol{\\mu}_i+\\mathbf{e}_i\n",
    "& ~\\Rightarrow~ & \\mathbf{e}_i~=~X_i-\\boldsymbol{\\mu}_i\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In this context, the displacement $\\boldsymbol{e}_i$ is called the *noise* or the measurement *error*, \n",
    "and arises due to imprecise, stochastic measurements of the unknown mean $\\boldsymbol{\\mu}_i$.\n",
    "This error has the distributional properties\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\mathbf{e}_i\\mid\\boldsymbol{\\theta}_i\\right] & = & \\mathbf{0}\\,,\n",
    "\\\\\n",
    "\\mathbb{E}\\left[\\mathbf{e}_i\\,\\mathbf{e}_i^T\\mid\\boldsymbol{\\theta}_i\\right]\n",
    "& = & \\texttt{Var}\\left[X_i\\mid\\boldsymbol{\\theta}_i\\right]~=~\\boldsymbol{\\Sigma}_{X_i}(\\boldsymbol{\\theta}_i)\\,.\n",
    "\\end{eqnarray}\n",
    "We must keep in mind that this variance is not necessarily constant, but is generally a function of the\n",
    "distributional parameters $\\boldsymbol{\\theta}_i$, especially the\n",
    "[mean](#Seperable-dependencies \"Section: Seperable dependencies\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a83f8",
   "metadata": {},
   "source": [
    "Next, we suppose that associated with each *response* (or *dependent*) variate $X_i$ is a corresponding *exogenous*\n",
    "(or *independent*) covariate $Z_i$. We futher suppose that $Z_i$ explains the mean $\\boldsymbol{\\mu}_i$ of $X_i$ via a parameterised regression function of the form\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_i & = & \\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\mathbf{r}_i\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{r}_i$ is a called the *residual* or *error* of fit. Note that, in contrast to the measurement error, the residual arises due to error in approximating the true mean $\\boldsymbol{\\mu}_i$ with an estimating function\n",
    "$\\hat{\\boldsymbol{\\mu}}_i=\\mathbf{f}(Z_,\\boldsymbol{\\phi})$. However, since the mean $\\boldsymbol{\\mu}_i$ is actually unknown, then we may (conceptually) encode our uncertainty about its true value into another PDF, such that the mean of this PDF obeys\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\boldsymbol{\\mu}_i\\mid Z_i,\\boldsymbol{\\phi}] ~=~\n",
    "\\hat{\\boldsymbol{\\mu}}_i~=~\\mathbf{f}(Z_i,\\boldsymbol{\\phi})\n",
    "& ~\\Rightarrow~ & \n",
    "\\mathbb{E}[\\mathbf{r}_i\\mid Z_i,\\boldsymbol{\\phi}] ~=~ \\mathbf{0}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, this PDF has a variance that measures our uncertainty, such that\n",
    "\\begin{eqnarray}\n",
    "\\texttt{Var}[\\boldsymbol{\\mu}_i\\mid Z_i,\\boldsymbol{\\phi}] & ~=~ &\n",
    "\\mathbb{E}[\\mathbf{r}_i^{}\\,\\mathbf{r}_i^T\\mid Z_i,\\boldsymbol{\\phi}]\n",
    "~=~\\boldsymbol{\\Sigma_{\\mathbf{r}_i}}(Z_i,\\boldsymbol{\\phi})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this variance is generally a function of both the covariates $Z_i$ and the regression parameter\n",
    "$\\boldsymbol{\\phi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c565f9",
   "metadata": {},
   "source": [
    "Combining the two distributions, we now obtain the regression model\n",
    "\\begin{eqnarray}\n",
    "X_i & = & \\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\boldsymbol{\\varepsilon}_i\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\boldsymbol{\\varepsilon}_i=\\mathbf{e}_i+\\mathbf{r}_i$ combines both measurement error and fitting error, and hence may be considered as either an error or a residual. Consequently, we deduce that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\boldsymbol{\\varepsilon}_i\\mid\\boldsymbol{\\theta}_i, Z_i, \\boldsymbol{\\phi}]\n",
    "& = & \\mathbf{0}\\,,\n",
    "\\\\\n",
    "\\mathbb{E}[\\boldsymbol{\\varepsilon}_i\\,\\boldsymbol{\\varepsilon}_i^T\\mid\\boldsymbol{\\theta}_i, Z_i, \\boldsymbol{\\phi}] & = & \n",
    "\\boldsymbol{\\Sigma}_{X_i}(\\boldsymbol{\\theta}_i)\n",
    "+\\boldsymbol{\\Sigma}_{\\mathbf{r}_i}(Z_i,\\boldsymbol{\\phi})\n",
    "~\\doteq~\\boldsymbol{\\Sigma}_i(\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi})\\,,\n",
    "\\end{eqnarray}\n",
    "on the assumption that the residual $\\mathbf{r}_i$ is independent of the noise $\\mathbf{e}_i$.\n",
    "In practice, we do not know the value of $\\boldsymbol{\\Sigma}_i$, and consequently must either estimate it from\n",
    "the empirical distribution of $\\boldsymbol{\\varepsilon}_i$, or else approximate it, for example by\n",
    "$\\boldsymbol{\\Sigma}_{X_i}$, which corresponds to assuming $\\boldsymbol{\\Sigma}_{\\mathbf{r}_i}=\\mathbf{O}$,\n",
    "i.e. being extremely certain of the regression function $\\mathbf{f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88a6e0",
   "metadata": {},
   "source": [
    "### Least-squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96656835",
   "metadata": {},
   "source": [
    "Recall from the [previous](#Mean-regresssion \"Section: Mean regresssion\") section\n",
    "that we are considering the regression model\n",
    "\\begin{eqnarray}\n",
    "X_i~=~\\boldsymbol{\\mu}_i+\\mathbf{e}_i\\,, & \\;\\; \\boldsymbol{\\mu}_i~=~\\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\mathbf{r}_i\n",
    "& ~\\Rightarrow~\n",
    "X_i~=~\\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\boldsymbol{\\varepsilon}_i\\,.\n",
    "\\end{eqnarray}\n",
    "In order to fit the regression function $\\mathbf{f}$ to observed data \n",
    "$\\mathbf{X}\\doteq(X_i)_{i=1}^{n}$ and $\\mathbf{Z}\\doteq(Z_i)_{i=1}^{n}$,\n",
    "we first redefine the\n",
    "sample mean [operator](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") $\\langle\\cdot\\rangle$ to include the covariate $Z$. Consequently, \n",
    "the sample average of an arbitrary function $\\mathbf{g}(X,Z,\\ldots)$ is now given by\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mathbf{g}\\rangle(\\ldots) & \\doteq &\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{g}(X_i,Z_i,\\ldots)\\,,\n",
    "\\end{eqnarray}\n",
    "where the ellipsis \"$\\ldots$\" represents arbitrary parameters that do not vary with $X$ or $Z$.\n",
    "Note that all parameters and variables that depend upon $X_i$ and/or $Z_i$ must also be indexed in this summation,\n",
    "e.g. the residual variance $\\boldsymbol{\\Sigma}_i$. If it becomes necessary to explicitly distinguish between constants and functions of $X_i$ and $Z_i$, then we may retain the subscript, e.g. $\\langle\\boldsymbol{\\Sigma}_i\\boldsymbol{\\phi}\\rangle$. Also note that the sample mean may still be treated as a function of $\\mathbf{X}$ and $\\mathbf{Z}$, when they are considered as variables rather than known, sampled values.\n",
    "This is useful for computing expectations of sample means, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5fd40",
   "metadata": {},
   "source": [
    "The fitting process requires estimating the best value of the function parameter $\\boldsymbol{\\phi}$ that\n",
    "minimises the overall error of fit. The *ordinary* least-squares (OLS) method is to minimise the mean of the squared lengths of the residuals, namely\n",
    "$S(\\boldsymbol{\\phi})=\\left\\langle\\boldsymbol{\\varepsilon}^{T}\\boldsymbol{\\varepsilon}\\right\\rangle$.\n",
    "However, use of OLS makes some implicit assumptions, in particular that:\n",
    "1. The residuals are independent of each other.\n",
    "2. All residuals are equally important.\n",
    "3. The elements of each residual are independent of each other.\n",
    "4. The elements of each residual are equally important.\n",
    "\n",
    "Only the first assumption of residual independence really holds. In practice, if the residuals are independent, then the plot of the fitted residuals against the covariate $Z$ should appear randomly distributed. However, if a pattern appears then the particular choice of the regression function $\\mathbf{f}$ must be reconsidered. The assumption that the elements of each residual $\\boldsymbol{\\varepsilon}_i$ are independent does not hold in general, since\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\boldsymbol{\\varepsilon}_i^{T}\\,\\boldsymbol{\\varepsilon}_i\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\n",
    "& ~=~ &\n",
    "\\mathbb{E}[\n",
    "\\texttt{trace}\\left(\\boldsymbol{\\varepsilon}_i\\,\\boldsymbol{\\varepsilon}_i^T\\right)\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\n",
    "~=~\\texttt{trace}\\left(\\boldsymbol{\\Sigma}_i\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the assumptions of equal importance do not hold either, since a residual (or an element of a residual)\n",
    "with higher variance has higher uncertainty associated with its fit, and hence should be assigned less weight.\n",
    "It turns out that weighting in inverse proportion to the variance is a good idea.\n",
    "Note that use of OLS corresponds to assuming constant and equal variances of the form\n",
    "$\\boldsymbol{\\Sigma}_i=\\mathbf{I}\\,\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af18f98",
   "metadata": {},
   "source": [
    "We can solve both problems of element-wise non-independence and unequal weighting of residuals and elements by applying a so-called\n",
    "*whitening transformation* that decouples within-residual correlations and standardises the variances. This transformation takes the form\n",
    "\\begin{eqnarray}\n",
    "\\tilde{\\boldsymbol{\\varepsilon}}_i & ~\\doteq~ & \n",
    "\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\boldsymbol{\\varepsilon}_i\n",
    "~=~\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\left[X_i-\\mathbf{f}(Z_i,\\boldsymbol{\\phi})\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "such that \n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\tilde{\\boldsymbol{\\varepsilon}}_i^{T}\\,\\tilde{\\boldsymbol{\\varepsilon}}_i\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\n",
    "& ~=~ &\n",
    "\\texttt{trace}\\left(\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\,\\mathbb{E}[\n",
    "\\boldsymbol{\\varepsilon}_i\\,\\boldsymbol{\\varepsilon}_i^T\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\\,\n",
    "\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\right)\n",
    "~=~\\texttt{trace}\\left(\\mathbf{I}\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the *weighted* least-squares (WLS) method is to minimise\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}) & ~=~ &\n",
    "\\left\\langle\\tilde{\\boldsymbol{\\varepsilon}}^{T}\\,\\tilde{\\boldsymbol{\\varepsilon}}\\right\\rangle\n",
    "~=~\\left\\langle\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]^{T}\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]\n",
    "\\right\\rangle\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6082e",
   "metadata": {},
   "source": [
    "However, we [recall](#Mean-regresssion \"Section: Mean regresssion\") that \n",
    "$\\boldsymbol{\\Sigma}_i=\\boldsymbol{\\Sigma}_i(\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi})$ is a function\n",
    "of $\\boldsymbol{\\phi}$. Consequently, WLS is typically a nonlinear problem even when $\\mathbf{f}$ is a linear function of $\\boldsymbol{\\phi}$. To overcome this difficulty, we \n",
    "use an iterative approximation where we\n",
    "evaluate $\\boldsymbol{\\Sigma}_i$ at the\n",
    "previous estimate $\\boldsymbol{\\phi}$, but evaluate $\\mathbf{f}$ at the new estimate\n",
    "$\\boldsymbol{\\phi}'$, resulting in\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}, \\boldsymbol{\\phi}') & ~=~ &\n",
    "\\left\\langle\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi}')\\right]^{T}\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi}')\\right]\n",
    "\\right\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we substitute the Taylor series approximation\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{f}(Z,\\boldsymbol{\\phi}') & ~\\approx~ &\n",
    "\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "+\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\phi}' & ~=~ \\boldsymbol{\\phi}+\\Delta\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "to obtain the new approximation\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}, \\Delta\\boldsymbol{\\phi}) & ~=~ &\n",
    "\\left\\langle\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "-\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\right]^{T}\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "-\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\right]\n",
    "\\right\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, we take the gradient with respect to $\\Delta\\boldsymbol{\\phi}$ to obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_{\\Delta\\boldsymbol{\\phi}}S & ~=~ &\n",
    "-2\\,\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "-\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\right]\n",
    "\\right\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "which vanishes when\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "\\right\\rangle\\,\\Delta\\boldsymbol{\\phi}\n",
    "& ~=~ &\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]\n",
    "\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "This is the nonlinear form of the *iteratively reweighted* least-squares (IRLS) method, due to the fact that the\n",
    "variance $\\boldsymbol{\\Sigma}_i(\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi})$ needs to be\n",
    "re-evaluated after every update of the parameter estimate $\\boldsymbol{\\phi}$.\n",
    "Note that the iterations will cease when $\\Delta\\boldsymbol{\\phi}=\\mathbf{0}$, at which point the \n",
    "solutions satisfies\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]\n",
    "\\right\\rangle & ~=~ & \\mathbf{0}\\,.\n",
    "\\end{eqnarray}\n",
    "This latter is just the solution to $\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}S=\\mathbf{0}$ from the original WLS formulation, on the assumption that $\\boldsymbol{\\Sigma}$ is held constant for the update and recomputed after the update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f92a",
   "metadata": {},
   "source": [
    "### Generalised nonlinear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7b62f",
   "metadata": {},
   "source": [
    "[Previously](#Mean-regresssion \"Section: Mean regresssion\"), we considered the\n",
    "regression of the mean $\\boldsymbol{\\mu}$ via some parameterised function\n",
    "$\\mathbf{f}(Z,\\boldsymbol{\\phi})$ of the covariate $Z$.\n",
    "In my opinion, the key contribution of Nelder and Wedderburn\n",
    "[[1]](#Citations \"Citation [1]: Generalized Linear Models\")\n",
    "to generalised linear modelling (GLM) lies in the fact that we may instead apply regression, not to $\\boldsymbol{\\mu}$,\n",
    "but to some more natural parameterisation $\\boldsymbol{\\eta}=\\mathbf{g}(\\boldsymbol{\\mu})$,\n",
    "where $\\mathbf{g}(\\cdot)$ is known as the *link* function.\n",
    "We may therefore depict the resulting relationships by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\varphi} & \\xrightarrow{\\boldsymbol{\\iota}_\\boldsymbol{\\psi}} & \\boldsymbol{\\mu}\n",
    "\\xrightarrow{\\mathbf{g}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "which may further be interpreted as meaning that the parameters $\\boldsymbol{\\varphi}$ and $\\boldsymbol{\\phi}$ are conditionally independent given $\\boldsymbol{\\eta}$ (and $Z$ and $\\boldsymbol{\\psi}$).\n",
    "It therefore [follows](#Parameter-transformations \"Section: Parameter transformations\") that\n",
    "the gradient of the log-likelihood $L$ with respect to the regression parameter $\\boldsymbol{\\phi}$ takes the form\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}L & = &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\boldsymbol{\\eta}^T\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\n",
    "~=~\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L(\\boldsymbol{\\theta}; X)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that although the last term on the right-hand side appears to be a function only of $X$ and\n",
    "$\\boldsymbol{\\theta}$, we must remember that $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$, and that $\\boldsymbol{\\varphi}$ is related to $\\boldsymbol{\\eta}=\\mathbf{f}(Z,\\boldsymbol{\\phi})$ via the graphical model above.\n",
    "In fact, in order to compute the gradient of the log-likelihood $L$ with respect to the parameter $\\boldsymbol{\\eta}$,\n",
    "we must use the fact that the link function $\\mathbf{g}$ is invertible, as represented\n",
    "by the inverted graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\varphi} & \\xleftarrow{\\boldsymbol{\\iota}_\\boldsymbol{\\psi}^{-1}} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L & = &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\mu}^T\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\mu}\\boldsymbol{\\varphi}^T\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\varphi}L\n",
    "\\\\& = &\n",
    "\\left[\\frac{\\partial\\mathbf{g}^T}{\\partial\\boldsymbol{\\mu}}\\right]^{-1}\\,\n",
    "\\left[\\frac{\\partial\\boldsymbol{\\mu}^T}{\\partial\\boldsymbol{\\varphi}}\\right]^{-1}\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\varphi}L\\,.\n",
    "\\end{eqnarray}\n",
    "Implicitly, we may therefore suppose that $\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu}(Z,\\boldsymbol{\\phi}))$,\n",
    "such that \n",
    "$\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu}(Z,\\boldsymbol{\\phi})))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a53582",
   "metadata": {},
   "source": [
    "The requisite \n",
    "[expectations](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\")\n",
    "are therefore given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}L\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}\\right]\n",
    "& = & \\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~ \\mathbf{0}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}L\n",
    "\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}\\right]\n",
    "& = & -\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, \n",
    "in the special case where there is no independent parameter $\\boldsymbol{\\psi}$,\n",
    "the [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "estimate $\\hat{\\boldsymbol{\\phi}}_\\texttt{ML}$ may be obtained iteratively via updates of the form\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}\n",
    "\\right\\rangle\\,\\Delta\\boldsymbol{\\phi} & = &\n",
    "\\left\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\n",
    "\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213ce2f",
   "metadata": {},
   "source": [
    "It must be noted that the *link* parameter $\\boldsymbol{\\eta}$ here does not necessarily have to be the same as any of the natural *distributional* parameters used\n",
    "[previously](#Seperable-dependencies \"Section: Seperable dependencies\") (which were, unfortunately, also denoted collectively by $\\boldsymbol{\\eta}$). In general they are not the same.\n",
    "Despite this, we [may](#General-form \"Section: General form\")\n",
    "still introduce a new variate $Y_\\boldsymbol{\\eta}$ as a function of $X$ (and possibly $\\boldsymbol{\\theta})$,\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L~=~Y_\\boldsymbol{\\eta}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}\n",
    "& ~~~\\Rightarrow~~~ & \n",
    "\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}~=~\\mathbb{E}\\left[Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The variance is then directly obtained as\n",
    "\\begin{eqnarray}\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\eta}} & ~=~ &\n",
    "\\mathbb{V}\\left[Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "It then follows, again only when there is **no** independent parameter $\\boldsymbol{\\psi}$,\n",
    "that the update for the parameter $\\boldsymbol{\\phi}$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\eta}}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}\n",
    "\\right\\rangle\\,\\Delta\\boldsymbol{\\phi} & = &\n",
    "\\left\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\n",
    "\\left[Y_\\boldsymbol{\\eta}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}\\right]\n",
    "\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Observe the similarity (and dissimilarity) with the nonlinear \n",
    "[IRLS](#Least-squares-regression \"Section: Least-squares regression\") update equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37b4da",
   "metadata": {},
   "source": [
    "In general, however, the parameter $\\boldsymbol{\\psi}$ does exist.\n",
    "Note that although, by definition, $\\boldsymbol{\\psi}$ is independent of \n",
    "$\\boldsymbol{\\phi}$ and $\\boldsymbol{\\eta}$, the estimates of these parameters are **not**\n",
    "independent.  This dependence was demonstrated \n",
    "[previously](#Beta-Bernoulli-distribution \"Section: Beta-Bernoulli distribution\").\n",
    "[Recall](#Regression-modelling \"Section: Regression modelling\") \n",
    "that the regression model itself takes the form\n",
    "$\\boldsymbol{\\eta}=\\mathbf{f}(Z,\\boldsymbol{\\phi})$,\n",
    "and that the dependent parameter $\\boldsymbol{\\varphi}$ is then obtained by inverting the link function \n",
    "$\\boldsymbol{\\eta}=\\mathbf{g}(\\boldsymbol{\\mu})$ to give \n",
    "$\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu})$.\n",
    "It follows that $\\boldsymbol{\\varphi}$ is implicitly a function of $\\boldsymbol{\\psi}$ and $\\boldsymbol{\\eta}$\n",
    "(and thus $\\boldsymbol{\\phi}$). \n",
    "\n",
    "This fact has two immediate consequences. Firstly, $\\boldsymbol{\\eta}$\n",
    "only affects the log-likelihood $L(\\boldsymbol{\\theta};X)$ indirectly via $\\boldsymbol{\\varphi}$,\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L & ~\\doteq~ & \n",
    "\\frac{\\partial\\boldsymbol{\\varphi}^T}{\\partial\\boldsymbol{\\eta}}\\,\\frac{\\partial L}{\\partial\\boldsymbol{\\varphi}}\n",
    "~=~Y_\\boldsymbol{\\eta}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and therefore\n",
    "\\begin{eqnarray}\n",
    "Y_\\boldsymbol{\\phi} & ~\\doteq~ & \\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "Y_\\boldsymbol{\\eta}\\,.\n",
    "\\end{eqnarray}\n",
    "Secondly, $\\boldsymbol{\\psi}$ affects the log-likelihood both directly, as before, and now also indirectly via\n",
    "$\\boldsymbol{\\varphi}$, such that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\psi}L & ~\\doteq~ & \n",
    "\\frac{\\partial L}{\\partial\\boldsymbol{\\psi}}\n",
    "+\\frac{\\partial\\boldsymbol{\\varphi}^T}{\\partial\\boldsymbol{\\psi}}\\,\\frac{\\partial L}{\\partial\\boldsymbol{\\varphi}}\n",
    "~=~Y_\\boldsymbol{\\psi}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\psi}}\\,.\n",
    "\\end{eqnarray}\n",
    "The dependence between the estimates of $\\boldsymbol{\\psi}$ and $\\boldsymbol{\\phi}$ now arises in practice due to the correlation between $Y_\\boldsymbol{\\psi}$ and $Y_\\boldsymbol{\\eta}$ via their covariance, namely\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi},Y_\\boldsymbol{\\eta}} ~\\doteq~\n",
    "\\mathtt{Cov}[Y_\\boldsymbol{\\psi},Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}]\n",
    "& \\;\\;\\;\\Rightarrow\\;\\;\\; & \n",
    "\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi},Y_\\boldsymbol{\\phi}} ~\\doteq~\n",
    "\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi},Y_\\boldsymbol{\\eta}}\\,\n",
    "\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}\\,.\n",
    "\\end{eqnarray}\n",
    "The complete update equation for the nonlinear regression model is therefore\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{cc}\n",
    "\\left\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\eta}}\\,\n",
    "\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}\\right\\rangle &\n",
    "\\left\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^T\\,\n",
    "\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi},Y_\\boldsymbol{\\eta}}^T\\right\\rangle\n",
    "\\\\\n",
    "\\left\\langle\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi},Y_\\boldsymbol{\\eta}}\\,\n",
    "\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}\\right\\rangle &\n",
    "\\left\\langle\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}\\right\\rangle\n",
    "\\end{array}\\right]\\,\n",
    "\\left[\\begin{array}{c}\n",
    "\\Delta\\boldsymbol{\\phi}\n",
    "\\\\\n",
    "\\Delta\\boldsymbol{\\psi}\n",
    "\\end{array}\\right] & ~=~ &\n",
    "\\left[\\begin{array}{c}\n",
    "\\left\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "\\left[Y_\\boldsymbol{\\eta}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}\\right]\\right\\rangle\n",
    "\\\\\n",
    "\\left\\langle Y_\\boldsymbol{\\psi}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\psi}}\\right\\rangle\n",
    "\\end{array}\\right]\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c6928",
   "metadata": {},
   "source": [
    "### Generalised linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343d836",
   "metadata": {},
   "source": [
    "Generalised linear modelling (GLM) now follows immediately from\n",
    "[nonlinear modelling](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\").\n",
    "In theory, the most general linear model is given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{f}(\\mathbf{z},\\boldsymbol{\\Phi}) & \\doteq & \\boldsymbol{\\Phi}^T\\,\\mathbf{z}\\,,\n",
    "\\end{eqnarray}\n",
    "for a multi-dimensional vector $Z=\\mathbf{z}$ of covariates (which may also include a constant component), and \n",
    "a matrix parameter $\\boldsymbol{\\Phi}$. However, we may then subsequently separate the regression function into its components $\\mathbf{f}=[f_i]$ by\n",
    "considering independent scalar functions parameterised by each column \n",
    " of $\\boldsymbol{\\Phi}=[\\boldsymbol{\\phi}_i]$.\n",
    " We may therefore instead assume, without loss of generality, that the regression function takes the simple, scalar form \n",
    "\\begin{eqnarray}\n",
    "\\eta & = & f(Z,\\boldsymbol{\\phi}) ~\\doteq~ \\boldsymbol{\\phi}^T\\,Z\n",
    "~=~Z^{T}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "Note, however, that in the full vector case, we would still need to reconstruct \n",
    "the link parameter vector\n",
    "$\\boldsymbol{\\eta}=[\\eta_i]$ in order to obtain\n",
    "the vector mean $\\boldsymbol{\\mu}=\\mathbf{g}^{-1}(\\boldsymbol{\\eta})$, and consequently \n",
    "the dependent parameter\n",
    "$\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu})$.\n",
    "\n",
    "For scalar link parameter $\\eta$, the general \n",
    "[update equation](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\")\n",
    "now reduces to\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{cc}\n",
    "\\left\\langle\\sigma^2_\\eta\\,ZZ^T\\right\\rangle &\n",
    "\\left\\langle Z\\,\\boldsymbol{\\sigma}_{Y_\\boldsymbol{\\psi},Y_\\eta}^T\\right\\rangle\n",
    "\\\\\n",
    "\\left\\langle\\boldsymbol{\\sigma}_{Y_\\boldsymbol{\\psi},Y_\\eta}\\,Z^T\\right\\rangle &\n",
    "\\left\\langle\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}\\right\\rangle\n",
    "\\end{array}\\right]\\,\n",
    "\\left[\\begin{array}{c}\n",
    "\\Delta\\boldsymbol{\\phi}\n",
    "\\\\\n",
    "\\Delta\\boldsymbol{\\psi}\n",
    "\\end{array}\\right] & ~=~ &\n",
    "\\left[\\begin{array}{c}\n",
    "\\left\\langle(Y_\\eta-\\mu_{Y_\\eta})\\,Z\\right\\rangle\n",
    "\\\\\n",
    "\\left\\langle Y_\\boldsymbol{\\psi}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\psi}}\\right\\rangle\n",
    "\\end{array}\\right]\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3940e",
   "metadata": {},
   "source": [
    "For comparison, let us now take another look at \n",
    "[least-squares](#Least-squares-regression \"Section: Least-squares regression\")\n",
    "regression. \n",
    "We start with WLS using the weighted residual $\\tilde{\\varepsilon}$ such that the square error is\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}) & ~=~ & \\left\\langle\\tilde{\\varepsilon}^2\\right\\rangle\n",
    "~=~\\left\\langle\\frac{(Y_\\eta-\\mu_{Y_\\eta})^2}{\\sigma_{Y_\\eta}^2}\\right\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we make use of IRLS by temporarily holding $\\sigma^2_{Y_\\eta}$ constant for each iterative update, and then taking the chain of gradients\n",
    "with respect to first $\\mu_{Y_\\eta}$, then $\\eta$, and finally $\\boldsymbol{\\phi}$.\n",
    "We make use of the [fact](#General-form \"Section: General form\") that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\mu_{Y_\\eta}}{\\partial\\eta} & = & \\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]\\,,\n",
    "\\end{eqnarray}\n",
    "for any arbitrary, scalar link parameter $\\eta$.\n",
    "It then follows that the approximate gradient of $S(\\boldsymbol{\\phi})$ with respect to $\\boldsymbol{\\phi}$\n",
    "is given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\,S & ~\\approx~ & \n",
    "-2\\,\\left\\langle\\frac{Y_\\eta-\\mu_{Y_\\eta}}{\\sigma^2_{Y_\\eta}}\\,\\frac{\\partial\\mu_{Y_\\eta}}{\\partial\\eta}\\,\\frac{\\partial\\eta}{\\partial\\boldsymbol{\\phi}}\n",
    "\\right\\rangle\n",
    "~=~-2\\,\\left\\langle(Y_\\eta-\\mu_{\\small Y_\\eta})\\,\n",
    "\\frac{\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]}{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we temporarily hold the expectation of $\\nabla_\\eta Y_\\eta$ constant, and take a second gradient to obtain\n",
    "the approximate Hessian as\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\,S & ~\\approx~ & \n",
    "2\\left\\langle\\frac{\\partial\\mu_{Y_\\eta}}{\\partial\\eta}\\,\\frac{\\partial\\eta}{\\partial\\boldsymbol{\\phi}}\\,\n",
    "\\frac{\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]}{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z^T\\right\\rangle\n",
    "\\\\\n",
    "&=& 2\\left\\langle\n",
    "\\frac{\\left(\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]\\right)^2}\n",
    "{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\,Z^T\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, we use the Newton-Raphson approach to obtain the approximate update\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\frac{\\left(\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]\\right)^2}\n",
    "{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\,Z^T\\right\\rangle\n",
    "\\,\\Delta\\boldsymbol{\\phi} & ~=~ &\n",
    "\\left\\langle(Y_\\eta-\\mu_{Y_\\eta})\\,\n",
    "\\frac{\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]}{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In the special case where $\\eta$ is a natural parameter, such that $\\nabla_\\eta Y_\\eta=0$, then this \n",
    "reduces to\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\sigma^2_{Y_\\eta}\n",
    "\\,Z\\,Z^T\\right\\rangle\n",
    "\\,\\Delta\\boldsymbol{\\phi} & ~=~ &\n",
    "\\left\\langle(Y_\\eta-\\mu_{Y_\\eta})\\,\n",
    "\\,Z\\right\\rangle\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which is just the update equation for $\\boldsymbol{\\phi}$ when there is no independent parameter $\\boldsymbol{\\psi}$.\n",
    "In other words, the [IRLS](#Least-squares-regression \"Section: Least-squares regression\") \n",
    "method only becomes identical to the \n",
    "[maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") \n",
    "approach in the special case for a single-parameter distribution\n",
    "for which $\\eta$ is both the natural parameter and the link parameter.\n",
    "This is important to note because Nelder and Wedderburn [[1]](#Citations \"Citation [1]: Generalized Linear Models\")\n",
    "claim equivalence of IRLS and ML but only actually demonstrate it for the special case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a360daf",
   "metadata": {},
   "source": [
    "## Linear distributional regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf6de6",
   "metadata": {},
   "source": [
    "In the following sections, we consider, for convenience, the linear regression function $\\eta=Z^T\\boldsymbol{\\phi}$.\n",
    "We examine what form the \n",
    "[maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") \n",
    "equations take for the regression parameter $\\boldsymbol{\\phi}$, for a variety of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78776941",
   "metadata": {},
   "source": [
    "### Bernoulli regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e638201",
   "metadata": {},
   "source": [
    "Recall that the [Bernoulli distribution](#Bernoulli-distribution \"Section: Bernoulli distribution\")\n",
    "is parameterised by $\\theta$ and has mean $\\mu=\\theta$ and variance\n",
    "$\\sigma_X^2=\\theta\\,(1-\\theta)$. We also recall that the natural parameterisation of the distribution is\n",
    "$\\eta=\\sigma^{-1}(\\theta)$, where $\\sigma^{-1}(\\cdot)$ is the logit function, such that the natural gradient of\n",
    "the log-likelihood $L$ is $\\nabla_\\eta L=X-\\mu$. Also, since $\\mu=\\theta$, we find that \n",
    "$\\eta=\\sigma^{-1}(\\mu)$ is the natural parameterisation of the mean $\\mu$.\n",
    "Therefore, the dependent parameter is $\\varphi=\\theta$, and there is no independent parameter $\\psi$.\n",
    "\n",
    "We [deduce](#Generalised-linear-models \"Section: Generalised linear models\") that \n",
    "the iterative parameter update for $\\boldsymbol{\\phi}$ therefore takes the form\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mu\\,(1-\\mu)\\,ZZ^T\\rangle\\,\\Delta\\boldsymbol{\\phi}\n",
    "& ~=~ & \\langle (X-\\mu)\\,Z\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Also note that here $Y_\\eta=X$ is the natural variate, and $\\eta$ is both the natural parameter and the link parameter. Hence,\n",
    "from the [previous](#Generalised-linear-models \"Section: Generalised linear models\") section, we see that the Bernoulli\n",
    "distribution is one of the special cases where the \n",
    "[maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") approach\n",
    "is equivalent to an iteratively reweighted \n",
    "[least-squares](#Least-squares-regression \"Section: Least-squares regression\") (IRLS) approach.\n",
    "\n",
    "Note that the resulting regression model is\n",
    "\\begin{eqnarray}\n",
    "X\\mid Z,\\boldsymbol{\\phi} & ~\\sim~ & \\texttt{Bern}\\left(\\sigma(Z^T\\boldsymbol{\\phi})\\right)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f0cae",
   "metadata": {},
   "source": [
    "### Poisson regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ee73e",
   "metadata": {},
   "source": [
    "Recall that the [Poisson distribution](#Poisson-distribution \"Section: Poison distribution\") has mean and variance given by\n",
    "$\\mu_X=\\sigma^2_X=\\lambda$, and natural parameter $\\eta=\\ln\\lambda$. Thus, we take the natural parameter to be the link parameter, and consequently\n",
    "[obtain](#Generalised-linear-models \"Section: Generalised linear models\") the iterative parameter update\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mu_X\\,ZZ^T\\rangle\\,\\Delta\\boldsymbol{\\phi}\n",
    "& ~=~ & \\langle (X-\\mu_X)\\,Z\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "This is equivalent to IRLS. Keep in mind that the mean $\\mu_X$ is recomputed at each iteration from the regression function via the inverse link function, namely\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & ~=~ & e^\\eta ~=~ e^{Z^T\\boldsymbol{\\phi}}\\,.\n",
    "\\end{eqnarray}\n",
    "Also note that $\\mu_X$ is computed within each iteration for every observed pair $(X,Z)$ of variate and covariates.\n",
    "The resulting regression model is therefore\n",
    "\\begin{eqnarray}\n",
    "X\\mid Z,\\boldsymbol{\\phi} & ~\\sim~ & \\texttt{Poiss}\\left(e^{Z^T\\boldsymbol{\\phi}}\\right)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498d654",
   "metadata": {},
   "source": [
    "### Beta regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf7e66",
   "metadata": {},
   "source": [
    "Recall that the [Beta distribution](#Beta-distribution \"Section: Beta distribution\")\n",
    "has natural parameters $\\alpha,\\beta>0$, with mean\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & \\frac{\\alpha}{\\alpha+\\beta}~=~\\frac{1}{1+\\frac{\\beta}{\\alpha}}\n",
    "~=~\\frac{1}{1+e^{-\\ln\\frac{\\alpha}{\\beta}}}\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, a useful choice of link parameter is\n",
    "\\begin{eqnarray}\n",
    "\\eta & ~=~ & \\ln\\frac{\\alpha}{\\beta}~=~\\sigma^{-1}(\\mu)\\,,\n",
    "\\end{eqnarray}\n",
    "which satisfies the dual constraints that $\\eta\\in\\mathbb{R}$ and $\\mu\\in(0,1)$.\n",
    "\n",
    "We may now invert this relationship to obtain either $\\alpha=\\beta\\,e^\\eta$ or $\\beta=\\alpha\\,e^{-\\eta}$.\n",
    "Note that since we cannot recover both $\\alpha$ and $\\beta$ from $\\mu=\\sigma(\\eta)$, we must choose one of these parameters to be the independent parameter $\\psi$, and the other to be the dependent parameter $\\varphi$.\n",
    "Following Kieschnick and McCullough \n",
    "[[3]](#Citations \"Citation [3]: Regression analysis of variates observed on $(0, 1)$\"),\n",
    "we choose $\\psi=\\alpha$ and $\\varphi=\\beta=\\alpha\\,e^{-\\eta}$.\n",
    "Consequently, we take the distributional parameter $\\psi$ and the link parameter $\\eta=f(Z,\\boldsymbol{\\phi})$ \n",
    "to be independent of each other.\n",
    "\n",
    "The required log-likelihood gradients are therefore \n",
    "[given](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\")\n",
    "by\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\eta L & ~\\doteq~ & \n",
    "\\frac{\\partial\\beta}{\\partial\\eta}\\,\\frac{\\partial L}{\\partial\\beta}\n",
    "~=~-\\beta\\,(Y_\\beta-\\mu_{Y_\\beta})\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\psi L & ~\\doteq~ & \\frac{\\partial L}{\\partial\\alpha}\n",
    "+\\frac{\\partial\\beta}{\\partial\\alpha}\\,\\frac{\\partial L}{\\partial\\beta}\n",
    "~=~(Y_\\alpha-\\mu_{Y_\\alpha})+\\frac{\\beta}{\\alpha}\\,(Y_\\beta-\\mu_{Y_\\beta})\\,,\n",
    "\\end{eqnarray}\n",
    "where $Y_\\alpha=\\ln X$ and $Y_\\beta=\\ln(1-X)$ are the natural variates\n",
    "corresponding to the natural parameters $\\alpha$ and $\\beta$, respectively,\n",
    "and $\\mu_{Y_\\alpha}$ and $\\mu_{Y_\\beta}$ are their respective means. \n",
    "We may therefore define the corresponding *link* variate \n",
    "\\begin{eqnarray}\n",
    "Y_\\eta & ~\\doteq~ & -\\beta Y_\\beta~=~-\\beta\\ln(1-X)\\,, \n",
    "\\end{eqnarray}\n",
    "with mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\eta} & ~\\doteq~ & \\mathbb{E}[Y_\\eta\\mid\\alpha,\\beta]~=~-\\beta\\mu_{Y_\\beta}\\,,\n",
    "\\\\\n",
    "\\sigma^2_{Y_\\eta} & ~\\doteq~ & \\mathbb{V}[Y_\\eta\\mid\\alpha,\\beta]~=~\\beta^2\\sigma^2_{Y_\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "Similarly, the *independent* variate is defined as \n",
    "\\begin{eqnarray}\n",
    "Y_\\psi & ~\\doteq~ & Y_\\alpha+\\frac{\\beta}{\\alpha}Y_\\beta~=~\\ln X+\\frac{\\beta}{\\alpha}\\,\\ln(1-X)\\,,\n",
    "\\end{eqnarray}\n",
    "with mean\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\psi} & ~\\doteq~ & \\mathbb{E}[Y_\\psi\\mid\\alpha,\\beta] ~=~ \\mu_{Y_\\alpha}+\\frac{\\beta}{\\alpha}\\,\\mu_{Y_\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "and variance\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\psi} & ~\\doteq~ \\texttt{Var}[Y_\\psi\\mid\\alpha,\\beta]\n",
    "~=~\\sigma^2_{Y_\\alpha}+\\frac{\\beta^2}{\\alpha^2}\\,\\sigma^2_{Y_\\beta}\n",
    "+\\frac{2\\beta}{\\alpha}\\sigma_{Y_\\alpha,Y_\\beta}\\,.\n",
    "\\end{eqnarray}\n",
    "The covariance between $Y_\\psi$ and $Y_\\eta$ is therefore\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\psi,Y_\\eta} & ~\\doteq~ & \\mathtt{Cov}[Y_\\psi,Y_\\eta\\mid\\alpha,\\beta]\n",
    "~=~-\\beta\\,\\sigma_{Y_\\alpha,Y_\\beta}-\\frac{\\beta^2}{\\alpha}\\,\\sigma^2_{Y_\\beta}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca9596",
   "metadata": {},
   "source": [
    "The iterative updates for the regression parameter $\\boldsymbol{\\phi}$ and \n",
    "the independent parameter $\\psi=\\alpha$ therefore take the form\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{cc}\n",
    "\\left\\langle\\sigma^2_\\eta\\,ZZ^T\\right\\rangle &\n",
    "\\left\\langle Z\\,\\sigma_{Y_\\psi,Y_\\eta}^T\\right\\rangle\n",
    "\\\\\n",
    "\\left\\langle\\sigma_{Y_\\psi,Y_\\eta}\\,Z^T\\right\\rangle &\n",
    "\\left\\langle\\sigma^2_\\psi\\right\\rangle\n",
    "\\end{array}\\right]\\,\n",
    "\\left[\\begin{array}{c}\n",
    "\\Delta\\boldsymbol{\\phi}\n",
    "\\\\\n",
    "\\Delta\\boldsymbol{\\alpha}\n",
    "\\end{array}\\right] & ~=~ &\n",
    "\\left[\\begin{array}{c}\n",
    "\\left\\langle(Y_\\eta-\\mu_{Y_\\eta})\\,Z\\right\\rangle\n",
    "\\\\\n",
    "\\left\\langle Y_\\psi-\\mu_{Y_\\psi}\\right\\rangle\n",
    "\\end{array}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "The resulting regression model is given by\n",
    "\\begin{eqnarray}\n",
    "X\\mid Z, \\alpha, \\boldsymbol{\\phi} & ~\\sim~ & \\texttt{Beta}\\left(\\alpha, \\alpha\\,e^{-Z^T\\boldsymbol{\\phi}}\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the estimated value of $\\alpha$ has no influence on the mean of the cconditional distribution, but helps to control\n",
    "the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3531a4",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec99e3c",
   "metadata": {},
   "source": [
    "Recall that the [Beta-Bernoulli distribution](#Beta-Bernoulli-distribution \"Section: Beta-Bernoulli distribution\"),\n",
    "namely\n",
    "\\begin{eqnarray}\n",
    "P(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{\\alpha^x\\,\\beta^{1-x}}{\\alpha+\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "has mean $\\mu_X$ and variance $\\sigma^2_X$ given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_X~=~\\frac{\\alpha}{\\alpha+\\beta}\\,, & \\;\\;\\mbox{and}\\;\\; & \\sigma^2_X~=~\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively, \n",
    "along with a natural parameter $\\eta$ given by\n",
    "\\begin{eqnarray}\n",
    "\\eta & ~=~ & \\ln\\frac{\\alpha}{\\beta}~=~\\sigma^{-1}(\\mu)\\,.\n",
    "\\end{eqnarray}\n",
    "We therefore deduce that $\\eta$ is also the natural link parameter for a logit link function.\n",
    "Following [Beta regression](#Beta-regression \"Section: Beta regression\"), \n",
    "we take $\\eta$ as the independent link parameter,\n",
    "$\\psi=\\alpha$ as the independent distributional parameter, and \n",
    "$\\varphi=\\beta=\\alpha\\,e^{-\\eta}$ as the dependent distributional parameter. \n",
    "This effectively reparameterises the distribution from \n",
    "$\\boldsymbol{\\theta}=(\\alpha,\\beta)$ to\n",
    "$\\boldsymbol{\\theta}'\\doteq(\\eta,\\psi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48db1c",
   "metadata": {},
   "source": [
    "Now, the derivative of the log-likelihood with respect to $\\eta$ is therefore\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\eta L & ~\\doteq~ & \n",
    "\\frac{\\partial\\beta}{\\partial\\eta}\\,\\frac{\\partial L}{\\partial\\beta}\n",
    "~=~-\\beta\\,(Y_\\beta-\\mu_{Y_\\beta})\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "Y_\\eta & ~\\doteq~ & -\\beta\\,Y_\\beta~=~X\\,.\n",
    "\\end{eqnarray}\n",
    "The corresponding derivative with respect to $\\psi$ is \n",
    "[given](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\")\n",
    "by\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\psi L & ~\\doteq~ & \\frac{\\partial L}{\\partial\\alpha}\n",
    "+\\frac{\\partial\\beta}{\\partial\\alpha}\\,\\frac{\\partial L}{\\partial\\beta}\n",
    "~=~(Y_\\alpha-\\mu_{Y_\\alpha})+\\frac{\\beta}{\\alpha}\\,(Y_\\beta-\\mu_{Y_\\beta})\\,,\n",
    "\\end{eqnarray}\n",
    "from which we deduce that\n",
    "\\begin{eqnarray}\n",
    "Y_\\psi & ~\\doteq~ & Y_\\alpha+\\frac{\\beta}{\\alpha}\\,Y_\\beta~\\equiv~ 0\\,.\n",
    "\\end{eqnarray}\n",
    "We conclude that there is no valid update equation for $\\psi=\\alpha$, which should therefore remain constant.\n",
    "\n",
    "However, we [previously](#Beta-Bernoulli-distribution \"Section: Beta-Bernoulli distribution\") noted\n",
    "that we can, in practice, utilise an approximate update scheme for $\\alpha$ by neglecting the known correlation between $Y_\\alpha$ and $Y_\\beta$. Taking a similar approach here, we first **redefine** \n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\psi L & ~\\doteq~ & \\frac{\\partial L}{\\partial\\alpha}\n",
    "~=~(Y_\\alpha-\\mu_{Y_\\alpha})\\,,\n",
    "\\end{eqnarray}\n",
    "such that now\n",
    "\\begin{eqnarray}\n",
    "Y_\\psi & ~\\doteq~ & Y_\\alpha~=~\\frac{X}{\\alpha}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the covariance between $Y_\\eta$ and $Y_\\psi$ is now given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\eta,Y_\\psi} & ~\\doteq~ & \\mathtt{Cov}\\left[Y_\\eta,Y_\\psi\\mid\\alpha,\\beta\\,\\right]\n",
    "~=~\\frac{\\sigma^2_X}{\\alpha}\\,,\n",
    "\\end{eqnarray}\n",
    "such that the corresponding covariance matrix for $Y_{\\boldsymbol{\\theta}'}\\doteq(Y_\\eta,Y_\\psi)$ is now given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}'} & ~\\doteq~ &\n",
    "\\mathtt{Var}\\left[Y_{\\boldsymbol{\\theta}'}\\mid\\alpha,\\beta\\,\\right]\n",
    "~=~\n",
    "\\left[\\begin{array}{cc}\n",
    "\\sigma^2_X & \\frac{\\sigma^2_X}{\\alpha}\n",
    "\\\\\n",
    "\\frac{\\sigma^2_X}{\\alpha} & \\frac{\\sigma^2_X}{\\alpha^2}\n",
    "\\end{array}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Unfortunately, this approximate covariance matrix is singular, just as the original covariance matrix\n",
    "$\\Sigma_{Y_\\boldsymbol{\\theta}}$ was also singular.\n",
    "Therefore, we additionally neglect the known correlation between $Y_\\eta$ and $Y_\\psi$, and take the approximate\n",
    "regression update equation to be\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{cc}\n",
    "\\left\\langle\\sigma^2_\\eta\\,ZZ^T\\right\\rangle &\n",
    "0\n",
    "\\\\\n",
    "0 &\n",
    "\\left\\langle\\sigma^2_\\psi\\right\\rangle\n",
    "\\end{array}\\right]\\,\n",
    "\\left[\\begin{array}{c}\n",
    "\\Delta\\boldsymbol{\\phi}\n",
    "\\\\\n",
    "\\Delta\\boldsymbol{\\alpha}\n",
    "\\end{array}\\right] & ~=~ &\n",
    "\\left[\\begin{array}{c}\n",
    "\\left\\langle(Y_\\eta-\\mu_{Y_\\eta})\\,Z\\right\\rangle\n",
    "\\\\\n",
    "\\left\\langle Y_\\psi-\\mu_{Y_\\psi}\\right\\rangle\n",
    "\\end{array}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "The resulting regression model is therefore\n",
    "\\begin{eqnarray}\n",
    "X\\mid Z, \\alpha, \\boldsymbol{\\phi} & ~\\sim~ & \\texttt{BetaBern}\\left(\\alpha,\\alpha\\,e^{-Z^T\\boldsymbol{\\phi}}\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "However, note that, in practice, the estimated value of $\\alpha$ has no influence on either the mean or the variance of\n",
    "the conditional distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73bb07",
   "metadata": {},
   "source": [
    "### Gamma regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12d40b",
   "metadata": {},
   "source": [
    "Recall that the  [Gamma distribution](#Gamma-distribution \"Section: Gamma distribution\") has natural parameters\n",
    "$\\alpha,\\beta>0$, with corresponding natural variates $Y_\\alpha=\\ln X$ and $Y_\\beta=-X$. The mean and variance of the distribution are\n",
    "given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_X~=~\\frac{\\alpha}{\\beta}\\,, & \\;\\;~\\mbox{and}~\\;\\; & \\sigma^2_X~=~\\frac{\\alpha}{\\beta^2}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. We choose the link parameter to be\n",
    "\\begin{eqnarray}\n",
    "\\eta~=~\\ln\\frac{\\alpha}{\\beta}~=~\\ln\\mu_X & ~\\Rightarrow~ & \\mu_X~=~e^\\eta\\,,\n",
    "\\end{eqnarray}\n",
    "which satisfies $\\eta\\in\\mathbb{R}$ and $\\mu_X>0$.\n",
    "\n",
    "Now, following an [earlier](#Beta-distribution \"Section: Beta distribution\") suggestion,\n",
    "we take either $\\alpha$ or $\\beta$ to be the independent parameter, and let the other be the dependent parameter.\n",
    "For example, if we were to estimate $\\alpha$ and $\\beta$ by the method of matching moments, then we could either\n",
    "estimate\n",
    "\\begin{eqnarray}\n",
    "\\alpha~=~\\frac{\\mu_X^2}{\\sigma_X^2} & ~~~\\Rightarrow~~~ & \\beta~=~\\frac{\\alpha}{\\mu_X}\\,,\n",
    "\\end{eqnarray}\n",
    "or\n",
    "\\begin{eqnarray}\n",
    "\\beta~=~\\frac{\\mu_X}{\\sigma_X^2} & ~~~\\Rightarrow~~~ & \\alpha~=~\\beta\\,\\mu_X\\,.\n",
    "\\end{eqnarray}\n",
    "To help us choose, we note the\n",
    "[fact](https://en.wikipedia.org/wiki/Beta_distribution \"Wikipedia: Beta distribution\")\n",
    "that if $X\\sim\\texttt{Gamma}(\\alpha,\\beta)$ and $Y\\sim\\texttt{Gamma}(\\nu,\\beta)$ then the proportion\n",
    "$\\frac{X}{X+Y}$ is distributed as\n",
    "\\begin{eqnarray}\n",
    "\\frac{X}{X+Y} & ~\\sim~ & \\texttt{Beta}(\\alpha,\\nu)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we assume the scale parameter $\\beta$ to be common across all observations.\n",
    "Hence, we take the independent parameter to be $\\psi=\\beta$, and the dependent parameter to be $\\varphi=\\alpha=\\beta\\,e^{\\eta}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6b755",
   "metadata": {},
   "source": [
    "Subsequently, we take the distributional parameter $\\psi$ \n",
    "to be independent of the link parameter $\\eta$, and thus independent of the regression parameter\n",
    "$\\boldsymbol{\\phi}$ of the regression function $\\eta=f(Z,\\boldsymbol{\\phi})$.\n",
    "The required log-likelihood gradients are now [given](#Generalised-linear-models \"Section: Generalised linear models\") by\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\eta L & ~\\doteq~ &\n",
    "\\frac{\\partial\\alpha}{\\partial\\eta}\\,\\frac{\\partial L}{\\partial\\alpha}\n",
    "~=~\\alpha\\,(Y_\\alpha-\\mu_{Y_\\alpha})\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\psi L & ~\\doteq~ &\n",
    "\\frac{\\partial L}{\\partial\\beta}+\\frac{\\partial\\alpha}{\\partial\\beta}\\,\\frac{\\partial L}{\\partial\\alpha}\n",
    "~=~(Y_\\beta-\\mu_{Y_\\beta})+\\frac{\\alpha}{\\beta}\\,(Y_\\alpha-\\mu_{Y_\\alpha})\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we define the link variate as\n",
    "\\begin{eqnarray}\n",
    "Y_\\eta & ~\\doteq~ & \\alpha\\,Y_\\alpha~=~\\alpha\\,\\ln X\\,,\n",
    "\\end{eqnarray}\n",
    "with mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\eta} & ~\\doteq~ & \\mathbb{E}[Y_\\eta|\\alpha,\\beta]~=~\\alpha\\,\\mu_{\\ln X}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\eta} & ~=~ & \\texttt{Var}[Y_\\eta\\mid\\alpha,\\beta]~=~\\alpha^2\\,\\sigma^2_{\\ln X}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Similarly, we define the independent variate as\n",
    "\\begin{eqnarray}\n",
    "Y_\\psi & ~\\doteq~ & Y_\\beta+\\frac{\\alpha}{\\beta}\\,Y_\\alpha~=~-X+\\frac{\\alpha}{\\beta}\\,\\ln X\\,,\n",
    "\\end{eqnarray}\n",
    "with mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\psi} & ~\\doteq~ & \\mathbb{E}[Y_\\psi|\\alpha,\\beta]~=~\n",
    "-\\mu_X+\\frac{\\alpha}{\\beta}\\,\\mu_{\\ln X}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\psi} & ~\\doteq~ & \\texttt{Var}[Y_\\psi\\mid\\alpha,\\beta]\n",
    "~=~\\sigma^2_X+\\frac{\\alpha^2}{\\beta^2}\\,\\sigma^2_{\\ln X}-\\frac{2\\alpha}{\\beta}\\,\\sigma_{X,\\ln X}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "Finally, we note that the covariance between $Y_\\eta$ and $Y_\\psi$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\eta,Y_\\psi} & ~\\doteq~ & \\texttt{Cov}\\left[Y_\\eta,Y_\\psi\\mid\\alpha,\\beta\\,\\right]\n",
    "~=~-\\alpha\\,\\sigma_{X,\\ln X}+\\frac{\\alpha^2}{\\beta}\\,\\sigma^2_{\\ln X}\\,.\n",
    "\\end{eqnarray}\n",
    "Taking $\\boldsymbol{\\theta}'\\doteq(\\eta,\\psi)$, we may determine that\n",
    "\\begin{eqnarray}\n",
    "\\left|\\Sigma_{Y_{\\boldsymbol{\\theta}'}}\\right| & ~=~ & \n",
    "\\alpha^2\\left(\\sigma^2_X\\,\\sigma^2_{\\ln X}-\\sigma_{X,\\ln X}^2\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "such that the covariance matrix $\\Sigma_{Y_{\\boldsymbol{\\theta}'}}$ is non-singular.\n",
    "Consequently, we may utilise the \n",
    "[standard](#Generalised-linear-models \"Section: Generalised linear models\")\n",
    "parameter update equations for $\\psi=\\beta$ and $\\boldsymbol{\\phi}$. \n",
    "The resulting regression model is\n",
    "\\begin{eqnarray}\n",
    "X\\mid Z,\\beta,\\boldsymbol{\\phi} & ~\\sim~ & \\texttt{Gamma}\\left(\\beta\\,e^{Z^T\\boldsymbol{\\phi}},\\beta\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the estimated value of $\\beta$ has no influence on the mean of the conditional distribution, but\n",
    "acts to control the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2decae",
   "metadata": {},
   "source": [
    "## Regression modelling revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1650cef",
   "metadata": {},
   "source": [
    "Up until this point we have naively assumed that the covariates $Z$ give rise to a regression function of the\n",
    "linear form $f(Z,\\boldsymbol{\\phi})=\\boldsymbol{\\phi}^TZ$. Essentially, we have assumed that the underlying\n",
    "covariates have already been transformed into features suitable for linear regression. This of course begs the question: what are these suitable transformations and how are they derived? \n",
    "In order to answer this question, we turn to an alternative approach of deriving probabilistic regression models, offered by\n",
    "Bergtold et al. [[4]](#Citations \"Citation [4]: Bernoulli Regression Models\"). \n",
    "\n",
    "Under this \"*probabilistic reduction*\" framework, both the dependent variate $X\\in\\mathcal{X}$ and the independent covariates $Z\\in\\mathcal{Z}$ are jointly modelled.\n",
    "For such a joint density to exist, it must be able to be factored into conditionals into two different ways, namely\n",
    "\\begin{eqnarray}\n",
    "p(X,Z\\mid\\Theta) & ~=~ & p(X\\mid\\Theta)\\,p(Z\\mid X,\\Theta)\n",
    "~=~ p(Z\\mid\\Theta)\\,p(X\\mid Z,\\Theta)\\,.\n",
    "\\end{eqnarray}\n",
    "For convenience, we reintroduce the prior (now marginal) distribution parameter $\\boldsymbol{\\theta}$ \n",
    "for the response variate $X$, and the regression parameter $\\boldsymbol{\\phi}$ for the covariates $Z$. \n",
    "Conversely, we now introduce corresponding parameters $\\boldsymbol{\\pi}$ and $\\boldsymbol{\\zeta}$ for \n",
    "$X$ and $Z$, rerspectively.\n",
    "Consequently, we take $\\Theta=(\\boldsymbol{\\theta},\\boldsymbol{\\pi},\\boldsymbol{\\phi},\\boldsymbol{\\zeta})$ such that\n",
    "\\begin{eqnarray}\n",
    "p(X,Z\\mid\\Theta) & ~=~ & p(X\\mid\\boldsymbol{\\theta})\\,p(Z\\mid\\boldsymbol{\\pi},X,\\boldsymbol{\\zeta})\n",
    "~=~ p(Z\\mid\\boldsymbol{\\pi})\\,p(X\\mid\\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "We may now combine both factorisations together and rearrange terms to obtain\n",
    "\\begin{eqnarray}\n",
    "p(X\\mid\\boldsymbol{\\theta}) & = &\n",
    "\\frac{p(Z\\mid\\boldsymbol{\\pi})\\,p(X\\mid\\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})}{p(Z\\mid\\boldsymbol{\\pi},X,\\boldsymbol{\\zeta})}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the apparent dependency of the right-hand side on $Z$ must actually cancel out, since the left-hand side is purely a function of $X$.\n",
    "\n",
    "The next step of \"*probabilistic reduction*\" is to consider two distinct values of $X$, say $x_0,x_1\\in\\mathcal{X}$, and evaluate the above formula at each point. Taking the ratio of the formula for the two values, rearranging terms, and taking the logarithm, we then obtain the *log-ratio formlua*:\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(X=x_1\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})}{p(X=x_0\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})}\n",
    "& ~=~ &\n",
    "\\ln\\frac{p(X=x_1\\mid\\boldsymbol{\\theta})}{p(X=x_0\\mid\\boldsymbol{\\theta})}\n",
    "+\\ln\\frac{p(Z\\mid\\boldsymbol{\\pi},X=x_1,\\boldsymbol{\\zeta})}{p(Z\\mid\\boldsymbol{\\pi},X=x_0,\\boldsymbol{\\zeta})}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the term on the left-hand side determines the form of the regression model, and depends upon the\n",
    "choice of the distribution of the dependent variate $X$, as we shall see in following sections.\n",
    "Also note that first term on the right-hand side\n",
    "is (for pre-determined $x_0$ and $x_1$) just a function of the (unknown) parameter $\\boldsymbol{\\theta}$,\n",
    "which we shall represent as $\\kappa(\\boldsymbol{\\theta})$.\n",
    "Hence, this term corresponds to a constant term in the regression function arguments.\n",
    "The remaining regression terms, as functions of the covariates $Z$, arise from the second term on the right-hand side, and depend upon the choice of the covariate conditional distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e71646",
   "metadata": {},
   "source": [
    "In the approach of Bergtold et al. [[4]](#Citations \"Citation [4]: Bernoulli Regression Models\"),\n",
    "we want to determine the form of the regression features based upon the choice of the covariate (marginal) distribution\n",
    "$p(Z\\mid\\boldsymbol{\\pi})$. Consequently, we assume that the conditional distribution\n",
    "$p(Z\\mid\\boldsymbol{\\pi},X,\\boldsymbol{\\zeta})$ takes the same functional form as the marginal distribution.\n",
    "One way to achieve this aim is to assume that each observed value $X=x\\in\\mathcal{X}$ of the response variate selects a specific\n",
    "marginal distribution for the covariates $Z$, corresponding to the model\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\boldsymbol{\\pi}}(x) ~\\doteq~ \\mathbf{G}(x;\\boldsymbol{\\pi},\\boldsymbol{\\zeta})\\,,\n",
    "& \\;\\;\\;\\; &\n",
    "p(Z\\mid\\boldsymbol{\\pi},X,\\boldsymbol{\\zeta})~\\doteq~p(Z\\mid\\hat{\\boldsymbol{\\pi}}(X))\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, letting $\\hat{\\boldsymbol{\\pi}}_0\\doteq\\hat{\\boldsymbol{\\pi}}(x_0)$ and\n",
    "$\\hat{\\boldsymbol{\\pi}}_1\\doteq\\hat{\\boldsymbol{\\pi}}(x_1)$, the log-ratio formula reduces to\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(X=x_1\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})}{p(X=x_0\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})}\n",
    "& ~=~ &\n",
    "\\kappa(\\boldsymbol{\\theta})+\\ln\\frac{p(Z\\mid\\hat{\\boldsymbol{\\pi}}_1)}{p(Z\\mid\\hat{\\boldsymbol{\\pi}}_0)}\\,,\n",
    "\\end{eqnarray}\n",
    "As we shall see in later sections, it seems to be a rule of thumb that the relevant covariate features correspond to the\n",
    "[natural variates](#Seperable-dependencies \"Section: Seperable dependencies\") of each distribution.\n",
    "Presumably this only holds true for members of \"the\" exponential family, especially since,\n",
    "as Bergtold et al. [[4]](#Citations \"Citation [4]: Bernoulli Regression Models\") notes, only \"the\" exponential family results in regression functions that are linear in their parameters. \n",
    "In such cases, the covariates will lead to an arbitrary number $n$ of features of the form $f_1(Z),\\ldots,f_n(Z)$, such that the regression model takes the form\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(X=x_1\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})}\n",
    "{p(X=x_0\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})}\n",
    "& ~=~ &\n",
    "f(Z,\\boldsymbol{\\phi})~\\doteq~\n",
    "\\phi_0+\\phi_1\\,f_1(Z)+\\cdots+\\phi_n\\,f_n(Z)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where the marginal function $\\kappa(\\boldsymbol{\\theta})$ has been absorbed into the constant parameter\n",
    "$\\phi_0$, on the basis that both parameters\n",
    "$\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$ are unknown and must be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc630d",
   "metadata": {},
   "source": [
    "It might also be convenient to assume that the conditional distribution \n",
    "$p(X\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})$ takes the same functional form as utilised for the marginal distribution $p(X\\mid\\boldsymbol{\\theta})$.\n",
    "This would correspond to the model\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\boldsymbol{\\theta}}(z)~\\doteq~\\mathbf{F}(z;\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\,,\n",
    "& \\;\\;\\;\\; & \n",
    "p(X\\mid \\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})~\\doteq~p(X\\mid\\hat{\\boldsymbol{\\theta}}(Z))\\,.\n",
    "\\end{eqnarray}\n",
    "However, as we shall see in later sections, this choice is not always feasible. In particular, there will\n",
    "typically be constraints on the parameter $\\boldsymbol{\\theta}$, e.g. non-negativity, that must be satisfied\n",
    "by the choice of regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bff478",
   "metadata": {},
   "source": [
    "### Bernoulli regression (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ff247",
   "metadata": {},
   "source": [
    "The [Bernoulli distribution](#Bernoulli-distribution \"Section: Bernoulli distribution\")\n",
    "has domain $\\mathcal{X}=\\{0,1\\}$ and takes the (marginal) form \n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & ~=~ & \\theta^x\\,(1-\\theta)^{1-x}\\,,\n",
    "\\end{eqnarray}\n",
    "for the given probability $\\theta\\in(0,1)$.\n",
    "We therefore suppose that the conditional distribution takes a similar form, namely\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta,Z,\\boldsymbol{\\phi}) & ~=~ & F(Z;\\boldsymbol{\\phi})^x\\,[1-F(Z;\\boldsymbol{\\phi})]^{1-x}\\,,\n",
    "\\end{eqnarray}\n",
    "where $F$ is contrained to satisfy $F(Z;\\boldsymbol{\\phi})\\in(0,1)$ for all $Z\\in\\mathcal{Z}$\n",
    "and all $\\boldsymbol{\\phi}\\in\\mathbb{R}^d$.\n",
    "Note that since the mean of the Bernoulli distribution is just $\\mu=\\theta$, this is equivalent\n",
    "to the [mean regression](#Mean-regression \"Section: Mean regression\") model\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & F(Z;\\boldsymbol{\\phi})\\,.\n",
    "\\end{eqnarray}\n",
    "We now take $x_0=0$ and $x_1=1$, such that the log-ratio formula reduces to\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{F(Z;\\boldsymbol{\\phi})}{1-F(Z;\\boldsymbol{\\phi})}\n",
    "& ~=~ & \\ln\\frac{\\theta}{1-\\theta} \n",
    "+ \\ln\\frac{p(Z\\mid\\boldsymbol{\\pi},X=1,\\boldsymbol{\\zeta})}\n",
    "{p(Z\\mid\\boldsymbol{\\pi},X=0,\\boldsymbol{\\zeta})}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "We recognise the left-hand side as the logit transform $\\sigma^{-1}(\\cdot)$ of $F(Z;\\boldsymbol{\\phi})$,\n",
    "and hence the Bernoulli regression model takes the logistic form of\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & F(Z;\\boldsymbol{\\phi}) ~=~\\sigma(f(Z,\\boldsymbol{\\phi}))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with\n",
    "\\begin{eqnarray}\n",
    "f(Z,\\boldsymbol{\\phi}) & ~\\doteq~ &\n",
    "\\kappa(\\theta) + \\ln\\frac{p(Z\\mid\\hat{\\boldsymbol{\\pi}}_1)}{p(Z\\mid\\hat{\\boldsymbol{\\pi}}_0)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The correct form of regression on the covariates $Z$ now follows directly from the \n",
    "assumed marginal distribution $p(Z\\mid\\boldsymbol{\\pi})$, as we demonstrate in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c8f00",
   "metadata": {},
   "source": [
    "### Bernoulli covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387216d",
   "metadata": {},
   "source": [
    "Suppose the covariate $Z$ is a binary variable that follows a \n",
    "[Bernoulli distribution](#Bernoulli-distribution \"Section: Bernoulli distribution\")\n",
    "\\begin{eqnarray}\n",
    "p(Z=z\\mid\\mu) & ~\\doteq~ & \\mu^z\\,(1-\\mu)^{1-z}\\,.\n",
    "\\end{eqnarray}\n",
    "Then the corresponding features arise from\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(Z\\mid\\mu_1)}{p(Z\\mid\\mu_0)} & ~=~ & [\\ln\\mu_1-\\ln\\mu_0]\\,Z+[\\ln(1-\\mu_1)-\\ln(1-\\mu_0)]\\,(1-Z)\n",
    "~=~\\phi_0'+\\phi_1\\,Z\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the feature for a Bernoulli covariate $Z$ is just $Z$ itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81e343",
   "metadata": {},
   "source": [
    "### Categorical covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b0737",
   "metadata": {},
   "source": [
    "Suppose the covariate $Z$ takes a value from a finite set $\\mathcal{Z}=\\{z_1,z_2,\\ldots,z_K\\}$ of distinct values\n",
    "with corresponding probabilities $\\boldsymbol{\\pi}=(\\pi_1,\\pi_2,\\ldots,\\pi_K)$. Then the categorical distribution\n",
    "is\n",
    "\\begin{eqnarray}\n",
    "p(Z=z_k\\mid\\boldsymbol{\\pi}) & ~\\doteq~ & \\pi_k~=~\\prod_{i=1}^{K}\\pi_i^{\\delta_{ik}}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\delta_{ii}=1$ and $\\delta_{ij}=0$ for $i\\ne j$. The corresponding features then arise from\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(Z\\mid\\boldsymbol{\\pi}_1)}{p(Z\\mid\\boldsymbol{\\pi}_0)} & ~=~ &\n",
    "\\sum_{k=1}^{K}(\\ln\\pi_{k1}-\\ln\\pi_{k0})\\,\\delta(Z=z_k)\n",
    "~=~\\sum_{k=1}^{K}\\phi_k\\,\\delta(Z=z_k)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the appropriate features for a categorical covariate $Z$ take the form of a binary indicator vector\n",
    "$\\boldsymbol{\\delta}\\doteq[\\delta(Z=z_k)]_{k=1}^{K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677614fa",
   "metadata": {},
   "source": [
    "### Beta covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec320b37",
   "metadata": {},
   "source": [
    "Suppose the covariate $Z$ is a probability or proportion drawn from a \n",
    "[Beta distribution](#Beta-distribution \"Section: Beta distribution\")\n",
    "\\begin{eqnarray}\n",
    "p(Z=z\\mid\\alpha,\\beta) & ~\\doteq~ & \\frac{z^{\\alpha-1}\\,(1-z)^{\\beta-1}}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "Then we find that\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(Z\\mid\\alpha_1,\\beta_1)}{p(Z\\mid\\alpha_0,\\beta_0)}\n",
    "& ~=~ &\n",
    "(\\alpha_1-\\alpha_0)\\ln Z+(\\beta_1-\\beta_0)\\ln(1-Z)-\\ln\\frac{B(\\alpha_1,\\beta_1)}{B(\\alpha_0,\\beta_0)}\n",
    "\\\\& ~=~ & \\phi_0'+\\phi_1\\ln Z+\\phi_2\\ln(1-Z)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the appropriate features for a Beta covariate $Z$ are, in general, $\\ln Z$ and $\\ln(1-Z)$.\n",
    "However, in the special case where we have theoretical reasons to suppose that $\\phi_2=-\\phi_1$, \n",
    "i.e. $\\alpha_0+\\beta_0=\\alpha_1+\\beta_1$, then we may use the single, logit-transformed feature $\\sigma^{-1}(Z)$\n",
    "instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5642342",
   "metadata": {},
   "source": [
    "### Gamma covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057b5d1",
   "metadata": {},
   "source": [
    "Suppose the covariate $Z$ represents either a generalised count or a non-negative measurement of some kind.\n",
    "Assuming that $Z$ is drawn from a \n",
    "[Gamma distribution](#Gamma-distribution \"Section: Gamma distribution\")\n",
    "\\begin{eqnarray}\n",
    "p(Z=z\\mid\\alpha,\\beta) & ~\\doteq~ & \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,z^{\\alpha-1}\\,e^{-\\beta z}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "then we observe that\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(Z\\mid\\alpha_1,\\beta_1)}{p(Z\\mid\\alpha_0,\\beta_0)}\n",
    "& ~=~ &\n",
    "[\\alpha_1\\ln\\beta_1-\\alpha_0\\ln\\beta_0+\\ln\\Gamma(\\alpha_0)-\\ln\\Gamma(\\alpha_1)]\n",
    "+(\\alpha_1-\\alpha_0)\\,\\ln Z+(\\beta_0-\\beta_1)\\,Z\n",
    "\\\\\n",
    "& ~=~ & \\phi_0'+\\phi_1\\,Z+\\phi_2\\,\\ln Z\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the appropriate features for a Gamma covariate $Z$ are both $Z$ and $\\ln Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b3ee9",
   "metadata": {},
   "source": [
    "### Poisson covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae221d65",
   "metadata": {},
   "source": [
    "Suppose the covariate $Z$ represents a count of the number of events that occurred in a fixed time-interval.\n",
    "Assuming that $Z$ is drawn from a Poisson distribution\n",
    "\\begin{eqnarray}\n",
    "p(Z=z\\mid\\lambda) & ~\\doteq~ & e^{-\\lambda}\\,\\frac{\\lambda^z}{z!}\\,,\n",
    "\\end{eqnarray}\n",
    "then we observe that\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(Z\\mid\\lambda_1)}{p(Z\\mid\\lambda_0)} & ~=~ &\n",
    "(\\lambda_0-\\lambda_1)+(\\ln\\lambda_1-\\ln\\lambda_0)\\,Z~=~\\phi_0'+\\phi_1\\,Z\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the appropriate feature for a Possion covariate $Z$ is just $Z$ itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9c290",
   "metadata": {},
   "source": [
    "### Poisson regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dcf0db",
   "metadata": {},
   "source": [
    "As another example of regression modelling using \"*probabilistic reduction*\", let us now consider that the response variate $X$ follows the Poisson distribution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\mu) & ~\\doteq~ & e^{-\\mu}\\frac{\\mu^x}{x!}\\,.\n",
    "\\end{eqnarray}\n",
    "Choosing $x_1=x_0+1$, it therefore follows that\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(X=x_1\\mid\\mu)}{p(X=x_0\\mid\\mu)} & ~=~ &\n",
    "\\ln\\mu-\\ln(x_0+1)\\,.\n",
    "\\end{eqnarray}\n",
    "This suggests that the regression model should take the form\n",
    "\\begin{eqnarray}\n",
    "\\ln\\hat{\\mu} & ~\\doteq~ & f(Z,\\boldsymbol{\\phi})\\,.\n",
    "\\end{eqnarray}\n",
    "This turns out to be a consistent choice for arbitrary $f:\\mathcal{Z}\\times\\mathbb{R}^d\\mapsto\\mathbb{R}$, \n",
    "since the distribution is constrained to have mean $\\mu\\in(0,\\infty)\\Rightarrow\\ln\\mu\\in\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c7664",
   "metadata": {},
   "source": [
    "### Geometric regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0831d7",
   "metadata": {},
   "source": [
    "So far we have seen that \"*probabilistic reduction*\" appears to work well. However, we previously cautioned\n",
    "that this might not always be the case. Let us now consider the geometric distibution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & ~\\doteq~ & (1-\\theta)^{x-1}\\,\\theta\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\theta\\in(0,1)$ is the probability of halting a sequence of independent trials, and $X$ is the number of\n",
    "trials up to and including the halted trial. Again choosing $x_1=x_0+1$, we deduce that\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(X=x_1\\mid\\theta)}{p(X=x_0\\mid\\theta)} & ~=~ & \\ln(1-\\theta)\\,.\n",
    "\\end{eqnarray}\n",
    "This suggests a regression model of the form\n",
    "\\begin{eqnarray}\n",
    "\\ln(1-\\hat{\\theta}) & ~\\doteq~ & f(Z,\\boldsymbol{\\phi})\\,.\n",
    "\\end{eqnarray}\n",
    "However, although the right-hand side may take any real value, the left-hand side is constrained to obey\n",
    "$\\hat{\\theta}\\in(0,1)\\Rightarrow\\ln(1-\\hat{\\theta})\\in(-\\infty,0)$.\n",
    "This mismatch of domains indicates that our distributional assumptions do not hold.\n",
    "In fact, given the constraint on $\\theta$, it makes more sense to assume a regression model like\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\theta} & ~\\doteq~ & \\sigma(f(Z,\\boldsymbol{\\phi}))\\,.\n",
    "\\end{eqnarray}\n",
    "Now, since the mean number of trials is $\\mu\\doteq\\mathbb{E}[X\\mid\\theta]=\\frac{1}{\\theta}$,\n",
    "then, in terms of \n",
    "[generalised nonlinear modelling](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\"),\n",
    "this is equivalent to choosing a link parameter $\\eta=\\sigma^{-1}(\\theta)$ with link function\n",
    "$g(\\mu)=\\sigma^{-1}\\left(\\frac{1}{\\mu}\\right)=-\\ln(\\mu-1)$.\n",
    "Consequently, we *hypothesise* that \"*probabilistic reduction*\" fails \n",
    "to specify a valid regression function when none of the natural parameters, or linear combinations thereof,\n",
    "are suitable for the link parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61f533",
   "metadata": {},
   "source": [
    "### Beta regression (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259f5c5",
   "metadata": {},
   "source": [
    "As another demonstration of potential issues with \"*probabilistic reduction*\", suppose that $X$ follows the\n",
    "[Beta distribution](#Beta-distribution \"Section: Beta distribution\")\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~\\doteq~ & \\frac{x^{\\alpha-1}\\,(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "Choosing $x_1=1-x_0\\in(0,1)$, it then follows that\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(X=x_1\\mid\\alpha,\\beta)}{p(X=x_0\\mid\\alpha,\\beta)}\n",
    "& ~=~ &\n",
    "(\\alpha-1)\\ln\\frac{x_1}{x_0}+(\\beta-1)\\ln\\frac{1-x_1}{1-x_0}\n",
    "~=~\\sigma^{-1}(x_1)\\,(\\alpha-\\beta)\\,.\n",
    "\\end{eqnarray}\n",
    "This suggests a regression model of the form\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\alpha}-\\hat{\\beta} & ~\\doteq~ & f(Z,\\boldsymbol{\\phi})\\,.\n",
    "\\end{eqnarray}\n",
    "Now, since $\\alpha,\\beta>0$, it follows that $\\eta=\\alpha-\\beta\\in\\mathbb{R}$, and so this\n",
    "model is consistent with the constraints.\n",
    "Also note that since we usually take $\\hat{\\eta}=0$ to indicate that only minimal predictive information\n",
    "is available from the covariates, this corresponds to initialising the distribution with\n",
    "default parameter values such that $\\hat{\\alpha}=\\hat{\\beta}$.\n",
    "\n",
    "The problem with this formulation lies with computing the separates estimates $\\hat{\\alpha}$ and $\\hat{\\beta}$, for which we require another parameter $\\psi$ that is independent of the link parameter $\\eta$.\n",
    "However, if we choose $\\beta$ as the independent parameter, then we cannot guarantee that $\\hat{\\alpha}=\\hat{\\eta}+\\hat{\\beta}>0$, since $\\hat{\\eta}$ may be large and negative.\n",
    "Similarly, we cannot guarantee that $\\hat{\\beta}=\\hat{\\alpha}-\\hat{\\eta}>0$ if we choose $\\alpha$ as the independent parameter, since $\\hat{\\eta}$ may be large and positive. This \n",
    "problem persists even if we take the independent parameter to be a function of both $\\alpha$ and $\\beta$.\n",
    "For example, we might choose $\\psi=\\alpha+\\beta$, from which we could recover the distributional parameters via\n",
    "$\\alpha=(\\psi+\\eta)/2$ and $\\beta=(\\psi-\\eta)/2$.\n",
    "However, once again we have the problem that $\\hat{\\eta}$ could take a very large positive or negative value,\n",
    "such that we cannot guarantee that $\\hat{\\alpha},\\hat{\\beta}>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb88605",
   "metadata": {},
   "source": [
    "Despite these problems, this alternative approach can actually work in practice, provided that\n",
    "the values of the regression model are never too extreme.\n",
    "The required gradients of the log-likelihood are given by\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\eta L & ~\\doteq~ & \\frac{\\partial\\alpha}{\\partial\\eta}\\,\\frac{\\partial L}{\\partial\\alpha}\n",
    "+\\frac{\\partial\\beta}{\\partial\\eta}\\,\\frac{\\partial L}{\\partial\\beta}\n",
    "~=~\\frac{1}{2}\\left(Y_\\alpha-\\mu_{Y_\\alpha}\\right)\n",
    "-\\frac{1}{2}\\left(Y_\\beta-\\mu_{Y_\\beta}\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\psi L & ~\\doteq~ & \\frac{\\partial\\alpha}{\\partial\\psi}\\,\\frac{\\partial L}{\\partial\\alpha}\n",
    "+\\frac{\\partial\\beta}{\\partial\\psi}\\,\\frac{\\partial L}{\\partial\\beta}\n",
    "~=~\\frac{1}{2}\\left(Y_\\alpha-\\mu_{Y_\\alpha}\\right)\n",
    "+\\frac{1}{2}\\left(Y_\\beta-\\mu_{Y_\\beta}\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "The corresponding variates are therefore\n",
    "\\begin{eqnarray}\n",
    "Y_\\eta & ~\\doteq~ & \\frac{1}{2}Y_\\alpha-\\frac{1}{2}Y_\\beta=~\\frac{1}{2}\\ln X-\\frac{1}{2}\\ln(1-X)\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "Y_\\psi & ~\\doteq~ & \\frac{1}{2}Y_\\alpha+\\frac{1}{2}Y_\\beta=~\\frac{1}{2}\\ln X+\\frac{1}{2}\\ln(1-X)\\,,\n",
    "\\end{eqnarray}\n",
    "respectively, with corresponding variances given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\eta} & ~\\doteq~ & \\mathtt{Var}\\left[Y_\\eta\\mid\\alpha,\\beta\\,\\right]~=~\n",
    "\\frac{1}{4}\\sigma^2_{Y_\\alpha}+\\frac{1}{4}\\sigma^2_{Y_\\beta}-\\frac{1}{2}\\sigma_{Y_\\alpha,Y_\\beta}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\psi} & ~\\doteq~ & \\mathtt{Var}\\left[Y_\\eta\\mid\\alpha,\\beta\\,\\right]~=~\n",
    "\\frac{1}{4}\\sigma^2_{Y_\\alpha}+\\frac{1}{4}\\sigma^2_{Y_\\beta}+\\frac{1}{2}\\sigma_{Y_\\alpha,Y_\\beta}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. The covariance is then given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\psi,Y_\\eta} & ~\\doteq~ & \\mathtt{Cov}\\left[Y_\\psi,Y_\\eta\\mid\\alpha,\\beta\\,\\right]~=~\n",
    "\\frac{1}{4}\\sigma^2_{Y_\\alpha}-\\frac{1}{4}\\sigma^2_{Y_\\beta}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "These values, in addition to the respective means of the variates, are sufficient for\n",
    "parameter estimation. Whether or not the model converges in practice depends upon the observed data\n",
    "and the settings of the [parameter update](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\")\n",
    "scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7293b",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95acf0",
   "metadata": {},
   "source": [
    "[1] J. A. Nelder and R. W. M. Wedderburn (1972), \"*Generalized Linear Models*\", J. Royal Stat. Soc. Series A, Vol. 135, No. 3, pp. 370-384.\n",
    "\n",
    "[2] M. G. Kendall and A. Stuart (1967), \"*The Advanced Theory of Statistics*\", 2nd ed., Vol. 2.\n",
    "\n",
    "[3] R. Kieschnick and B. D. McCullough (2003), \"*Regression analysis of variates observed on $(0, 1)$*\", Statistical Modelling 3(3):193-213. [[PDF]](https://journals.sagepub.com/doi/10.1191/1471082X03st053oa \"journals.sagepub.com\")\n",
    "\n",
    "[4] J. S. Bergtold, A. Spanos and E. Onukwugha (2010), \"*Bernoulli Regression Models: Revisiting the\n",
    "Specification of Statistical Models with Binary Dependent Variables*\",\n",
    "J. Choice Modelling 3(2), pp 1-28."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
