{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d86176",
   "metadata": {},
   "source": [
    "# Training a Sequential RBM on a timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd7e80",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to examine how well the sequential Bernoulli RBM fits a temporal series of binary values. The model is explained in a previous\n",
    "[notebook](rbm_models.ipynb), but we shall revisit the mathematics here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6a600",
   "metadata": {},
   "source": [
    "For the purposes of the current experiment, we utilise the file `acgl.us.txt` of stock prices, obtained from [Kaggle](https://www.kaggle.com/gulabpatelcovid/stock-market-analysis-and-time-series-prediction/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6497bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9613d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e93fb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../../../projects/financial-timeseries/stocks\"  # YMMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12717635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>OpenInt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-02-25</td>\n",
       "      <td>13.583</td>\n",
       "      <td>13.693</td>\n",
       "      <td>13.430</td>\n",
       "      <td>13.693</td>\n",
       "      <td>156240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>13.697</td>\n",
       "      <td>13.827</td>\n",
       "      <td>13.540</td>\n",
       "      <td>13.827</td>\n",
       "      <td>370509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-03-01</td>\n",
       "      <td>13.780</td>\n",
       "      <td>13.913</td>\n",
       "      <td>13.720</td>\n",
       "      <td>13.760</td>\n",
       "      <td>224484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-03-02</td>\n",
       "      <td>13.717</td>\n",
       "      <td>13.823</td>\n",
       "      <td>13.667</td>\n",
       "      <td>13.810</td>\n",
       "      <td>286431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-03-03</td>\n",
       "      <td>13.783</td>\n",
       "      <td>13.783</td>\n",
       "      <td>13.587</td>\n",
       "      <td>13.630</td>\n",
       "      <td>193824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close  Volume  OpenInt\n",
       "0  2005-02-25  13.583  13.693  13.430  13.693  156240        0\n",
       "1  2005-02-28  13.697  13.827  13.540  13.827  370509        0\n",
       "2  2005-03-01  13.780  13.913  13.720  13.760  224484        0\n",
       "3  2005-03-02  13.717  13.823  13.667  13.810  286431        0\n",
       "4  2005-03-03  13.783  13.783  13.587  13.630  193824        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"acgl.us.txt\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1579d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>OpenInt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>2017-11-06</td>\n",
       "      <td>94.49</td>\n",
       "      <td>95.65</td>\n",
       "      <td>94.02</td>\n",
       "      <td>95.55</td>\n",
       "      <td>420192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>95.86</td>\n",
       "      <td>95.95</td>\n",
       "      <td>95.20</td>\n",
       "      <td>95.56</td>\n",
       "      <td>464011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>95.41</td>\n",
       "      <td>95.90</td>\n",
       "      <td>94.89</td>\n",
       "      <td>95.45</td>\n",
       "      <td>471756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>94.93</td>\n",
       "      <td>96.14</td>\n",
       "      <td>94.47</td>\n",
       "      <td>95.91</td>\n",
       "      <td>353498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>95.89</td>\n",
       "      <td>95.99</td>\n",
       "      <td>94.39</td>\n",
       "      <td>95.35</td>\n",
       "      <td>452833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date   Open   High    Low  Close  Volume  OpenInt\n",
       "3196  2017-11-06  94.49  95.65  94.02  95.55  420192        0\n",
       "3197  2017-11-07  95.86  95.95  95.20  95.56  464011        0\n",
       "3198  2017-11-08  95.41  95.90  94.89  95.45  471756        0\n",
       "3199  2017-11-09  94.93  96.14  94.47  95.91  353498        0\n",
       "3200  2017-11-10  95.89  95.99  94.39  95.35  452833        0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ece33f",
   "metadata": {},
   "source": [
    "For want of a better idea, we follow the usual idea of tracking the closing price of the stock at the end of each day. Since we currently only have an implementation of a Bernoulli RBM, we shall signal a rise in price (from one day to the next) by 1, and\n",
    "a fall (or the same value) by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb6a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_series = df.Close.values\n",
    "d_series = np.diff(c_series)\n",
    "b_series = np.asarray(d_series > 0, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4ff26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 1 0 0 0 0]\n",
      "[1 0 0 0 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(b_series[:10])\n",
    "print(b_series[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a976361a",
   "metadata": {},
   "source": [
    "Next, we need to form the timeseries into a collection of vectors, based upon sliding a window of fixed length along the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5085382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import sliding_window_view as _window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cae80",
   "metadata": {},
   "source": [
    "Let us start with the simplest possible Bernoulli RBM, with one input bit and one output bit. We note in advance that the modelled input bit probability, $p(x_1=1)$, must be constant.\n",
    "In fact, the parameter estimates should converge via gradient ascent when the model probability equals the empirical probability (as we shall verify later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cad87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 1\n",
    "num_output = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce8c6e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 1)\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = _window(b_series, num_input)\n",
    "print(X_train.shape)\n",
    "print(X_train[:9,:])\n",
    "print(X_train[-9:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b982e392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two possible states:\n",
      "[[0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"There are two possible states:\")\n",
    "idx = [1, 0]\n",
    "X_states = X_train[idx, 0:1]\n",
    "print(X_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffa6f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical probabilities are:\n",
      "p(x1=0)=0.472187, p(x1=1)=0.527813\n"
     ]
    }
   ],
   "source": [
    "ind0 = X_train[:, 0] == 0\n",
    "p0 = sum(ind0) / len(ind0)\n",
    "p1 = sum(~ind0) / len(~ind0)\n",
    "base_probs = (p0, p1)\n",
    "print(\"The empirical probabilities are:\")\n",
    "print(\"p(x1=0)=%f, p(x1=1)=%f\" % base_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e65857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bernoulli_rbm import SequentialBernoulliRBM, ExactSequentialBernoulliRBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c6d6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47164896 0.52835104]\n"
     ]
    }
   ],
   "source": [
    "rbm = SequentialBernoulliRBM(\n",
    "    num_output, num_input, \n",
    ")\n",
    "rbm.fit(X_train)\n",
    "\n",
    "preds = rbm.reconstruct(X_states)\n",
    "probs = preds[:, 0]  # p(x_1)\n",
    "probs = np.array(list(zip(1 - probs, probs)))\n",
    "print(probs[range(2), X_states[:, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ef1d0",
   "metadata": {},
   "source": [
    "Note that we have not achieved the required probability. It is, however, feasible that the default use of batching during training is interfering with estimating distributional properties of the data. Thus, we shall try again without batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca35e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4721875 0.5278125]\n"
     ]
    }
   ],
   "source": [
    "rbm = SequentialBernoulliRBM(\n",
    "    num_output, num_input, \n",
    "    batch_size=1.0\n",
    ")\n",
    "rbm.fit(X_train)\n",
    "\n",
    "preds = rbm.reconstruct(X_states)\n",
    "probs = preds[:, 0]  # p(x_1)\n",
    "probs = np.array(list(zip(1 - probs, probs)))\n",
    "print(probs[range(2), X_states[:, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb383c7",
   "metadata": {},
   "source": [
    "Here we have used the standard model which uses mean-field approximations to bypass the usual intractability of Boltzmann machine calculations (caused by the necessity of marginalising over many variables).\n",
    "\n",
    "As an alternative, let us now derive a sequence model that explicitly marginalises over the unknown output.\n",
    "We shall start from first principles, assuming the simplest possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b09740",
   "metadata": {},
   "source": [
    "### One-input, one-output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794da4b9",
   "metadata": {},
   "source": [
    "To see what the exact gradient should be, let us start with the simplest Bernoulli RBM model, namely\n",
    "\\begin{eqnarray}\n",
    "p(x_1,y_1) & = & \n",
    "\\frac{e^{f(x_1,y_1)}}\n",
    "{\\sum_{x_1'=0}^{1}\\sum_{y_1'=0}^{1} e^{f(x_1',y_1')}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with\n",
    "\\begin{eqnarray}\n",
    "f(x_1,y_1) & = & a_1 x_1 + x_1 W_{11} y_1 + b_1 y_1\\,.\n",
    "\\end{eqnarray}\n",
    "Thus\n",
    "\\begin{eqnarray}\n",
    "p(x_1) & = & \n",
    "\\frac{\\sum_{y_1=0}^{1} e^{f(x_1,y_1)}}\n",
    "{\\sum_{x_1'=0}^{1}\\sum_{y_1'=0}^{1} e^{f(x_1',y_1')}}\n",
    "\\\\& = &\n",
    "\\frac{e^{a_1 x_1}\\left[1+e^{b_1 + x_1 W_{11}}\\right]}\n",
    "{\\sum_{x_1'=0}^{1}e^{a_1 x_1'}\\left[1+e^{b_1 + x_1' W_{11}}\\right]}\n",
    "\\\\& = &\n",
    "\\frac{e^{a_1 x_1}\\left[1+e^{b_1 + x_1 W_{11}}\\right]}\n",
    "{\\left[1+e^{b_1}\\right]+e^{a_1}\\left[1+e^{b_1 + W_{11}}\\right]}\n",
    "\\\\& = &\n",
    "\\frac{\n",
    " e^{a_1 x_1+\\ln\\left[1+e^{b_1 + x_1 W_{11}}\\right]-\\ln\\left[1+e^{b_1}\\right]}\n",
    "}\n",
    "{\n",
    " 1+e^{a_1+\\ln\\left[1+e^{b_1 + W_{11}}\\right]-\\ln\\left[1+e^{b_1}\\right]}\n",
    "}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with the consequence that\n",
    "\\begin{eqnarray}\n",
    "p(x_1=1) & = & \\sigma\\left(\n",
    "a_1+\\ln\\left[1+e^{b_1 + W_{11}}\\right]-\\ln\\left[1+e^{b_1}\\right]\n",
    "\\right)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\sigma(\\cdot)$ is the logistic sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346ad45",
   "metadata": {},
   "source": [
    "Now let $p_1\\doteq p(x_1=1)$, such that the log-likelihood of a single observation of $x_1$ is\n",
    "\\begin{eqnarray}\n",
    "L & = & \\ln\\left[p_1^{x_1}\\,(1-p_1)^{1-x_1}\\right]\n",
    "\\\\& = &\n",
    "x_1\\ln p_1 + (1-x_1)\\ln(1-p_1)\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\nabla L & = &\n",
    "\\frac{x_1}{p_1}\\nabla p_1-\\frac{1-x_1}{1-p_1}\\nabla p_1\n",
    "\\\\& = &\n",
    "\\frac{x-p_1}{p_1 (1-p_1)}\\nabla p_1\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1a584",
   "metadata": {},
   "source": [
    "However, we recall that\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\sigma(\\theta) & = & \\sigma(\\theta)\\,[1-\\sigma(\\theta)]\\,\\nabla\\theta\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla p_1 & = & p_1 (1-p_1)\\nabla\\theta\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla L & = & (x_1-p_1)\\nabla\\theta\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\theta & = & a_1+\\ln\\left[1+e^{b_1 + W_{11}}\\right]-\\ln\\left[1+e^{b_1}\\right]\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da153bb9",
   "metadata": {},
   "source": [
    "Clearly, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\theta}{\\partial a_1} & = & 1\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial a_1} & = & x_1 - p_1\\,,\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial\\langle L\\rangle}{\\partial a_1} & = &\n",
    "\\left\\langle\\frac{\\partial L}{\\partial a_1}\\right\\rangle \n",
    "= \\langle x_1\\rangle - p_1\\,,\n",
    "\\end{eqnarray}\n",
    "and thus the estimates of $a_1$ should converge when $p_1=\\langle x_1\\rangle$, which is just the empirical probability of the input bit being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c52c57",
   "metadata": {},
   "source": [
    "Similarly, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\theta}{\\partial W_{11}} & = &\n",
    "\\frac{e^{b_1+W_{11}}}{1+e^{b_1+W_{11}}} = \\sigma(b_1+W_{11})\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial W_{11}}\n",
    "& = & \\left(x_1-p_1\\right)\\,\\sigma(b_1+W_{11})\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial\\theta}{\\partial b_{1}} & = &\n",
    "\\frac{e^{b_1+W_{11}}}{1+e^{b_1+W_{11}}}\n",
    "-\\frac{e^{b_1}}{1+e^{b_1}}\n",
    "\\\\& = &\n",
    "\\sigma(b_1+W_{11})-\\sigma(b_1)\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial b_{1}}\n",
    "& = & \\left(x_1-p_1\\right)\\,\n",
    "\\left[\\sigma(b_1+W_{11})-\\sigma(b_1)\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cd002",
   "metadata": {},
   "source": [
    "Alternatively, we can go back to the model and examine the estimation approach to the gradients. Thus\n",
    "\\begin{eqnarray}\n",
    "L & = & \n",
    "\\ln\\sum_{y_1=0}^{1} e^{f(x_1,y_1)}-\n",
    "\\ln\\sum_{x_1'=0}^{1}\\sum_{y_1'=0}^{1} e^{f(x_1',y_1')}\\,,\n",
    "\\\\\\Rightarrow\n",
    "\\nabla L & = & \\mathbb{E}_{y_1\\mid x_1}\\left[\\nabla f(x_1,y_1)\\right]\n",
    "-\\mathbb{E}_{x_1',y_1'}\\left[\\nabla f(x_1',y_1')\\right]\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbda2d3",
   "metadata": {},
   "source": [
    "Now\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial f}{\\partial a_1} & = & x_1\\,,\n",
    "\\\\\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial a_1} & = & \n",
    "\\mathbb{E}_{y_1\\mid x_1}\\left[x_1\\right]\n",
    "-\\mathbb{E}_{x_1',y_1'}\\left[x_1'\\right]\n",
    "\\\\& = &\n",
    "x_1-p(x_1=1) = x_1-p_1\\,.\n",
    "\\end{eqnarray}\n",
    "In comparison, the mean-field gradient is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial a_1} & = & x_1-\\bar{x}_1'\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\bar{x}_1' \\doteq p(x_1=1\\mid\\bar{y}_1)\\,, &&\n",
    "\\bar{y}_1 \\doteq p(y_1=1\\mid x_1)\\,.\n",
    "\\end{eqnarray}\n",
    "Hence,\n",
    "we see that the reconstruction probability $\\bar{x}_1'$ is now replaced by the\n",
    "exact probability $p_1$, denoted as $\\bar{x}_1'\\hookleftarrow p_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153e791",
   "metadata": {},
   "source": [
    "Similarly, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial f}{\\partial b_1} & = & y_1\\,,\n",
    "\\\\\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial b_1} & = & \n",
    "\\mathbb{E}_{y_1\\mid x_1}\\left[y_1\\right]\n",
    "-\\mathbb{E}_{x_1',y_1'}\\left[y_1'\\right]\n",
    "\\\\& = & p(y_1=1\\mid x_1)-p(y_1=1)\n",
    "\\\\& = & \\bar{y}_1-q_1\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "using the definition of $\\bar{y}_1$ above,\n",
    "where we have now defined $q_1\\doteq p(y_1=1)$ for reasons of symmetry.\n",
    "We deduce that\n",
    "\\begin{eqnarray}\n",
    "p(y_1\\mid x_1) & = & \n",
    "\\frac{e^{f(x_1,y_1)}}\n",
    "{\\sum_{y_1'=0}^{1} e^{f(x_1,y_1')}}\n",
    "\\\\& = &\n",
    "\\frac{e^{a_1 x_1+x_1 W_{11}y_1+b_1 y_1}}\n",
    "{\\sum_{y_1'=0}^{1} e^{a_1 x_1+x_1 W_{11}y_1'+b_1 y_1'}}\n",
    "\\\\& = &\n",
    "\\frac{e^{(b_1+x_1 W_{11})y_1}}\n",
    "{1+e^{b_1+x_1 W_{11}}}\\,,\n",
    "\\\\\\Rightarrow\n",
    "p(y_1=1\\mid x_1) & = & \\sigma(b_1+x_1 W_{11})\\,,\n",
    "\\end{eqnarray}\n",
    "and therefore\n",
    "\\begin{eqnarray}\n",
    "p(y_1=1) & = & p(y_1=1\\mid x_1=1)\\,p(x_1=1)+p(y_1=1\\mid x_1=0)\\,p(x_1=0)\n",
    "\\\\& = &\n",
    "p_1\\sigma(b_1+W_{11})+(1-p_1)\\sigma(b_1)\\,,\n",
    "\\end{eqnarray}\n",
    "giving\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial b_1} & = & \n",
    "\\sigma(b_1+x_1 W_{11})-\\left[p_1\\sigma(b_1+W_{11})+(1-p_1)\\sigma(b_1)\\right]\n",
    "\\\\& = &\n",
    "(x_1-p_1)\\left[\\sigma(b_1+W_{11})-\\sigma(b_1)\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "as expected. We note that the mean-field approximation is\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial b_1} & = & \\bar{y}_1-\\bar{y}_1'\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\bar{y}_1' & \\doteq & p(y_1=1\\mid\\bar{x}_1')\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the exact gradient is obtained via the replacement \n",
    "$\\bar{y}_1'\\hookleftarrow q_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3421ffd",
   "metadata": {},
   "source": [
    "Finally, we have\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial f}{\\partial W_{11}} & = & x_1 y_1\\,,\n",
    "\\\\\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial W_{11}} & = & \n",
    "\\mathbb{E}_{y_1\\mid x_1}\\left[x_1 y_1\\right]\n",
    "-\\mathbb{E}_{x_1',y_1'}\\left[x_1' y_1'\\right]\n",
    "\\\\& = &\n",
    "x_1\\bar{y}_1-\\mathbb{E}_{x_1'}\\left[x_1' p(y_1'=1\\mid x_1')\\right]\n",
    "\\\\& = &\n",
    "x_1\\bar{y}_1-p_1\\,p(y_1=1\\mid x_1=1)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "For completeness, we note that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial W_{11}} & = & \n",
    "x_1\\sigma(b_1+x_1 W_{11})-p_1\\sigma(b_1+W_{11})\n",
    "\\\\& = & (x_1-p_1)\\,\\sigma(b_1+W_{11})\\,,\n",
    "\\end{eqnarray}\n",
    "as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd6214",
   "metadata": {},
   "source": [
    "In comparison, the mean-field approximation is \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial W_{11}} & = & x_1\\bar{y}_1-\\bar{x}_1'\\bar{y}_1'\\,, \n",
    "\\end{eqnarray}\n",
    "with $\\bar{y}_1$ and $\\bar{y}_1'$ defined above.\n",
    "Thus, the exact gradient is obtained by the replacement\n",
    "$\\bar{y}_1'\\hookleftarrow p(y_1=1\\mid x_1=1)=\\sigma(b_1+W_{11})$. Note that this\n",
    "is a different replacement for $\\bar{y}_1'$ than that used for $\\frac{\\partial L}{\\partial b_1}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687947b",
   "metadata": {},
   "source": [
    "### Two-input, one-output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08419f6",
   "metadata": {},
   "source": [
    "Next, we extend the model such that the input is a sequence of two bits. We now have\n",
    "\\begin{eqnarray}\n",
    "f(x_1,x_2,y_1) & = & a_1 x_1 + a_2 x_2 + x_1 W_{11} y_1 + x_2 W_{21} y_1 + b_1 y_1\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(x_1,x_2,y_1) & = &\n",
    "\\frac{e^{f(x_1,x_2,y_1)}}\n",
    "{\\sum_{x_1'=0}^{1}\\sum_{x_2'=0}^{1}\\sum_{y_1'=0}^{1}e^{f(x_1',x_2',y_1')}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "We note a property of the RBM that $x_1$ and $x_2$ are conditionally independent,\n",
    "namely\n",
    "\\begin{eqnarray}\n",
    "p(x_1,x_2\\mid y_1) & = &\n",
    "\\frac{e^{f(x_1,x_2,y_1)}}\n",
    "{\\sum_{x_1'=0}^{1}\\sum_{x_2'=0}^{1}e^{f(x_1',x_2',y_1)}}\n",
    "\\\\& = & \n",
    "\\frac{e^{(a_1 + W_{11}y_1) x_1+(a_2 + W_{21}y_1) x_2+b_1 y_1}}\n",
    "{\\sum_{x_1'=0}^{1}\\sum_{x_2'=0}^{1}\n",
    " e^{(a_1 + W_{11}y_1) x_1'+(a_2 + W_{21}y_1) x_2'+b_1 y_1}}\n",
    "\\\\& = &\n",
    "\\frac{e^{(a_1 + W_{11}y_1) x_1}}\n",
    "{\\sum_{x_1'=0}^{1}e^{(a_1 + W_{11}y_1) x_1'}}\n",
    "\\frac{e^{(a_2 + W_{21}y_1) x_2}}\n",
    "{\\sum_{x_2'=0}^{1}e^{(a_2 + W_{21}y_1) x_2'}}\n",
    "\\\\& = &\n",
    "p(x_1\\mid y_1)\\,p(x_2\\mid y_1)\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(x_i=1\\mid y_1) & = & \\sigma(a_i+W_{i1}y_1)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb49f548",
   "metadata": {},
   "source": [
    "However, $x_1$ and $x_2$ are **not** unconditionally independent, because\n",
    "\\begin{eqnarray}\n",
    "p(x_2\\mid x_1) & = & \n",
    "\\frac{\\sum_{y_1=0}^{1}e^{f(x_1,x_2,y_1)}}\n",
    "{\\sum_{x_2'=0}^{1}\\sum_{y_1'=0}^{1}e^{f(x_1,x_2',y_1')}}\n",
    "\\\\& = &\n",
    "\\frac{\\sum_{y_1=0}^{1}e^{a_1 x_1+a_2 x_2 + (b_1+x_1 W_{11}+x_2 W_{21}) y_1}}\n",
    "{\n",
    "\\sum_{x_2'=0}^{1}\\sum_{y_1'=0}^{1}\n",
    " e^{a_1 x_1+a_2 x_2' + (b_1+x_1 W_{11}+x_2' W_{21}) y_1'}\n",
    "}\n",
    "\\\\& = &\n",
    "\\frac{\n",
    "e^{a_2 x_2}\\sum_{y_1=0}^{1}e^{(b_1+x_1 W_{11}+x_2 W_{21}) y_1}\n",
    "}\n",
    "{\n",
    "\\sum_{x_2'=0}^{1}\n",
    "e^{a_2 x_2'}\\sum_{y_1'=0}^{1}e^{(b_1+x_1 W_{11}+x_2' W_{21}) y_1'}\n",
    "}\n",
    "\\\\& = &\n",
    "\\frac{\n",
    "e^{a_2 x_2+\\ln\\left[1+e^{b_1+x_1 W_{11}+x_2 W_{21}}\\right]}\n",
    "}\n",
    "{\n",
    "\\sum_{x_2'=0}^{1}\n",
    "e^{a_2 x_2'+\\ln\\left[1+e^{b_1+x_1 W_{11}+x_2' W_{21}}\\right]}\n",
    "}\n",
    "\\\\& = &\n",
    "\\frac{e^{\n",
    "a_2 x_2+\\ln\\left[1+e^{b_1+x_1 W_{11}+x_2 W_{21}}\\right]\n",
    "-\\ln\\left[1+e^{b_1+x_1 W_{11}}\\right]\n",
    "}}\n",
    "{\n",
    "1+e^{\n",
    "a_2+\\ln\\left[1+e^{b_1+x_1 W_{11}+W_{21}}\\right]\n",
    "-\\ln\\left[1+e^{b_1+x_1 W_{11}}\\right]\n",
    "}}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(x_2=1\\mid x_1) & = & \\sigma\\left(\n",
    "a_2+\\ln\\left[1+e^{b_1+x_1 W_{11}+W_{21}}\\right]\n",
    "-\\ln\\left[1+e^{b_1+x_1 W_{11}}\\right]\n",
    "\\right)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21da21",
   "metadata": {},
   "source": [
    "Now, due to this dependence, we note that the initial probability for the two-bit input model is \n",
    "in fact given by\n",
    "\\begin{eqnarray}\n",
    "p(x_1=1) & = & \\sum_{x_2=0}^{1} p(x_1=1,x_2)\\,.\n",
    "\\end{eqnarray}\n",
    "However, summing over unknown future values of a sequence is quickly going to become intractable for large sequences. Hence, we make an appeal to causality that future values cannot affect past values (although they\n",
    "will affect *inferences* about past values). Thus, as an approximation, we assume the initial probability\n",
    "is just\n",
    "\\begin{eqnarray}\n",
    "p(x_1=1) & \\doteq & \\sigma\\left(\n",
    "a_1+\\ln\\left[1+e^{b_1+W_{11}}\\right]\n",
    "-\\ln\\left[1+e^{b_1}\\right]\n",
    "\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "as derived in the previous section for the one-bit input model.\n",
    "In effect, we truncate the full model of $f(\\cdot)$ to only include as much of the input sequence as has\n",
    "been observed.\n",
    "This causal approximation is the true basis of our sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbb705",
   "metadata": {},
   "source": [
    "### $F$-input, $H$-output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4ce29",
   "metadata": {},
   "source": [
    "The full model, for input ${\\bf x}\\in\\{0,1\\}^{F}$ and output\n",
    "${\\bf y}\\in\\{0,1\\}^{H}$, is given by\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = &\n",
    "\\frac{e^{f({\\bf x},{\\bf y})}}\n",
    "{\\sum_{{\\bf x}'\\in\\{0,1\\}^{F}}\n",
    "\\sum_{{\\bf y}'\\in\\{0,1\\}^{H}}\n",
    "e^{f({\\bf x}',{\\bf y}')}\n",
    "}\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "f({\\bf x},{\\bf y}) & = & {\\bf a}^{T}{\\bf x}+{\\bf x}^{T}{\\bf W}{\\bf y}+\n",
    "{\\bf b}^{T}{\\bf y}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e8868",
   "metadata": {},
   "source": [
    "The sequential model for partial input ${\\bf x}_{1:i}=(x_1,\\ldots,x_i)$ is\n",
    "therefore (as a generalisation from the previous section) given by\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}_{1:i},{\\bf y}) & = &\n",
    "\\frac{e^{f({\\bf x}_{1:i},{\\bf y})}}\n",
    "{\\sum_{{\\bf x}_{1:i}'\\in\\{0,1\\}^{i}}\n",
    "\\sum_{{\\bf y}'\\in\\{0,1\\}^{H}}\n",
    "e^{f({\\bf x}_{1:i}',{\\bf y}')}\n",
    "}\\,,\n",
    "\\end{eqnarray}\n",
    "where we define the truncated model as\n",
    "\\begin{eqnarray}\n",
    "f({\\bf x}_{1:i},{\\bf y}) & \\doteq & \n",
    "{\\bf a}_{1:i}^{T}{\\bf x}_{1:i}+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,:}{\\bf y}+\n",
    "{\\bf b}^{T}{\\bf y}\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, we obtain\n",
    "\\begin{eqnarray}\n",
    "p(x_i\\mid{\\bf x}_{1:i-1}) & = &\n",
    "\\frac{\\sum_{{\\bf y}\\in\\{0,1\\}^{H}}e^{f({\\bf x}_{1:i-1},x_i,{\\bf y})}}\n",
    "{\\sum_{x_i'\\in\\{0,1\\}}\n",
    "\\sum_{{\\bf y}'\\in\\{0,1\\}^{H}}\n",
    "e^{f({\\bf x}_{1:i-1},x_i',{\\bf y}')}\n",
    "}\\,,\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "p(x_i=1\\mid{\\bf x}_{1:i-1}) & = &\n",
    "\\sigma\\left(\n",
    " a_i+\\sum_{j=1}^{H}\\left\\{\n",
    "  \\ln\\left[1+e^{B_{ij}+W_{ij}}\\right]-\\ln\\left[1+e^{B_{ij}}\\right]\n",
    " \\right\\}\n",
    "\\right)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "B_{ij} & \\doteq & b_j+\\sum_{k=1}^{i-1}x_k W_{kj}\n",
    "=b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da416dc4",
   "metadata": {},
   "source": [
    "We now define the log-likelihood of the single sequence ${\\bf x}$ as $L=\\sum_{i=1}^{F} L_i$, for conditional log-likelihood\n",
    "\\begin{eqnarray}\n",
    "L_i & \\doteq & \\ln p(x_i\\mid{\\bf x}_{1:i-1})\n",
    "\\\\& = &\n",
    "\\ln\\sum_{{\\bf y}\\in\\{0,1\\}^{H}}e^{f({\\bf x}_{1:i-1},x_i,{\\bf y})}\n",
    "-\\ln\\sum_{x_i'\\in\\{0,1\\}}\n",
    "\\sum_{{\\bf y}'\\in\\{0,1\\}^{H}}\n",
    "e^{f({\\bf x}_{1:i-1},x_i',{\\bf y}')\n",
    "}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\nabla L & = & \\sum_{i=1}^{F}\\left\\{\n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i}}\\left[\\nabla f({\\bf x}_{1:i-1},x_i,{\\bf y})\\right]\n",
    "-\\mathbb{E}_{x_i',{\\bf y}'\\mid{\\bf x}_{1:i-1}}\\left[\n",
    "  \\nabla f({\\bf x}_{1:i-1},x_i',{\\bf y}')\\right]\n",
    "\\right\\}\n",
    "\\\\& = &\n",
    "\\sum_{i=1}^{F}\\left\\{\n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i-1},x_i}\\left[\n",
    "  \\nabla f({\\bf x}_{1:i-1},x_i,{\\bf y})\\right]\n",
    "-\\mathbb{E}_{x_i'\\mid{\\bf x}_{1:i-1}}\\left[\n",
    "\\mathbb{E}_{{\\bf y}'\\mid {\\bf x}_{1:i-1},x_i'}\\left[\n",
    "  \\nabla f({\\bf x}_{1:i-1},x_i',{\\bf y}')\\right]\n",
    "\\right]\n",
    "\\right\\}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2265e",
   "metadata": {},
   "source": [
    "We observe that for the $j$-th output variable $y_j$ we have\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i}}[y_j] & = &\n",
    "p(y_j=1\\mid{\\bf x}_{1:i}) \\doteq \\bar{y}_j({\\bf x}_{1:i-1},x_i)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{x_i,{\\bf y}\\mid{\\bf x}_{1:i-1}}\\left[y_j\\right]\n",
    "& = & p(y_j=1\\mid{\\bf x}_{1:i-1}) \\doteq \\tilde{y}_j({\\bf x}_{1:i-1})\\,,\n",
    "\\end{eqnarray}\n",
    "where we shall derive the required formulae later. Thus, \n",
    "we obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial f}{\\partial b_j} = y_j & \\Rightarrow &\n",
    "\\frac{\\partial L}{\\partial b_j} = \\sum_{i=1}^{F}\\left\\{\n",
    "\\bar{y}_j({\\bf x}_{1:i-1},x_i)-\\tilde{y}_j({\\bf x}_{1:i-1})\n",
    "\\right\\}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e95dca",
   "metadata": {},
   "source": [
    "However, due to our assumption of causality, we note that for the $k$-th input variable $x_k$ we have\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i}}[x_k] & = &\n",
    "\\left\\{\\begin{array}{ll}\n",
    "x_k\\,, & k\\le i\n",
    "\\\\\n",
    "0\\,, & k>i\n",
    "\\end{array}\\right.\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{x_i\\mid{\\bf x}_{1:i-1}}[x_k] & = &\n",
    "\\left\\{\\begin{array}{ll}\n",
    "x_k\\,, & k<i\n",
    "\\\\\n",
    "p(x_i=1\\mid{\\bf x}_{1:i-1})\\,, & k=i\n",
    "\\\\\n",
    "0\\,, & k>i\n",
    "\\end{array}\\right.\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, we obtain\n",
    "\\begin{eqnarray}\n",
    "    \\frac{\\partial f}{\\partial a_i} = x_i & \\Rightarrow &\n",
    "\\frac{\\partial L}{\\partial a_i} = x_i - \\tilde{x}_i({\\bf x}_{1:i-1})\\,,\n",
    "\\end{eqnarray}\n",
    "where we define\n",
    "\\begin{eqnarray}\n",
    "    \\tilde{x}_i({\\bf x}_{1:i-1}) & \\doteq &\n",
    "\\mathbb{E}_{x_i\\mid{\\bf x}_{1:i-1}}[x_i] = p(x_i=1\\mid{\\bf x}_{1:i-1})\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692875f5",
   "metadata": {},
   "source": [
    "We therefore deduce that\n",
    "\\begin{eqnarray}\n",
    "\\tilde{y}_j({\\bf x}_{1:i-1}) & \\doteq &\n",
    "\\mathbb{E}_{x_i\\mid{\\bf x}_{1:i-1}}\\left[\n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i-1},x_i}[y_j] \n",
    "\\right]\n",
    "\\\\\n",
    "& = &\n",
    "\\mathbb{E}_{x_i\\mid{\\bf x}_{1:i-1}}\\left[\n",
    "p(y_j=1\\mid{\\bf x}_{1:i-1},x_i)\n",
    "\\right]\n",
    "\\\\\n",
    "& = &\n",
    "p(x_i=0\\mid{\\bf x}_{1:i-1})\\,p(y_j=1\\mid{\\bf x}_{1:i-1},x_i=0)+\n",
    "p(x_i=1\\mid{\\bf x}_{1:i-1})\\,p(y_j=1\\mid{\\bf x}_{1:i-1},x_i=1)\n",
    "\\\\\n",
    "& = &\n",
    "[1-\\tilde{x}_i({\\bf x}_{1:i-1})]\\,\\bar{y}_j({\\bf x}_{1:i-1},0)\n",
    "+\\tilde{x}_i({\\bf x}_{1:i-1})\\,\\bar{y}_j({\\bf x}_{1:i-1},1)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8208a2",
   "metadata": {},
   "source": [
    "Lastly, we examine the interaction between the $k$-th input and the $j$-th output. We observe that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i}}[x_k y_j] & = &\n",
    "\\left\\{\\begin{array}{ll}\n",
    "x_k\\,\\bar{y}_j({\\bf x}_{1:i-1},x_i)\\,, & k\\le i\n",
    "\\\\\n",
    "0\\,, & k>i\n",
    "\\end{array}\\right.\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and therefore\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{x_i\\mid{\\bf x}_{1:i-1}}\\left[\\mathbb{E}_{{\\bf y}\\mid{\\bf x}_{1:i}}[x_k y_j]\\right] & = &\n",
    "\\left\\{\\begin{array}{ll}\n",
    "x_k\\,\\tilde{y}_j({\\bf x}_{1:i-1})\\,, & k<i\n",
    "\\\\\n",
    "\\tilde{x}_i({\\bf x}_{1:i-1})\\,\\bar{y}_j({\\bf x}_{1:i-1},1)\\,, & k=i\n",
    "\\\\\n",
    "0\\,, & k>i\n",
    "\\end{array}\\right.\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a101c0",
   "metadata": {},
   "source": [
    "Consequently, we observe that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial f}{\\partial W_{kj}} & = &x_k\\,y_j\n",
    "\\\\\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial W_{kj}} & = &\n",
    "x_k\\,\\bar{y}_j({\\bf x}_{1:k-1},x_k)\n",
    "-\\tilde{x}_k({\\bf x}_{1:k-1})\\,\\bar{y}_j({\\bf x}_{1:k-1},1)\n",
    "+ x_k\\sum_{i=k+1}^{F}\\left[\n",
    "  \\bar{y}_j({\\bf x}_{1:i-1},x_i)-\\tilde{y}_j({\\bf x}_{1:i-1})\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Swapping indices $i$ and $k$ for convenience, this gives the more familiar form of\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial W_{ij}} & = &\n",
    "x_i\\,\\bar{y}_j({\\bf x}_{1:i-1},x_i)\n",
    "-\\tilde{x}_i({\\bf x}_{1:i-1})\\,\\bar{y}_j({\\bf x}_{1:i-1},1)\n",
    "+ x_i\\sum_{k=i+1}^{F}\\left[\n",
    "  \\bar{y}_j({\\bf x}_{1:k-1},x_k)-\\tilde{y}_j({\\bf x}_{1:k-1})\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, we see that inference of $a_i$ relies only on past observations ${\\bf x}_{1:i-1}$ and present \n",
    "observation $x_i$, but that inferences of $b_j$ and $W_{ij}$ also rely on future observations. This\n",
    "nicely demonstrates what we said earlier about causality, in that future observations cannot physically affect\n",
    "past observations, but they can affect inferences about past observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437826f4",
   "metadata": {},
   "source": [
    "Finally, for completeness, we derive the missing formula for $\\bar{y}_j({\\bf x}_{1:i-1},x_i)$ by noting that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}_{1:i}) & = &\n",
    "\\frac{e^{f({\\bf x}_{1:i},{\\bf y})}}\n",
    "{\\sum_{{\\bf y}'\\in\\{0,1\\}^{H}}e^{f({\\bf x}_{1:i},{\\bf y}')}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{e^{\n",
    " {\\bf a}_{1:i}^{T}{\\bf x}_{1:i}+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,:}{\\bf y}+{\\bf b}^{T}{\\bf y}\n",
    "}}\n",
    "{\n",
    "\\sum_{{\\bf y}'\\in\\{0,1\\}^{H}}\n",
    "e^{\n",
    " {\\bf a}_{1:i}^{T}{\\bf x}_{1:i}+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,:}{\\bf y}'+{\\bf b}^{T}{\\bf y}'\n",
    "}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{\\prod_{j=1}^{H}e^{(b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j})y_j}}\n",
    "{\\prod_{j=1}^{H}\\sum_{y_j=0}^{1}e^{(b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j})y_j}}\n",
    "\\\\\n",
    "& = &\n",
    "\\prod_{j=1}^{H}\\frac{e^{(b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j})y_j}}\n",
    "{1+e^{b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j}}}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "p(y_j=1\\mid{\\bf x}_{1:i})\n",
    "& = & \\sigma\\left(b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j}\\right)\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\bar{y}_j({\\bf x}_{1:i-1},x_i) & = & \n",
    "\\sigma\\left(B_{ij}+x_i W_{ij}\\right)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $B_{ij}$ was defined earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d8fc3",
   "metadata": {},
   "source": [
    "### $F$-input, $H$-output Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab4ac7",
   "metadata": {},
   "source": [
    "We now consider the special case where the binary output vector ${\\bf y}$ is restricted to being a\n",
    "$1$-of-$H$ or one-hot vector, such that ${\\bf y}\\in{\\cal Y}_H$ where \n",
    "${\\cal Y}_H=\\left\\{{\\bf y}\\in\\{0,1\\}^{H}\\mid\\sum_{j=1}^{H}y_j=1\\right\\}$.\n",
    "This model corresponds to a $H$-class classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa7075e",
   "metadata": {},
   "source": [
    "The predictive model is therefore\n",
    "\\begin{eqnarray}\n",
    "p(x_i\\mid{\\bf x}_{1:i-1}) & = &\n",
    "\\frac{\\sum_{{\\bf y}\\in{\\cal Y}_{H}}e^{f({\\bf x}_{1:i-1},x_i,{\\bf y})}}\n",
    "{\\sum_{x_i'\\in\\{0,1\\}}\n",
    "\\sum_{{\\bf y}'\\in{\\cal Y}_{H}}\n",
    "e^{f({\\bf x}_{1:i-1},x_i',{\\bf y}')}\n",
    "}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{\\sum_{{\\bf y}\\in{\\cal Y}_{H}}e^{\n",
    " {\\bf a}_{1:i-1}^{T}{\\bf x}_{1:i-1}+a_i x_i+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,:}{\\bf y}\n",
    " +x_i W_{i,:}{\\bf y}+{\\bf b}^{T}{\\bf y}\n",
    "}}\n",
    "{\\sum_{x_i'\\in\\{0,1\\}}\n",
    "\\sum_{{\\bf y}'\\in{\\cal Y}_{H}}\n",
    "e^{\n",
    " {\\bf a}_{1:i-1}^{T}{\\bf x}_{1:i-1}+a_i x_i'+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,:}{\\bf y}'\n",
    " +x_i' W_{i,:}{\\bf y}'+{\\bf b}^{T}{\\bf y}'\n",
    "}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{\n",
    " e^{a_i x_i}\n",
    " \\sum_{{\\bf y}\\in{\\cal Y}_{H}}e^{\n",
    " \\left({\\bf b}^{T}+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,:}+x_i W_{i,:}\\right){\\bf y}\n",
    "}}\n",
    "{\\sum_{x_i'\\in\\{0,1\\}}e^{a_i x_i'}\n",
    "\\sum_{{\\bf y}'\\in{\\cal Y}_{H}}\n",
    "e^{\n",
    " \\left({\\bf b}^{T}+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,:}+x_i' W_{i,:}\\right){\\bf y}'\n",
    "}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{\n",
    " e^{a_i x_i}\n",
    " \\sum_{j=1}^{H}e^{\n",
    "  b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}+x_i W_{ij}\n",
    "}}\n",
    "{\\sum_{x_i'\\in\\{0,1\\}}e^{a_i x_i'}\n",
    "\\sum_{j=1}^{H}\n",
    "e^{\n",
    "  b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}+x_i' W_{ij}\n",
    "}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{\n",
    " e^{a_i x_i+\\ln\n",
    " \\sum_{j=1}^{H}e^{\n",
    "  b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}+x_i W_{ij}\n",
    "}}}\n",
    "{\n",
    " e^{\\ln\n",
    "  \\sum_{j=1}^{H}e^{\n",
    "   b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}\n",
    "  }\n",
    " }\n",
    "+\n",
    " e^{a_i+\\ln\n",
    "  \\sum_{j=1}^{H}e^{\n",
    "   b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}+W_{ij}\n",
    "  }\n",
    " }\n",
    "}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{\n",
    " e^{a_i x_i\n",
    "  +\\ln\\sum_{j=1}^{H}e^{b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}+x_i W_{ij}}\n",
    "  -\\ln\\sum_{j=1}^{H}e^{b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}}\n",
    " }\n",
    "}\n",
    "{\n",
    " 1+e^{\n",
    "  a_i\n",
    "  +\\ln\\sum_{j=1}^{H}e^{b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}+W_{ij}}\n",
    "  -\\ln\\sum_{j=1}^{H}e^{b_j+{\\bf x}_{1:i-1}^{T}{\\bf W}_{1:i-1,j}}\n",
    " }\n",
    "}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19579dc7",
   "metadata": {},
   "source": [
    "Consequently, we have\n",
    "\\begin{eqnarray}\n",
    "\\tilde{x}_i({\\bf x}_{1:i-1}) & \\doteq & p(x_i=1\\mid{\\bf x}_{1:i-1})\n",
    "=\\sigma\\left(a_i+\\ln\\sum_{j=1}^{H}e^{B_{ij}+W_{i,j}}-\\ln\\sum_{j=1}^{H}e^{B_{ij}}\\right)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $B_{ij}$ was defined in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dd5c4",
   "metadata": {},
   "source": [
    "Similarly, we have\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}_{1:i}) & = &\n",
    "\\frac{e^{f({\\bf x}_{1:i},{\\bf y})}}\n",
    "{\\sum_{{\\bf y}'\\in{\\cal Y}_{H}}e^{f({\\bf x}_{1:i},{\\bf y}')}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{e^{\n",
    " {\\bf a}_{1:i}^{T}{\\bf x}_{1:i}+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,:}{\\bf y}+{\\bf b}^{T}{\\bf y}\n",
    "}}\n",
    "{\n",
    "\\sum_{{\\bf y}'\\in{\\cal Y}_{H}}\n",
    "e^{\n",
    " {\\bf a}_{1:i}^{T}{\\bf x}_{1:i}+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,:}{\\bf y}'+{\\bf b}^{T}{\\bf y}'\n",
    "}}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{\\prod_{j=1}^{H}e^{(b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j})y_j}}\n",
    "{\\sum_{j=1}^{H}e^{b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j}}}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "p(y_j=1\\mid{\\bf x}_{1:i})\n",
    "& = & \\frac{e^{b_j+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j}}}\n",
    "{\\sum_{j'=1}^{H}e^{b_{j'}+{\\bf x}_{1:i}^{T}{\\bf W}_{1:i,j'}}}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\bar{y}_j({\\bf x}_{1:i-1},x_i) & = & \n",
    "\\frac{e^{B_{ij}+x_i W_{ij}}}\n",
    "{\\sum_{j'=1}^{H}e^{B_{ij'}+x_i W_{ij'}}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0184e27",
   "metadata": {},
   "source": [
    "### Continue the Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f0397",
   "metadata": {},
   "source": [
    "Now let us continue the experiment by training on the exact model (which sums over all possible outputs ${\\bf y}$), rather than using the mean-field approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aa515c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4721875 0.5278125]\n"
     ]
    }
   ],
   "source": [
    "rbm = ExactSequentialBernoulliRBM(\n",
    "    num_output, num_input, \n",
    "    batch_size=1.0\n",
    ")\n",
    "rbm.fit(X_train)\n",
    "\n",
    "preds = rbm.reconstruct(X_states)\n",
    "probs = preds[:, 0]  # p(x_1)\n",
    "probs = np.array(list(zip(1 - probs, probs)))\n",
    "print(probs[range(2), X_states[:, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12971a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.86721418e-10]),\n",
       " array([[-1.77391897e-10]]),\n",
       " array([-3.57298949e-11]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm._compute_score_gradients(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480aac9e",
   "metadata": {},
   "source": [
    "We see that the model appears to have converged, and we obtained the empirical probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93eeeab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x_1=1) = 0.527813\n"
     ]
    }
   ],
   "source": [
    "print(\"p(x_1=1) = %f\" % p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fae92f",
   "metadata": {},
   "source": [
    "### 2-input, 1-output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4109f5b",
   "metadata": {},
   "source": [
    "Let us now examine the difference, if any, between the two models (exact and mean-field) for the case of 2 binary inputs. We retain 1 binary output for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e685c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output = 1\n",
    "num_input = 2\n",
    "X_train = _window(b_series, num_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fb45fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_states = np.array([[0,0],[0,1], [1,0], [1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3e640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x_1=1) = 0.527977, p(x_2=1) = 0.527665\n"
     ]
    }
   ],
   "source": [
    "ind1 = X_train[:, 0] == 1\n",
    "p1 = sum(ind1) / len(ind1)\n",
    "ind2 = X_train[:, 1] == 1\n",
    "p2 = sum(ind2) / len(ind2)\n",
    "print(\"p(x_1=1) = %f, p(x_2=1) = %f\" % (p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14ed0ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x_2=1|x_1=0) = 0.556954, p(x_2=1|x_1=1) = 0.501480\n"
     ]
    }
   ],
   "source": [
    "p2g0 = sum(ind2 & ~ind1) / sum(~ind1) \n",
    "p2g1 = sum(ind2 & ind1) / sum(ind1)\n",
    "print(\"p(x_2=1|x_1=0) = %f, p(x_2=1|x_1=1) = %f\" % (p2g0, p2g1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efcdde89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52797749 0.55695364]\n",
      " [0.52797749 0.55695364]\n",
      " [0.52797749 0.50148017]\n",
      " [0.52797749 0.50148017]]\n"
     ]
    }
   ],
   "source": [
    "rbm = ExactSequentialBernoulliRBM(\n",
    "    num_output, num_input,\n",
    "    n_iter=10000,\n",
    "    batch_size=1.0\n",
    ")\n",
    "rbm.fit(X_train)\n",
    "\n",
    "preds = rbm.reconstruct(X_states)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a55b9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.07885486e-14, -1.41857558e-13]),\n",
       " array([[-3.73429744e-13],\n",
       "        [ 3.24606307e-13]]),\n",
       " array([-3.59688313e-14]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm._compute_score_gradients(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75334967",
   "metadata": {},
   "source": [
    "Note that we require about 10,000 iterations to converge to the empirical values, even for this simple model.\n",
    "Clearly, some form of gradient acceleration would be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff45f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52797749 0.55695364]\n",
      " [0.52797749 0.55695364]\n",
      " [0.52797749 0.50148017]\n",
      " [0.52797749 0.50148017]]\n"
     ]
    }
   ],
   "source": [
    "rbm = SequentialBernoulliRBM(\n",
    "    num_output, num_input,\n",
    "    n_iter=10000,\n",
    "    batch_size=1.0\n",
    ")\n",
    "rbm.fit(X_train)\n",
    "\n",
    "preds = rbm.reconstruct(X_states)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6059127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.46681153e-12,  2.68710371e-12]),\n",
       " array([[ 4.12093269e-12],\n",
       "        [-4.44209540e-12]]),\n",
       " array([-1.75486504e-12]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm._compute_score_gradients(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bda87",
   "metadata": {},
   "source": [
    "Surprisingly, there doesn't seem to be any difference between the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af21a5",
   "metadata": {},
   "source": [
    "And now for something completely different - let's take a quick look at gradient acceleration.\n",
    "\n",
    "In particular, we use a 1-step LBFGS algorithm with optional line-search (to ensure the `x_score` always increases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fe145",
   "metadata": {},
   "source": [
    "To explain the algorithm, we use \n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS) as a reference, including\n",
    "copying its notation, but assume exactly $m=1$ step.\n",
    "\n",
    "Imagine we have some parameter estimate ${\\bf x}_{k-1}$, and\n",
    "then move to a new estimate ${\\bf x}_k$. \n",
    "The second-order Taylor series expansion is then\n",
    "\\begin{eqnarray}\n",
    "f({\\bf x}_k) & = & f({\\bf x}_{k-1})+\\nabla^{T}f({\\bf x}_{k-1})\n",
    "\\,({\\bf x}_k-{\\bf x}_{k-1})\n",
    "+({\\bf x}_k-{\\bf x}_{k-1})^{T}\\nabla\\nabla^{T}f({\\bf x}_{k-1})\n",
    "\\,({\\bf x}_k-{\\bf x}_{k-1})+O(\\|{\\bf x}_k-{\\bf x}_{k-1}\\|^3)\n",
    "\\\\& = & f({\\bf x}_{k-1})+{\\bf g}_{k-1}^{T}{\\bf s}_{k-1}\n",
    "+{\\bf s}_{k-1}^{T}{\\bf H}_{k-1}^{-1}{\\bf s}_{k-1}+O(\\|{\\bf s}_{k-1}\\|^3)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and its gradient is\n",
    "\\begin{eqnarray}\n",
    "\\nabla f({\\bf x}_k) & = & \\nabla f({\\bf x}_{k-1})\n",
    "+\\nabla\\nabla^{T}f({\\bf x}_{k-1})\\,({\\bf x}_k-{\\bf x}_{k-1})\n",
    "+O(\\|{\\bf x}_k-{\\bf x}_{k-1}\\|^2)\n",
    "\\\\\\Rightarrow{\\bf g}_k & = &\n",
    "{\\bf g}_{k-1}+{\\bf H}_{k-1}^{-1}{\\bf s}_{k-1}+O(\\|{\\bf s}_{k-1}\\|^2)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Letting ${\\bf y}_{k-1}\\doteq{\\bf g}_k-{\\bf g}_{k-1}$ then gives\n",
    "\\begin{eqnarray}\n",
    "{\\bf y}_{k-1} \\approx {\\bf H}_{k-1}^{-1}{\\bf s}_{k-1}\n",
    "& \\Rightarrow & \n",
    "{\\bf y}_{k-1}^{T}{\\bf H}_{k-1}{\\bf y}_{k-1} \\approx {\\bf y}_{k-1}^{T}{\\bf s}_{k-1}\\,.\n",
    "\\end{eqnarray}\n",
    "For convenience, we assume\n",
    "\\begin{eqnarray}\n",
    "{\\bf H}_{k-1}\\approx\\gamma_k{\\bf I} & \\Rightarrow &\n",
    "\\gamma_k \\doteq \n",
    "\\frac{{\\bf y}_{k-1}^{T}{\\bf s}_{k-1}}\n",
    "{{\\bf y}_{k-1}^{T}{\\bf y}_{k-1}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that [Wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS) called this\n",
    "${\\bf H}^0_k$ rather than ${\\bf H}_{k-1}$, in order to allow for further (i.e. $m>1$)\n",
    "steps into the past; presumably this is why they defined $\\gamma_k$ rather than $\\gamma_{k-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b997b1",
   "metadata": {},
   "source": [
    "We now seek an update to the next parameter estimate ${\\bf x}_{k+1}$.\n",
    "This is obtained from the gradient equation\n",
    "\\begin{eqnarray}\n",
    "\\nabla f({\\bf x}_{k+1}) & = & \\nabla f({\\bf x}_{k})\n",
    "+\\nabla\\nabla^{T}f({\\bf x}_{k})\\,({\\bf x}_{k+1}-{\\bf x}_{k})\n",
    "+O(\\|{\\bf x}_{k+1}-{\\bf x}_{k}\\|^2)\n",
    "\\end{eqnarray}\n",
    "by assuming that $\\nabla f({\\bf x}_{k+1})\\approx{\\bf 0}$,\n",
    "giving\n",
    "\\begin{eqnarray}\n",
    "{\\bf x}_{k+1} & \\approx & {\\bf x}_k\n",
    "-\\left[\\nabla\\nabla^T f({\\bf x}_k)\\right]^{-1}\\,\\nabla f({\\bf x}_k)\n",
    "={\\bf x}_k-{\\bf H}_k{\\bf g}_k\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, the Newton-Raphson update direction is just \n",
    "${\\bf d}_k=-{\\bf H}_k{\\bf g}_k$. Note that the actual update will typically be some\n",
    "${\\bf s}_k=\\epsilon\\,{\\bf d}_k$, with step-size $\\epsilon$, to allow for line searching. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04d2ef",
   "metadata": {},
   "source": [
    "\n",
    "The BFGS update from ${\\bf H}_{k-1}$ to ${\\bf H}_k$ is given by\n",
    "\\begin{eqnarray}\n",
    "{\\bf H}_k & = &\n",
    "\\left(I-\\rho_{k-1}{\\bf s}_{k-1}{\\bf y}_{k-1}^T\\right){\\bf H}_{k-1}\n",
    "\\left(I-\\rho_{k-1}{\\bf y}_{k-1}{\\bf s}_{k-1}^T\\right)\n",
    "+\\rho_{k-1}{\\bf s}_{k-1}{\\bf s}_{k-1}^{T}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\rho_{k-1}\\doteq\\frac{1}{{\\bf y}_{k-1}^T{\\bf s}_{k-1}}$.\n",
    "Thus, the update direction is\n",
    "\\begin{eqnarray}\n",
    "{\\bf d}_k & = & -{\\bf H}_k{\\bf g}_k\n",
    "\\\\& \\approx &\n",
    "-\\left(I-\\rho_{k-1}{\\bf s}_{k-1}{\\bf y}_{k-1}^T\\right)\\gamma_{k}{\\bf I}\n",
    "\\left(I-\\rho_{k-1}{\\bf y}_{k-1}{\\bf s}_{k-1}^T\\right){\\bf g}_k\n",
    "-\\rho_{k-1}{\\bf s}_{k-1}{\\bf s}_{k-1}^{T}{\\bf g}_k\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "For convenience, we define $\\alpha_{k-1}\\doteq\\rho_{k-1}{\\bf s}_{k-1}^{T}{\\bf g}_k$,\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "{\\bf q}_{k-1} & \\doteq & \\left(I-\\rho_{k-1}{\\bf y}_{k-1}{\\bf s}_{k-1}^T\\right){\\bf g}_k\n",
    "={\\bf g}_k-\\alpha_{k-1}{\\bf y}_{k-1}\\,,\n",
    "\\end{eqnarray}\n",
    "resulting in\n",
    "\\begin{eqnarray}\n",
    "{\\bf d}_k & \\approx &\n",
    "-\\left(I-\\rho_{k-1}{\\bf s}_{k-1}{\\bf y}_{k-1}^T\\right)\\gamma_{k}{\\bf q}_{k-1}\n",
    "-\\alpha_{k-1}{\\bf s}_{k-1}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, we define \n",
    "$\\beta_{k-1}\\doteq\\gamma_k\\rho_{k-1}{\\bf y}_{k-1}^T{\\bf q}_{k-1}$ for convenience,\n",
    "giving\n",
    "\\begin{eqnarray}\n",
    "{\\bf d}_k & \\approx &\n",
    "-\\gamma_{k}{\\bf q}_{k-1}+\\beta_{k-1}{\\bf s}_{k-1}\n",
    "-\\alpha_{k-1}{\\bf s}_{k-1}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this direction is called ${\\bf z}$ in [Wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d2283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(a, b):\n",
    "    return tuple(_a - _b for _a, _b in zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "437ad4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    return sum(np.vdot(_a, _b) for _a, _b in zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76a1e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(s, v):\n",
    "    return tuple(s * _v for _v in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20f6ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return tuple(_a + _b for _a, _b in zip(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "093257b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_accel(s0, y0, g1):\n",
    "    #s0 = diff(x1, x0)\n",
    "    #y0 = diff(g1, g0)\n",
    "    k1 = dot(s0, y0)  # 1 / rho_0\n",
    "    k2 = dot(y0, y0)\n",
    "    k3 = dot(s0, g1)\n",
    "    alpha_0 = k3 / k1\n",
    "    gamma_1 = k1 / k2\n",
    "    d = mult(-gamma_1, diff(g1, mult(alpha_0, y0)))\n",
    "    beta0 = -dot(y0, d) / k1\n",
    "    d = add(d, mult(beta0 - alpha_0, s0))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c7d8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_abs(x):\n",
    "    return np.max([np.max(np.abs(_x)) for _x in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d874cd74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter= 0, score=-1.39138621\n",
      "iter= 1, score=-1.38886319\n",
      "iter= 2, score=-1.38302286\n",
      "iter= 3, score=-1.38301780\n",
      "iter= 4, score=-1.38297607\n",
      "iter= 5, score=-1.38282466\n",
      "iter= 6, score=-1.38218130*\n",
      "iter= 7, score=-1.38187787*\n",
      "iter= 8, score=-1.38169158*\n",
      "iter= 9, score=-1.38165707\n",
      "iter=10, score=-1.38165699\n",
      "iter=11, score=-1.38165695\n",
      "iter=12, score=-1.38165693\n",
      "iter=13, score=-1.38165691\n",
      "iter=14, score=-1.38165688\n",
      "iter=15, score=-1.38165688\n",
      "iter=16, score=-1.38165688\n",
      "iter=17, score=-1.38165688\n",
      "iter=18, score=-1.38165688\n",
      "iter=19, score=-1.38165688\n",
      "iter=20, score=-1.38165688*\n",
      "iter=21, score=-1.38165688\n",
      "iter=22, score=-1.38165688\n",
      "iter=23, score=-1.38165688\n",
      "iter=24, score=-1.38165688\n",
      "iter=25, score=-1.38165688\n",
      "iter=26, score=-1.38165688\n",
      "iter=27, score=-1.38165688\n",
      "iter=28, score=-1.38165688\n",
      "iter=29, score=-1.38165688\n",
      "iter=30, score=-1.38165688*\n",
      "iter=31, score=-1.38165688\n",
      "iter=32, score=-1.38165688\n",
      "iter=33, score=-1.38165688*\n",
      "total steps=42\n"
     ]
    }
   ],
   "source": [
    "rbm = SequentialBernoulliRBM(\n",
    "    num_output, num_input,\n",
    "    n_iter=1,\n",
    "    batch_size=1.0\n",
    ")\n",
    "num_iters = 0\n",
    "score0 = rbm.score(X_train)['x_score']\n",
    "print(\"iter=%2d, score=%10.8f\" % (num_iters, score0))\n",
    "x0 = rbm.copy_parameters()\n",
    "g0 = rbm._compute_score_gradients(X_train)\n",
    "rho0 = rbm.step_size\n",
    "num_iters += 1\n",
    "# Perform simple gradient ascent\n",
    "x1 = add(x0, mult(rho0, g0))\n",
    "num_steps = 1\n",
    "rbm._set_parameters(*x1)\n",
    "score1 = rbm.score(X_train)['x_score']\n",
    "print(\"iter=%2d, score=%10.8f\" % (num_iters, score1))\n",
    "g1 = rbm._compute_score_gradients(X_train)\n",
    "while True:\n",
    "    s0 = diff(x1, x0)\n",
    "    y0 = diff(g1, g0)\n",
    "    if max_abs(y0) < 1e-8: # Any smaller leads to instability\n",
    "        # Gradient has barely changed\n",
    "        #   - stop to prevent division by zero\n",
    "        break\n",
    "    # Compute quasi-Newton gradient direction\n",
    "    d = grad_accel(s0, y0, g1)\n",
    "    x0 = x1\n",
    "    g0 = g1\n",
    "    score0 = score1\n",
    "    ls = False\n",
    "    while True:\n",
    "        # Perform optional line-search until objective score improves\n",
    "        x1 = add(x0, d)\n",
    "        num_steps += 1\n",
    "        rbm._set_parameters(*x1)\n",
    "        score1 = rbm.score(X_train)['x_score']\n",
    "        if score1 < score0:\n",
    "            ls = True\n",
    "            d = mult(0.5, d)\n",
    "        else:\n",
    "            break\n",
    "    g1 = rbm._compute_score_gradients(X_train)\n",
    "    num_iters += 1\n",
    "    print(\"iter=%2d, score=%10.8f%s\" % (num_iters, score1, \"*\" if ls else \"\"))\n",
    "print(\"total steps=%d\" % num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c08057db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.98904858e-10, -3.52086159e-09]),\n",
       " array([[-1.82154583e-09],\n",
       "        [-1.27871612e-10]]),\n",
       " array([1.56080269e-09]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm._compute_score_gradients(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58e892f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52797749 0.55695367]\n",
      " [0.52797749 0.55695367]\n",
      " [0.52797749 0.50148015]\n",
      " [0.52797749 0.50148015]]\n"
     ]
    }
   ],
   "source": [
    "preds = rbm.reconstruct(X_states)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd189d9",
   "metadata": {},
   "source": [
    "Although the number of iterations required depends strongly on the random initialisation,\n",
    "we see that even 1-step LBFGS is significantly better than the (approximately) 10,000 iterations required without gradient acceleration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d791c1",
   "metadata": {},
   "source": [
    "We note that gradient acceleration schemes tend to exacerbate overshooting, which is manifested by the log-likelihood score oscillating rather than smoothly increasing.\n",
    "To prevent this, we require knowledge of the score being optimised, in order to perform\n",
    "line search along the quasi-gradient direction. Ordinary gradient ascent can also overshoot, but typically suffers from undershooting.\n",
    "\n",
    "For our RBMs, this means knowing in advance whether we are optimising `x_score`, `y_score` or `xy_score`, which requires more detailed information about the RBM settings and computations.\n",
    "\n",
    "Alternatively, we could potentially detect oscillation without knowledge of the score, in the case where consecutive gradients point in \"opposite\" directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68578a99",
   "metadata": {},
   "source": [
    "Let's try a simple acceleration scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92fd02cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: iter=  0, score=-1.38574164, rho=0.500000\n",
      "DEBUG: iter=  1, score=-1.38335399, rho=3.188998\n",
      "DEBUG: iter=  2, score=-1.38332831, rho=24.265647\n",
      "DEBUG: iter=  3, score=-1.38330405, rho=3.501003\n",
      "DEBUG: iter=  4, score=-1.38328073, rho=24.190131\n",
      "DEBUG: iter=  5, score=-1.38325842, rho=3.510679\n",
      "DEBUG: iter=  6, score=-1.38323673, rho=24.182819\n",
      "DEBUG: iter=  7, score=-1.38321571, rho=3.517796\n",
      "DEBUG: iter=  8, score=-1.38319505, rho=24.223687\n",
      "DEBUG: iter=  9, score=-1.38317478, rho=3.522110\n",
      "DEBUG: iter= 10, score=-1.38315969, rho=12.150934+\n",
      "DEBUG: iter= 11, score=-1.38315071, rho=5.522082\n",
      "DEBUG: iter= 12, score=-1.38314399, rho=3.478143+\n",
      "DEBUG: iter= 13, score=-1.38297196, rho=232.301231\n",
      "DEBUG: iter= 14, score=-1.38284810, rho=1.563340+\n",
      "DEBUG: iter= 15, score=-1.38281501, rho=1.643893+\n",
      "DEBUG: iter= 16, score=-1.38280404, rho=2.036955+\n",
      "DEBUG: iter= 17, score=-1.38279332, rho=6.002907+\n",
      "DEBUG: iter= 18, score=-1.38277799, rho=13.029081\n",
      "DEBUG: iter= 19, score=-1.38276629, rho=2.013571+\n",
      "DEBUG: iter= 20, score=-1.38275544, rho=5.680813+\n",
      "DEBUG: iter= 21, score=-1.38273804, rho=14.588799\n",
      "DEBUG: iter= 22, score=-1.38272476, rho=1.940165+\n",
      "DEBUG: iter= 23, score=-1.38271462, rho=4.664044+\n",
      "DEBUG: iter= 24, score=-1.38268549, rho=26.081194\n",
      "DEBUG: iter= 25, score=-1.38266314, rho=1.725945+\n",
      "DEBUG: iter= 26, score=-1.38265346, rho=2.631283+\n",
      "DEBUG: iter= 27, score=-1.38254548, rho=69.172453+\n",
      "DEBUG: iter= 28, score=-1.38249945, rho=3.309566\n",
      "DEBUG: iter= 29, score=-1.38246983, rho=15.805861+\n",
      "DEBUG: iter= 30, score=-1.38245466, rho=4.493099\n",
      "DEBUG: iter= 31, score=-1.38244323, rho=4.388767+\n",
      "DEBUG: iter= 32, score=-1.38241592, rho=14.173044+\n",
      "DEBUG: iter= 33, score=-1.38240496, rho=2.365928+\n",
      "DEBUG: iter= 34, score=-1.38236162, rho=23.507684+\n",
      "DEBUG: iter= 35, score=-1.38234256, rho=3.836787\n",
      "DEBUG: iter= 36, score=-1.38232827, rho=6.336005+\n",
      "DEBUG: iter= 37, score=-1.38231587, rho=5.110904+\n",
      "DEBUG: iter= 38, score=-1.38229878, rho=8.125677+\n",
      "DEBUG: iter= 39, score=-1.38228836, rho=3.585739+\n",
      "DEBUG: iter= 40, score=-1.38221235, rho=43.240999+\n",
      "DEBUG: iter= 41, score=-1.38218354, rho=3.330348\n",
      "DEBUG: iter= 42, score=-1.38216217, rho=11.453990+\n",
      "DEBUG: iter= 43, score=-1.38215273, rho=2.600872+\n",
      "DEBUG: iter= 44, score=-1.38200931, rho=89.640815+\n",
      "DEBUG: iter= 45, score=-1.38197681, rho=1.529449+\n",
      "DEBUG: iter= 46, score=-1.38196640, rho=1.846300+\n",
      "DEBUG: iter= 47, score=-1.38195809, rho=4.596178+\n",
      "DEBUG: iter= 48, score=-1.38194401, rho=9.826845+\n",
      "DEBUG: iter= 49, score=-1.38193723, rho=2.838226+\n",
      "DEBUG: iter= 50, score=-1.38166673, rho=270.371180+\n",
      "DEBUG: iter= 51, score=-1.38166270, rho=3.035446\n",
      "DEBUG: iter= 52, score=-1.38166243, rho=4.873750+\n",
      "DEBUG: iter= 53, score=-1.38166207, rho=8.153147+\n",
      "DEBUG: iter= 54, score=-1.38166188, rho=3.253439+\n",
      "DEBUG: iter= 55, score=-1.38166042, rho=42.716705+\n",
      "DEBUG: iter= 56, score=-1.38166015, rho=1.587619+\n",
      "DEBUG: iter= 57, score=-1.38166003, rho=2.487037+\n",
      "DEBUG: iter= 58, score=-1.38165882, rho=56.311853+\n",
      "DEBUG: iter= 59, score=-1.38165858, rho=3.085191\n",
      "DEBUG: iter= 60, score=-1.38165843, rho=11.762057+\n",
      "DEBUG: iter= 61, score=-1.38165837, rho=2.340660+\n",
      "DEBUG: iter= 62, score=-1.38165804, rho=31.087451+\n",
      "DEBUG: iter= 63, score=-1.38165797, rho=1.666165+\n",
      "DEBUG: iter= 64, score=-1.38165793, rho=3.142095+\n",
      "DEBUG: iter= 65, score=-1.38165755, rho=51.019421+\n",
      "DEBUG: iter= 66, score=-1.38165749, rho=1.555258+\n",
      "DEBUG: iter= 67, score=-1.38165747, rho=2.281191+\n",
      "DEBUG: iter= 68, score=-1.38165737, rho=24.610219+\n",
      "DEBUG: iter= 69, score=-1.38165733, rho=3.499415\n",
      "DEBUG: iter= 70, score=-1.38165731, rho=6.149866+\n",
      "DEBUG: iter= 71, score=-1.38165729, rho=4.410568+\n",
      "DEBUG: iter= 72, score=-1.38165726, rho=9.125729+\n",
      "DEBUG: iter= 73, score=-1.38165724, rho=2.798208+\n",
      "DEBUG: iter= 74, score=-1.38165697, rho=106.772091+\n",
      "DEBUG: iter= 75, score=-1.38165697, rho=2.579095+\n",
      "DEBUG: iter= 76, score=-1.38165692, rho=78.563738+\n",
      "DEBUG: iter= 77, score=-1.38165691, rho=1.528862+\n",
      "DEBUG: iter= 78, score=-1.38165691, rho=2.122512+\n",
      "DEBUG: iter= 79, score=-1.38165691, rho=13.651626+\n",
      "DEBUG: iter= 80, score=-1.38165691, rho=2.151493+\n",
      "DEBUG: iter= 81, score=-1.38165690, rho=15.156752+\n",
      "DEBUG: iter= 82, score=-1.38165690, rho=2.049866+\n",
      "DEBUG: iter= 83, score=-1.38165690, rho=10.586467+\n",
      "DEBUG: iter= 84, score=-1.38165690, rho=2.495707+\n",
      "DEBUG: iter= 85, score=-1.38165689, rho=58.149649+\n",
      "DEBUG: iter= 86, score=-1.38165689, rho=1.539756+\n",
      "DEBUG: iter= 87, score=-1.38165689, rho=2.188621+\n",
      "DEBUG: iter= 88, score=-1.38165689, rho=17.300961+\n",
      "DEBUG: iter= 89, score=-1.38165689, rho=3.893266\n",
      "DEBUG: iter= 90, score=-1.38165689, rho=4.522399+\n",
      "DEBUG: iter= 91, score=-1.38165689, rho=8.432208+\n",
      "DEBUG: iter= 92, score=-1.38165689, rho=2.996896+\n",
      "DEBUG: iter= 93, score=-1.38165688, rho=71.278422+\n",
      "DEBUG: iter= 94, score=-1.38165688, rho=1.527088+\n",
      "DEBUG: iter= 95, score=-1.38165688, rho=2.113458+\n",
      "DEBUG: iter= 96, score=-1.38165688, rho=13.231398+\n",
      "DEBUG: iter= 97, score=-1.38165688, rho=2.184985+\n",
      "DEBUG: iter= 98, score=-1.38165688, rho=17.152054+\n",
      "DEBUG: iter= 99, score=-1.38165688, rho=1.949892+\n",
      "DEBUG: iter=100, score=-1.38165688, rho=7.586680+\n",
      "DEBUG: iter=101, score=-1.38165688, rho=3.338176+\n",
      "DEBUG: iter=102, score=-1.38165688, rho=32.040637+\n",
      "DEBUG: iter=103, score=-1.38165688, rho=3.307498\n",
      "DEBUG: iter=104, score=-1.38165688, rho=7.661698+\n",
      "DEBUG: iter=105, score=-1.38165688, rho=3.301261+\n",
      "DEBUG: iter=106, score=-1.38165688, rho=34.504615+\n",
      "DEBUG: iter=107, score=-1.38165688, rho=3.266141\n",
      "DEBUG: iter=108, score=-1.38165688, rho=8.138236+\n",
      "DEBUG: iter=109, score=-1.38165688, rho=3.100511+\n",
      "DEBUG: iter=110, score=-1.38165688, rho=54.696100+\n",
      "DEBUG: iter=111, score=-1.38165688, rho=3.091689\n",
      "DEBUG: iter=112, score=-1.38165688, rho=11.319854+\n",
      "DEBUG: iter=113, score=-1.38165688, rho=2.386997+\n",
      "DEBUG: iter=114, score=-1.38165688, rho=37.745742+\n",
      "DEBUG: iter=115, score=-1.38165688, rho=1.610754+\n",
      "DEBUG: iter=116, score=-1.38165688, rho=2.693604+\n",
      "DEBUG: iter=117, score=-1.38165688, rho=102.042705+\n",
      "DEBUG: iter=118, score=-1.38165688, rho=3.819397\n",
      "DEBUG: iter=119, score=-1.38165688, rho=4.725572+\n",
      "DEBUG: iter=120, score=-1.38165688, rho=7.468784+\n",
      "DEBUG: iter=121, score=-1.38165688, rho=3.395513+\n",
      "DEBUG: iter=122, score=-1.38165688, rho=28.674434+\n",
      "DEBUG: iter=123, score=-1.38165688, rho=1.688908+\n",
      "DEBUG: iter=124, score=-1.38165688, rho=3.388576+\n",
      "DEBUG: iter=125, score=-1.38165688, rho=29.045614+\n",
      "DEBUG: iter=126, score=-1.38165688, rho=1.684538+\n",
      "DEBUG: iter=127, score=-1.38165688, rho=3.345043+\n",
      "DEBUG: iter=128, score=-1.38165688, rho=31.568284+\n",
      "DEBUG: iter=129, score=-1.38165688, rho=1.658073+\n",
      "DEBUG: iter=130, score=-1.38165688, rho=3.093893+\n",
      "DEBUG: iter=131, score=-1.38165688, rho=55.563946+\n",
      "DEBUG: iter=132, score=-1.38165688, rho=1.544018+\n",
      "DEBUG: iter=133, score=-1.38165688, rho=2.220144+\n",
      "DEBUG: iter=134, score=-1.38165688, rho=19.581910+\n",
      "DEBUG: iter=135, score=-1.38165688, rho=3.724627\n",
      "DEBUG: iter=136, score=-1.38165688, rho=5.042870+\n",
      "DEBUG: iter=137, score=-1.38165688, rho=6.379149+\n",
      "DEBUG: iter=138, score=-1.38165688, rho=4.158786+\n",
      "DEBUG: iter=139, score=-1.38165688, rho=10.992081+\n",
      "DEBUG: iter=140, score=-1.38165688, rho=2.432154+\n",
      "DEBUG: iter=141, score=-1.38165688, rho=45.287306+\n",
      "DEBUG: iter=142, score=-1.38165688, rho=3.147402\n",
      "DEBUG: iter=143, score=-1.38165688, rho=10.017718+\n",
      "DEBUG: iter=144, score=-1.38165688, rho=2.594061+\n",
      "DEBUG: iter=145, score=-1.38165688, rho=81.979837+\n",
      "DEBUG: iter=146, score=-1.38165688, rho=1.536929+\n",
      "DEBUG: iter=147, score=-1.38165688, rho=2.175250+\n",
      "DEBUG: iter=148, score=-1.38165688, rho=16.560463+\n",
      "DEBUG: iter=149, score=-1.38165688, rho=3.951166\n",
      "DEBUG: iter=150, score=-1.38165688, rho=4.365102+\n",
      "DEBUG: iter=151, score=-1.38165688, rho=9.362262+\n",
      "DEBUG: iter=152, score=-1.38165688, rho=2.733603+\n",
      "DEBUG: iter=153, score=-1.38165688, rho=105.569807+\n",
      "DEBUG: iter=154, score=-1.38165688, rho=5.111165+\n",
      "DEBUG: iter=155, score=-1.38165688, rho=6.190296+\n",
      "DEBUG: iter=156, score=-1.38165688, rho=4.350304+\n",
      "DEBUG: iter=157, score=-1.38165688, rho=9.462372+\n",
      "DEBUG: iter=158, score=-1.38165688, rho=2.710284+\n",
      "DEBUG: iter=159, score=-1.38165688, rho=103.889808+\n",
      "DEBUG: iter=160, score=-1.38165688, rho=2.317099+\n",
      "DEBUG: iter=161, score=-1.38165688, rho=28.522554+\n",
      "DEBUG: iter=162, score=-1.38165688, rho=1.690716+\n",
      "DEBUG: iter=163, score=-1.38165688, rho=6.813848\n",
      "DEBUG: iter=164, score=-1.38165688, rho=2.263719+\n",
      "DEBUG: iter=165, score=-1.38165688, rho=23.135870+\n",
      "DEBUG: iter=166, score=-1.38165688, rho=3.549424\n",
      "DEBUG: iter=167, score=-1.38165688, rho=11.641497\n",
      "DEBUG: iter=168, score=-1.38165688, rho=1.774712+\n",
      "DEBUG: iter=169, score=-1.38165688, rho=8.764849\n",
      "DEBUG: iter=170, score=-1.38165688, rho=1.972049+\n",
      "DEBUG: iter=171, score=-1.38165688, rho=8.165576+\n",
      "DEBUG: iter=172, score=-1.38165688, rho=6.179736\n",
      "DEBUG: iter=173, score=-1.38165688, rho=2.429351+\n",
      "DEBUG: iter=174, score=-1.38165688, rho=44.780453+\n",
      "DEBUG: iter=175, score=-1.38165688, rho=1.575691+\n",
      "DEBUG: iter=176, score=-1.38165688, rho=2.433045+\n"
     ]
    }
   ],
   "source": [
    "rbm = SequentialBernoulliRBM(\n",
    "    num_output, num_input,\n",
    "    n_iter=1,\n",
    "    batch_size=1.0\n",
    ")\n",
    "\n",
    "rho0 = rbm.step_size\n",
    "\n",
    "num_iters = 0\n",
    "num_steps = 0\n",
    "num_grads = 0\n",
    "\n",
    "score = rbm.score(X_train)['x_score']  # not used in algorithm\n",
    "print(\"DEBUG: iter=%3d, score=%10.8f, rho=%f\" % (num_iters, score, rho0))\n",
    "\n",
    "x0 = rbm.copy_parameters()\n",
    "g0 = rbm._compute_score_gradients(X_train)\n",
    "num_grads += 1\n",
    "g0_g0 = dot(g0, g0)\n",
    "while np.sqrt(g0_g0) > 1e-8:\n",
    "    rho = rho0\n",
    "    ls1 = False\n",
    "    while True:\n",
    "        # Simple gradient step\n",
    "        x1 = add(x0, mult(rho, g0))\n",
    "        rbm._set_parameters(*x1)\n",
    "        num_steps += 1\n",
    "        g1 = rbm._compute_score_gradients(X_train)\n",
    "        num_grads += 1\n",
    "        # Estimate quadratic turning point\n",
    "        g1_g0 = dot(g1, g0)\n",
    "        rho_star = -rho0 * g0_g0 / (g1_g0 - g0_g0)\n",
    "        if rho_star < 0:\n",
    "            # Overshoot - perform line search\n",
    "            rho *= 0.5\n",
    "            ls1 = True\n",
    "        else:\n",
    "            rho = rho_star\n",
    "            break\n",
    "    ls2 = False\n",
    "    while True:\n",
    "        # Accelerated gradient step\n",
    "        x1 = add(x0, mult(rho, g0))\n",
    "        rbm._set_parameters(*x1)\n",
    "        num_steps += 1\n",
    "        g1 = rbm._compute_score_gradients(X_train)\n",
    "        num_grads += 1\n",
    "        g1_g0 = dot(g1, g0)\n",
    "        if g1_g0 < 0:\n",
    "            # Oscillation - perform line search\n",
    "            rho *= 0.5\n",
    "            ls2 = True\n",
    "        else:\n",
    "            break\n",
    "    # Reset at new position\n",
    "    num_iters += 1\n",
    "    ls1 = (\"-\" if ls1 else \"\")\n",
    "    ls2 = (\"+\" if ls2 else \"\")\n",
    "    score = rbm.score(X_train)['x_score']  # not used in algorithm\n",
    "    print(\n",
    "        \"DEBUG: iter=%3d, score=%10.8f, rho=%f%s%s\" \n",
    "        % (num_iters, score, rho, ls1, ls2)\n",
    "    )\n",
    "    x0 = x1\n",
    "    g0 = g1\n",
    "    g0_g0 = dot(g0, g0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b64e18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iters=176, num_steps=494, num_grads=495\n"
     ]
    }
   ],
   "source": [
    "print(\"num_iters=%d, num_steps=%d, num_grads=%d\" % (num_iters, num_steps, num_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c9c1dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.40472600e-09,  4.37890231e-09]),\n",
       " array([[ 5.02921309e-09],\n",
       "        [-5.25535339e-09]]),\n",
       " array([-2.65663522e-09]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm._compute_score_gradients(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "434e4a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5279775  0.55695357]\n",
      " [0.5279775  0.55695357]\n",
      " [0.5279775  0.50148022]\n",
      " [0.5279775  0.50148022]]\n"
     ]
    }
   ],
   "source": [
    "preds = rbm.reconstruct(X_states)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d92f2a",
   "metadata": {},
   "source": [
    "We see that even this simplistic acceleration, which computes at least two gradients per iteration, greatly improves on having no acceleration.\n",
    "\n",
    "Note that we also seem to have prevented score oscillation, even without the algorithm actually computing the score. In fact, computing the score is a distraction, since it appears to converge way before the gradient shrinks enough to halt. Remember that it is the gradient that controls how accurately the model predictions match the empirical probabilities. Of course, this also essentially means we are deliberately (over?) fitting the training data, so the actual convergence test must remain domain-sepcific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2501e",
   "metadata": {},
   "source": [
    "### Future prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9d72c",
   "metadata": {},
   "source": [
    "Let us now consider a practical prediction task. We first train the RBM model on fortnightly binary sequences of daily stock rises and/or falls. We then test the model by using weekly sequences to predict the characteristics of the following weeks. For convenience, we compute the proportion of daily rises in the second week as a coarse measure of the weekly trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bc9b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output = 7\n",
    "num_input = 14\n",
    "X_train = _window(b_series, num_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb906a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter= 0, score=-9.70296379\n",
      "iter= 1, score=-9.69084844\n",
      "iter=1000, score=-9.63359750\n",
      "iter=2000, score=-9.63122430*\n",
      "iter=3000, score=-9.63120562\n",
      "iter=4000, score=-9.63120220\n",
      "iter=5000, score=-9.63120007\n",
      "iter=6000, score=-9.63119798\n",
      "iter=7000, score=-9.63119776\n",
      "iter=8000, score=-9.63119660\n",
      "iter=9000, score=-9.63119641\n",
      "iter=10000, score=-9.63119637\n",
      "iter=11000, score=-9.63119631\n",
      "iter=12000, score=-9.63119624\n",
      "iter=13000, score=-9.63119608*\n",
      "iter=14000, score=-9.63119585\n",
      "iter=15000, score=-9.63119584\n",
      "iter=16000, score=-9.63119582\n",
      "iter=17000, score=-9.63119581\n",
      "total steps=19899\n"
     ]
    }
   ],
   "source": [
    "rbm = SequentialBernoulliRBM(\n",
    "    num_output, num_input,\n",
    "    n_iter=1,\n",
    "    batch_size=1.0\n",
    ")\n",
    "num_iters = 0\n",
    "score0 = rbm.score(X_train)['x_score']\n",
    "print(\"iter=%2d, score=%10.8f\" % (num_iters, score0))\n",
    "x0 = rbm.copy_parameters()\n",
    "g0 = rbm._compute_score_gradients(X_train)\n",
    "rho0 = rbm.step_size\n",
    "num_iters += 1\n",
    "# Perform simple gradient ascent\n",
    "x1 = add(x0, mult(rho0, g0))\n",
    "num_steps = 1\n",
    "rbm._set_parameters(*x1)\n",
    "score1 = rbm.score(X_train)['x_score']\n",
    "print(\"iter=%2d, score=%10.8f\" % (num_iters, score1))\n",
    "g1 = rbm._compute_score_gradients(X_train)\n",
    "while True:\n",
    "    s0 = diff(x1, x0)\n",
    "    y0 = diff(g1, g0)\n",
    "    if max_abs(y0) < 1e-8: # Any smaller leads to instability\n",
    "        # Gradient has barely changed\n",
    "        #   - stop to prevent division by zero\n",
    "        break\n",
    "    # Compute quasi-Newton gradient direction\n",
    "    d = grad_accel(s0, y0, g1)\n",
    "    x0 = x1\n",
    "    g0 = g1\n",
    "    score0 = score1\n",
    "    ls = False\n",
    "    while True:\n",
    "        # Perform optional line-search until objective score improves\n",
    "        x1 = add(x0, d)\n",
    "        num_steps += 1\n",
    "        rbm._set_parameters(*x1)\n",
    "        score1 = rbm.score(X_train)['x_score']\n",
    "        if score1 < score0:\n",
    "            ls = True\n",
    "            d = mult(0.5, d)\n",
    "        else:\n",
    "            break\n",
    "    g1 = rbm._compute_score_gradients(X_train)\n",
    "    num_iters += 1\n",
    "    if num_iters % 1000 == 0:\n",
    "        print(\"iter=%2d, score=%10.8f%s\" % (num_iters, score1, \"*\" if ls else \"\"))\n",
    "print(\"total steps=%d\" % num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1dbabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_train = np.mean(X_train[:, num_input // 2 :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d2f0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_train[:, 0 : num_input // 2]\n",
    "X_pred = rbm.reconstruct(X_test)\n",
    "P_test = np.mean(X_pred[:, num_input // 2 :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79e24af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5283966 , 0.50742833, 0.55501361, 0.51741974, 0.50329951,\n",
       "        0.50436403, 0.50900354, 0.53485183, 0.52806694, 0.49459783,\n",
       "        0.52608767, 0.52342192, 0.55378466, 0.39684074],\n",
       "       [0.5283966 , 0.55189003, 0.5088311 , 0.52766827, 0.51447007,\n",
       "        0.5126051 , 0.56410039, 0.56183184, 0.50001194, 0.46740981,\n",
       "        0.57054727, 0.52816222, 0.49202629, 0.54494837],\n",
       "       [0.5283966 , 0.50742833, 0.55501361, 0.51741974, 0.49861138,\n",
       "        0.54501982, 0.59475269, 0.54632544, 0.47018422, 0.61173605,\n",
       "        0.53557064, 0.51979243, 0.50485836, 0.59886131],\n",
       "       [0.5283966 , 0.55189003, 0.5088311 , 0.48065853, 0.55377796,\n",
       "        0.59445282, 0.54725008, 0.47239098, 0.58968305, 0.58063962,\n",
       "        0.48000649, 0.56621592, 0.55099259, 0.40690981],\n",
       "       [0.5283966 , 0.50742833, 0.50817973, 0.55695585, 0.60267585,\n",
       "        0.54457053, 0.50357121, 0.58492747, 0.55424213, 0.50317137,\n",
       "        0.53451907, 0.54383604, 0.53780564, 0.44658077],\n",
       "       [0.5283966 , 0.50742833, 0.55501361, 0.5888173 , 0.54397941,\n",
       "        0.54075672, 0.5589574 , 0.55880402, 0.47967032, 0.49632719,\n",
       "        0.56421835, 0.50226589, 0.54394057, 0.47425503],\n",
       "       [0.5283966 , 0.55189003, 0.54523018, 0.57210774, 0.56113093,\n",
       "        0.57170413, 0.50544937, 0.53077805, 0.52942319, 0.56349926,\n",
       "        0.50696   , 0.52948121, 0.55045766, 0.4327859 ],\n",
       "       [0.5283966 , 0.55189003, 0.54523018, 0.57210774, 0.56113093,\n",
       "        0.49705579, 0.50355448, 0.55842292, 0.5351435 , 0.56303788,\n",
       "        0.5296416 , 0.53882337, 0.54033123, 0.45344299],\n",
       "       [0.5283966 , 0.55189003, 0.54523018, 0.57210774, 0.50676233,\n",
       "        0.54133541, 0.51659312, 0.54621699, 0.52213493, 0.55913089,\n",
       "        0.52357762, 0.49930163, 0.51640952, 0.49879749],\n",
       "       [0.5283966 , 0.55189003, 0.54523018, 0.50931955, 0.5147814 ,\n",
       "        0.53080207, 0.54039433, 0.50895518, 0.53318054, 0.55723915,\n",
       "        0.49907993, 0.55700619, 0.54298058, 0.42263183]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8136033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28571429, 0.28571429, 0.42857143, 0.42857143, 0.57142857,\n",
       "       0.42857143, 0.42857143, 0.42857143, 0.57142857, 0.42857143])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c8bb49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03bb2423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50823594, 0.52356253, 0.54104692, 0.52097692, 0.5292975 ,\n",
       "       0.51706877, 0.52048361, 0.53126336, 0.52365272, 0.5172962 ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e21e2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = ((P_train <= 0.5) & (P_test <= 0.5)) | ((P_train > 0.5) & (P_test > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ba51735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5660495764041418"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8ce29",
   "metadata": {},
   "source": [
    "We now compare this to the empirical baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e39e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_train0 = np.mean(X_train[:, : num_input // 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3c8907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[571 809]\n",
      " [810 997]]\n",
      "[1380 1807]\n"
     ]
    }
   ],
   "source": [
    "count00 = np.sum((P_train0 <= 0.5) & (P_train <= 0.5))\n",
    "count01 = np.sum((P_train0 <= 0.5) & (P_train > 0.5))\n",
    "count10 = np.sum((P_train0 > 0.5) & (P_train <= 0.5))\n",
    "count11 = np.sum((P_train0 > 0.5) & (P_train > 0.5))\n",
    "A = np.array([[count00, count01], [count10, count11]])\n",
    "print(A)\n",
    "print(np.sum(A, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1388c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58623188 0.55174322]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([\n",
    "    count01 / (count00 + count01),\n",
    "    count11 / (count10 + count11)\n",
    "])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b1eb77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666771258236587"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(P_train0 <= 0.5) * P[0] + np.mean(P_train0 > 0.5) * P[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ffe057a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43332287, 0.56667713])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(A, axis=0) / np.sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfe161",
   "metadata": {},
   "source": [
    "Unfortunately, the predictive model appears to be less accurate than the baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2159d2",
   "metadata": {},
   "source": [
    "### Sequence clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af06a59",
   "metadata": {},
   "source": [
    "Another possible use for the sequential model is clustering. What patterns can we discover amongst the various fortnighly sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a8836",
   "metadata": {},
   "source": [
    "Firstly, we shall examine the various scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e71f17cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: xy_score=-11.650081, y_score=-1.945817, x_score=-9.704264, rmse=1.870856, mae=7.168183\n",
      "Iteration 100: xy_score=-11.627625, y_score=-1.945837, x_score=-9.681788, rmse=1.867852, mae=6.605271\n",
      "Iteration 200: xy_score=-11.627635, y_score=-1.945849, x_score=-9.681786, rmse=1.867851, mae=6.605271\n",
      "Iteration 300: xy_score=-11.627630, y_score=-1.945846, x_score=-9.681784, rmse=1.867851, mae=6.605271\n",
      "Iteration 400: xy_score=-11.627607, y_score=-1.945827, x_score=-9.681780, rmse=1.867851, mae=6.605271\n",
      "Iteration 500: xy_score=-11.627562, y_score=-1.945788, x_score=-9.681774, rmse=1.867850, mae=6.605271\n"
     ]
    }
   ],
   "source": [
    "rbm = SequentialBernoulliRBM(\n",
    "    num_output, num_input,\n",
    "    n_iter=500, n_report=100,\n",
    "    batch_size=1.0,\n",
    "    is_RBC=True\n",
    ")\n",
    "rbm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33acb9b",
   "metadata": {},
   "source": [
    "Although the implementation maximises $p({\\bf x})$, it appears that \n",
    "$p({\\bf y}\\mid{\\bf x})$ might also be maximised (after a bit of struggling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4edc6a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: xy_score=-11.627562, y_score=-1.945788, x_score=-9.681774, rmse=1.867850, mae=6.605271\n",
      "Iteration 1000: xy_score=-11.595553, y_score=-1.917486, x_score=-9.678066, rmse=1.867355, mae=6.605271\n",
      "Iteration 2000: xy_score=-11.334338, y_score=-1.671932, x_score=-9.662407, rmse=1.865266, mae=6.567305\n",
      "Iteration 3000: xy_score=-11.221774, y_score=-1.565064, x_score=-9.656711, rmse=1.864511, mae=6.555067\n",
      "Iteration 4000: xy_score=-11.172699, y_score=-1.519828, x_score=-9.652871, rmse=1.864005, mae=6.536241\n",
      "Iteration 5000: xy_score=-11.131369, y_score=-1.484020, x_score=-9.647348, rmse=1.863275, mae=6.532789\n",
      "Iteration 6000: xy_score=-11.066540, y_score=-1.423568, x_score=-9.642971, rmse=1.862700, mae=6.514277\n",
      "Iteration 7000: xy_score=-11.038727, y_score=-1.396842, x_score=-9.641886, rmse=1.862560, mae=6.516159\n",
      "Iteration 8000: xy_score=-11.025159, y_score=-1.383549, x_score=-9.641611, rmse=1.862526, mae=6.534672\n",
      "Iteration 9000: xy_score=-11.013104, y_score=-1.371242, x_score=-9.641862, rmse=1.862561, mae=6.538751\n",
      "Iteration 10000: xy_score=-10.998553, y_score=-1.356064, x_score=-9.642490, rmse=1.862645, mae=6.534672\n"
     ]
    }
   ],
   "source": [
    "rbm.n_iter=10000\n",
    "rbm.n_report=1000\n",
    "rbm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9d315",
   "metadata": {},
   "source": [
    "Hmm, the `x_score` has started decreasing again! Also note that other experiments (not shown here) seem to indicate that gradient acceleration doesn't work well for the sequence classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "035d566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds = rbm.predict(X_train)\n",
    "Y_clust = np.argmax(Y_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3fcad292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02708047, 0.09811742, 0.04298212, 0.00420563, 0.10739373,\n",
       "        0.43580077, 0.28441986],\n",
       "       [0.17865666, 0.05016133, 0.08979948, 0.25316403, 0.24717617,\n",
       "        0.02877312, 0.1522692 ],\n",
       "       [0.01104685, 0.04642428, 0.44590375, 0.00452311, 0.01943421,\n",
       "        0.26370667, 0.20896113],\n",
       "       [0.02737035, 0.00864949, 0.00568331, 0.70144692, 0.05232983,\n",
       "        0.04244447, 0.16207563],\n",
       "       [0.02071594, 0.1898094 , 0.29451291, 0.00221642, 0.25731883,\n",
       "        0.16510055, 0.07032596],\n",
       "       [0.5678663 , 0.00318556, 0.05368146, 0.13288579, 0.00913285,\n",
       "        0.03663585, 0.19661219],\n",
       "       [0.0031616 , 0.10050457, 0.07944795, 0.00289434, 0.19359126,\n",
       "        0.42172774, 0.19867254],\n",
       "       [0.46236998, 0.11921742, 0.06784628, 0.2345592 , 0.01995492,\n",
       "        0.05135421, 0.04469799],\n",
       "       [0.00187161, 0.16281702, 0.26803496, 0.02945881, 0.06190926,\n",
       "        0.06557174, 0.41033659],\n",
       "       [0.03186542, 0.0010867 , 0.03254956, 0.14396242, 0.07294062,\n",
       "        0.58329149, 0.13430379]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_preds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d4fdb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 2, 3, 2, 0, 5, 0, 6, 5])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clust[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ff56743f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7c793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
