{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813c8914",
   "metadata": {},
   "source": [
    "# Appendix D: Modelling Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce153bf",
   "metadata": {},
   "source": [
    "The purpose of this appendix is to discuss some of the stochastic distributions that might be useful for modelling\n",
    "the outcomes of sporting matches, particularly Australian Rules football."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074252b",
   "metadata": {},
   "source": [
    "## Beta-Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad9e55",
   "metadata": {},
   "source": [
    "Consider the situation where the outcome $X$ (i.e. a win or loss) of a match between team A and team B is stochastically decided,\n",
    "given some (unknown) probability $\\theta$ that team A wins. Essentially, the outcome may be considered as a biased coin toss.\n",
    "However, prior to the match we might consider the relative strengths of teams A and B, and this knowledge might inform us\n",
    "as to likely values of the probability $\\theta$. Consequently, we may model $X$ by a continuous mixture distribution over \n",
    "$\\theta$, and then use this to derive a posterior distribution for $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9b571",
   "metadata": {},
   "source": [
    "### Bernoulli data likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98ce62",
   "metadata": {},
   "source": [
    "Let the variate $X\\in\\{0,1\\}$ represent the outcome of a single match between some arbitrary team A and team B, with $X=1$ indicating a win by team A, and $X=0$ indicating a win by team B.\n",
    "For convenience, we consider the Bernoulli distribution\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\theta & ~\\sim~ & \\texttt{Bern}(\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\theta$ is the assumed probability that team A wins, which is decided before the match commences.\n",
    "Thus, we see that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & ~=~ & \\theta^x\\,(1-\\theta)^{1-x}\\,.\n",
    "\\end{eqnarray}\n",
    "If we further wish to model the outcomes of an ordered sequence of $n$ *conditionally independent* matches between the same teams A and B, \n",
    "with the same parameter\n",
    "$\\theta$, then we will obtain the joint data likelihood\n",
    "\\begin{eqnarray}\n",
    "p(X=w\\mid n,\\theta) & ~=~ & \\theta^w\\,(1-\\theta)^{n-w}\\,,\n",
    "\\end{eqnarray}\n",
    "where $w\\in\\{0,1,\\ldots,n\\}$ represents the total number of wins for team A, and $\\ell=n-w$ represents the total number of\n",
    "losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119e3df",
   "metadata": {},
   "source": [
    "Note that the use of the Bernoulli distribution explicitly presumes that only wins and losses are possible.\n",
    "How then might we deal with games for which matches may end in a draw?\n",
    "It turns out that there are a number of good reasons to treat a draw as being half-a-win and half-a-loss for each team.\n",
    "Thus, we might define the weighted log-likelihood for a draw as being given by\n",
    "\\begin{eqnarray}\n",
    "L_\\texttt{draw}(\\theta) & ~\\doteq~ & \\frac{1}{2}\\ln p(X=1\\mid\\theta)+\\frac{1}{2}\\ln p(X=0\\mid\\theta)\n",
    "\\\\& =  &\n",
    "\\frac{1}{2}\\ln\\theta+\\frac{1}{2}\\ln\\,(1-\\theta)\n",
    "~=~\n",
    "\\ln\\left[\\theta^{\\frac{1}{2}}\\,(1-\\theta)^{\\frac{1}{2}}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "We observe that functionally this takes the same form as\n",
    "\\begin{eqnarray}\n",
    "L_\\texttt{draw}(\\theta) & ~=~ & \\ln p(X=0.5\\mid\\theta)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we may numerically treat observed draws as having the value $X=\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37488b8d",
   "metadata": {},
   "source": [
    "### Beta prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53eec6",
   "metadata": {},
   "source": [
    "We seek a non-informative prior distribution for $\\theta$. \n",
    "Following Box and Tiao [[1]](#Citations \"Citation [1]: Bayesian Inference in Statistical Analysis\"), \n",
    "we desire a transformation $\\phi(\\theta)$ such that the likelihood $p(X=w\\mid n,\\theta)$, plotted as a function of $\\phi$, remains\n",
    "approximately invariant in shape and size for some fixed $n$ as $w$ varies (but varies in its mean location). \n",
    "A uniform prior for $\\phi$ then induces a\n",
    "non-informative prior for $\\theta$. In general, it turns out that the relevant prior is usually inversely\n",
    "proportional to the square-root of the variance. For the Bernoulli distribution, we have\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}[X\\mid\\theta] & ~=~ \\theta\\,(1-\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "whereupon the non-informative prior is therefore\n",
    "\\begin{eqnarray}\n",
    "p(\\theta) ~\\propto~ \\frac{1}{\\sqrt{\\theta\\,(1-\\theta)}}\n",
    "& ~~~\\Rightarrow~~~ & \\theta~\\sim~\\mathtt{Beta}\\left(\\frac{1}{2},\\frac{1}{2}\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that this is just \n",
    "[Jeffreys' prior](https://en.wikipedia.org/wiki/Jeffreys_prior \"Wikipeida: Jeffreys' prior\"), \n",
    "which notionally corresponds to a single prior pseudo-match\n",
    "with half-a-win and half-a-loss, i.e. a draw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95e78e",
   "metadata": {},
   "source": [
    "More generally, we might choose an arbitrary beta prior distribution\n",
    "\\begin{eqnarray}\n",
    "\\theta & ~\\sim~ & \\mathtt{Beta}(\\alpha,\\beta)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for some hyper-parameters $\\alpha$ and $\\beta$, where\n",
    "$\\alpha$ represents the prior number of pseudo-wins and $\\beta$ represents the prior number of pseudo-losses.\n",
    "Note that technically the parameter dependence is explicitly $\\theta\\mid\\alpha,\\beta$. However, by convention\n",
    "the hyper-parameters are often left implicit to indicate that they are held constant. Despite this useful convention,\n",
    "it is best to always explicitly condition on all relevant parameters when dealing with the actual probability functions,\n",
    "in order to avoid accidental ambiguity, e.g. by hiding important modelling assumptions.\n",
    "\n",
    "The special case of $\\alpha=\\beta$ corresponds to having no other prior information with which to\n",
    "preference a win/loss over a loss/win.\n",
    "We could, for example, choose the \n",
    "[Haldane prior](https://en.wikipedia.org/wiki/Beta_distribution \"Wikipedia: Beta distribution\") \n",
    "with $\\alpha=\\beta=0$ to indicate that we have no\n",
    "prior pseudo-matches.\n",
    "Similarly, the uniform prior with $\\alpha=\\beta=1$ corresponds to one previous pseudo-win and one pseudo-loss.\n",
    "Alternatively, \n",
    "Kerman [[2]](#Citations \"Citation [2]: Neutral noninformative and informative conjugate beta and gamma prior distributions\") \n",
    "derives $\\alpha=\\beta=\\frac{1}{3}$ as being a \"*neutral*\" prior that has the property that it leads \"to posterior distributions with approximately 50 per cent probability that the true value is either smaller or larger than the maximum likelihood estimate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334727d5",
   "metadata": {},
   "source": [
    "Note that the beta distribution is also a good choice due to its complementary symmetry in $\\alpha$ and $\\beta$.\n",
    "Suppose that $\\alpha$ denotes our pre-match knowledge about the strength of team A, and similarly let\n",
    "$\\beta$ represent our prior knowlegde about team B. Now, $\\theta=\\theta_A$ is the prior probability of team A winning,\n",
    "such that $\\theta_A\\sim\\texttt{Beta}(\\alpha,\\beta)$.\n",
    "However, the ordering of teams A and B is arbitrary. If we swapped the order of the teams and their respective hyper-parameters, \n",
    "then we would instead have $\\theta_B\\sim\\texttt{Beta}(\\beta,\\alpha)$.\n",
    "The beta distribution correctly satisfies the constraint that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\theta\\mid\\alpha,\\beta] & ~=~ \\frac{\\alpha}{\\alpha+\\beta}~=~1-\\mathbb{E}[\\theta\\mid\\beta,\\alpha]\\,. \n",
    "\\end{eqnarray}\n",
    "This is the aforementioned complementary symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e97624",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli mixture distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6694fcc",
   "metadata": {},
   "source": [
    "The mixture distribution for a single match is now derived as\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\int_0^1 p(x\\mid\\theta)\\,p(\\theta\\mid\\alpha,\\beta)\\,d\\theta\n",
    "\\\\& = &\n",
    "\\int_0^1 \\theta^x(1-\\theta)^{1-x}\\,\n",
    "\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\,d\\theta\n",
    "\\\\& = &\n",
    "\\frac{1}{B(\\alpha,\\beta)}\\int_0^1 \\theta^{\\alpha+x-1}(1-\\theta)^{\\beta-x}\\,d\\theta\n",
    "\\\\& =  &\n",
    "\\frac{B(\\alpha+x,\\beta-x+1)}{B(\\alpha,\\beta)}\\int_0^1 \n",
    "\\frac{\\theta^{\\alpha+x-1}(1-\\theta)^{\\beta-x}}{B(\\alpha+x,\\beta-x+1)}\\,d\\theta\\,.\n",
    "\\end{eqnarray}\n",
    "Recognising the final integrand as the $\\texttt{Beta}(\\alpha+x,\\beta-x+1)$ distribution, we obtain\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ &\n",
    "\\frac{B(\\alpha+x,\\beta-x+1)}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we expand $B(\\cdot,\\cdot)$ in terms of the *gamma* function $\\Gamma(\\cdot)$ to obtain\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ &\n",
    "\\frac{\\Gamma(\\alpha+x)\\,\\Gamma(\\beta+1-x)}{\\Gamma(\\alpha+\\beta+1)}\\,\n",
    "\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, using the recurrence relation that $\\Gamma(z+1)=z\\,\\Gamma(z)$, we deduce that\n",
    "\\begin{eqnarray}\n",
    "p(X=1\\mid\\alpha,\\beta)~=~\\frac{\\alpha}{\\alpha+\\beta}\\,, & \\;\\;\\;\\; &\n",
    "p(X=0\\mid\\alpha,\\beta)~=~\\frac{\\beta}{\\alpha+\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{\\alpha^x\\,\\beta^{1-x}}{\\alpha+\\beta}\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that this is just\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\bar{\\theta} & ~\\sim~ & \\texttt{Bern}(\\bar{\\theta})\\,,\n",
    "\\end{eqnarray}\n",
    "with $\\bar{\\theta}=\\frac{\\alpha}{\\alpha+\\beta}$ being the mean of the beta distribution.\n",
    "\n",
    "Subsequently, for the case of $n$ *unconditionally independent* matches between teams A and B, we would obtain the joint distribution\n",
    "\\begin{eqnarray}\n",
    "p_u(X=w\\mid n,\\alpha,\\beta) & ~=~ & \\frac{\\alpha^w\\,\\beta^{n-w}}{(\\alpha+\\beta)^n}~=~\\bar{\\theta}^w\\,(1-\\bar{\\theta})^{n-w}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this is notionally different from the *conditionally independent* case assumed\n",
    "[previously](#Bernoulli-data-likelihood \"Section: Bernoulli data likelihood\"),\n",
    "where the $n$ matches all shared the same value of the parameter $\\theta$. In the unconditional form, the parameter\n",
    "$\\theta$ is assumed to be resampled before every match.\n",
    "\n",
    "Consequently, for the conditionally independent case, we would instead derive that\n",
    "\\begin{eqnarray}\n",
    "p_c(X=w\\mid n,\\alpha,\\beta) & ~=~ & \\int_0^1 p(X=w\\mid n,\\theta)\\,p(\\theta\\mid\\alpha,\\beta)\\,d\\theta\n",
    "\\\\& = &\n",
    "\\int_0^1 \\theta^w\\,(1-\\theta)^{n-w}\\,\n",
    "\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\,d\\theta\n",
    "\\\\& = &\n",
    "\\frac{1}{B(\\alpha,\\beta)}\\int_0^1 \\theta^{\\alpha+w-1}(1-\\theta)^{\\beta+n-w-1}\\,d\\theta\n",
    "\\\\& =  &\n",
    "\\frac{B(\\alpha+w,\\beta+n-w)}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529307b",
   "metadata": {},
   "source": [
    "### Beta posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070eba8",
   "metadata": {},
   "source": [
    "For the case of observing a sequence of $n$ conditionally independent matches, the posterior distribution is given by\n",
    "\\begin{eqnarray}\n",
    "p(\\theta\\mid n,w,\\alpha,\\beta) & ~=~ &\n",
    "\\frac{p(X=w\\mid n,\\theta)\\,p(\\theta\\mid\\alpha,\\beta)}{p_c(X=w\\mid n,\\alpha,\\beta)}\n",
    "~=~\n",
    "\\frac{\\theta^{\\alpha+w-1}(1-\\theta)^{\\beta+n-w-1}}{B(\\alpha+w,\\beta+n-w)}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\theta\\mid n,w & ~\\sim~ & \\texttt{Beta}(\\alpha+w,\\beta+n-w)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691c541",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66d9b1",
   "metadata": {},
   "source": [
    "Suppose the result of the $(n+1)$-th match is now $X$, conditionally independent of previous matches for the same\n",
    "parameter $\\theta$. Then the likelihood of this result is\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & ~=~ & \\theta^{x}\\,(1-\\theta)^{1-x}\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the predictive probability of this result is given by\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \\int_0^1 p(x\\mid\\theta)\\,p(\\theta\\mid n,w,\\alpha,\\beta)\\,d\\theta\n",
    "\\\\&~=~&\n",
    "\\int_0^1\\frac{\\theta^{\\alpha+w+x-1}\\,(1-\\theta)^{\\beta+n-w-x}}\n",
    "{B(\\alpha+w,\\beta+n-w)}\\,d\\theta\n",
    "\\\\ & = &\n",
    "\\frac{B(\\alpha+w+x,\\beta+n-w+1-x)}\n",
    "{B(\\alpha+w,\\beta+n-w)}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9d1c4",
   "metadata": {},
   "source": [
    "In terms of the gamma function, $\\Gamma(\\cdot)$, this becomes\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \n",
    "\\frac{\\Gamma(\\alpha+w+x)\\,\\Gamma(\\beta+n-w+1-x)}\n",
    "     {\\Gamma(\\alpha+\\beta+n+1)}\\,\n",
    "\\frac{\\Gamma(\\alpha+\\beta+n)}\n",
    "     {\\Gamma(\\alpha+w)\\,\\Gamma(\\beta+n-w)}\\,.\n",
    "\\end{eqnarray}\n",
    "For a loss, i.e. $X=0$, the respective probability reduces to\n",
    "\\begin{eqnarray}\n",
    "p(X=0\\mid n,w,\\alpha,\\beta) & ~=~ & \\frac{\\beta+n-w}{\\alpha+\\beta+n}\\,,\n",
    "\\end{eqnarray}\n",
    "again using the recurrence relation that $\\Gamma(z+1)=z\\,\\Gamma(z)$.\n",
    "The corresponding probability of a win is therefore\n",
    "\\begin{eqnarray}\n",
    "p(X=1\\mid n,w,\\alpha,\\beta) & ~=~ & \\frac{\\alpha+w}{\\alpha+\\beta+n}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \\frac{(\\alpha+w)^x\\,(\\beta+n-w)^{1-x}}{\\alpha+\\beta+n}\\,.\n",
    "\\end{eqnarray}\n",
    "Defining $\\hat{\\theta}\\doteq\\frac{\\alpha+w}{\\alpha+\\beta+n}$, we observe that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,w,\\alpha,\\beta) & ~=~ & \\left(\\hat{\\theta}\\right)^x\\,\\left(1-\\hat{\\theta}\\right)^{1-x}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\hat{\\theta} & ~\\sim~ & \\texttt{Bern}(\\hat{\\theta})\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb62c5",
   "metadata": {},
   "source": [
    "Note that the denominator of $\\hat{\\theta}$ corresponds to assuming $n$ observed matches plus $\\alpha+\\beta$ prior pseudo-matches, and the numerator corresponds to $w$ observed wins plus $\\alpha$ prior pseudo-wins.\n",
    "Thus, $\\hat{\\theta}$ is just a *smoothed* estimate of the empirical probability of a win."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5648033",
   "metadata": {},
   "source": [
    "## Gamma-Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d56fc1",
   "metadata": {},
   "source": [
    "Consider a Poisson-like count $X\\in\\mathbb{Z}^{\\ge 0}$ with over-dispersion, namely \n",
    "$\\mathbb{V}[X]>\\mathbb{E}[X]$. This suggests the need for an additonal parameter, beyond the usual Poisson rate $\\lambda$, \n",
    "to control the extra variance. \n",
    "One way in which over-dispersion can arise is when $\\lambda$ is only held constant for a single trial \n",
    "or a single sequence of trials, but exhibits inter-trial or inter-sequence variation.\n",
    "This variation may be captured by a prior distribution, giving rise to a continuous mixture distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7598d",
   "metadata": {},
   "source": [
    "### Poisson data likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6875901",
   "metadata": {},
   "source": [
    "For some event rate $\\lambda$ (per unit time), let the count $X$ be distributed as\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\lambda & ~\\sim~ & \\mathtt{Poisson}(\\lambda)\\,,\n",
    "\\end{eqnarray}\n",
    "with mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X\\mid\\lambda]~=~\\lambda\\,, & \\;\\;\\mbox{and}\\;\\; &\n",
    "\\mathbb{V}[X\\mid\\lambda]~=~\\lambda\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Then we may consider a \n",
    "single sequence of $n$ conditionally independent counts with $\\lambda$ held constant just for that sequence.\n",
    "If the average count of the sequence is $\\langle X\\rangle=\\bar{X}$, then the joint likelihood is given by\n",
    "\\begin{eqnarray}\n",
    "p(X_1,\\ldots,X_n\\mid\\lambda) & ~=~ & \\frac{e^{-n\\lambda}\\,\\lambda^{n\\bar{X}}}{\\prod_{i=1}^n X_i!}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8baa118",
   "metadata": {},
   "source": [
    "### Gamma prior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31bfa64",
   "metadata": {},
   "source": [
    "Consider, in general, some arbitrary prior distribution for the event rate $\\lambda$, say $\\lambda\\sim D(\\boldsymbol{\\theta})$, governed by one or more \n",
    "(constant) hyper-parameters denoted by $\\boldsymbol{\\theta}$.\n",
    "Let this distribution have mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_\\lambda ~\\doteq~\\mathbb{E}[\\lambda\\mid\\boldsymbol{\\theta}]\\,, \n",
    "& ~~~ & \\sigma^2_\\lambda~\\doteq~\\mathbb{V}[\\lambda\\mid\\boldsymbol{\\theta}]\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "Then it follows that the unconditional mean and variance\n",
    "of the variate $X$ are given\n",
    "[by](https://en.wikipedia.org/wiki/Law_of_total_expectation\n",
    "\"Wikipedia: Law of total expectation\")\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X] & ~\\doteq~ & \\mathbb{E}\\left[\\,\\mathbb{E}[X\\mid\\lambda]\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}[\\lambda\\mid\\boldsymbol{\\theta}]~=~\\mu_\\lambda\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "[and](https://en.wikipedia.org/wiki/Law_of_total_variance\n",
    "\"Wikipedia: Law of total variance\")\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}[X] & ~\\doteq~ & \n",
    "\\mathbb{V}\\left[\\,\\mathbb{E}[X\\mid\\lambda]\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\n",
    "\\mathbb{E}\\left[\\,\\mathbb{V}[X\\mid\\lambda]\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\\\\n",
    "& ~=~ & \n",
    "\\mathbb{V}[\\lambda\\mid\\boldsymbol{\\theta}]\n",
    "+\n",
    "\\mathbb{E}[\\lambda\\mid\\boldsymbol{\\theta}]\n",
    "~=~\\sigma^2_\\lambda+\\mu_\\lambda\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we observe that $\\mathbb{V}[X]>\\mathbb{E}[X]$, such that the overall process is over-dispersed compared\n",
    "to a simple Poisson process.\n",
    "Clearly we require a *proper* prior distribution with finite mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a09d0",
   "metadata": {},
   "source": [
    "Following the rationale discussed [previously](#Beta-prior-distribution \"Section: Beta prior distribution\"),\n",
    "we first seek a non-informative prior for the Possion rate $\\lambda\\in (0,\\infty)$ via a variance-stabilising transformation. However, this results in the improper prior\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda) & ~\\propto~ & \\frac{1}{\\sqrt{\\mathbb{V}[X\\mid\\lambda]}}~=~\\frac{1}{\\sqrt{\\lambda}}\\,.\n",
    "\\end{eqnarray}\n",
    "If we truncate the domain to $\\lambda\\in(0,L]$, then the truncated prior becomes proper, namely\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda\\mid L) & ~=~ & \\frac{1}{2\\sqrt{L\\lambda}}\\,,\n",
    "\\end{eqnarray}\n",
    "with mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\lambda\\mid L]~=~\\frac{1}{3}L\\,, & \\;\\;\\mbox{and}\\;\\; &\n",
    "\\mathbb{V}[\\lambda\\mid L]~=~\\frac{4}{45}L^2\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. The overall mean and variance are then given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X]~=~\\frac{1}{3}L\\,, & \\;\\;\\mbox{and}\\;\\; &\n",
    "\\mathbb{V}[X]~=~\\frac{1}{3}L+\\frac{4}{45}L^2\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Clearly, the hyper-parameter $L$ may be estimated (using the method of moments) from the sample mean as $\\hat{L}=3\\bar{X}$.\n",
    "However, if the sample variance does not match $\\frac{1}{3}\\hat{L}+\\frac{4}{45}\\hat{L}^2$, then \n",
    "having only a single prior hyper-parameter does not provide enough modelling flexibility.\n",
    "Consequently, we require a prior distribution having at least two hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c7178",
   "metadata": {},
   "source": [
    "To deduce a suitable form for such a prior, [recall](#Poisson-data-likelihood \"Sectrion: Poisson data likelihood\")\n",
    "that the joint likelihood of a sequence of $n$ trials, considered as a function of $\\lambda$ with fixed\n",
    "observed counts, is proportional to $\\lambda^{n\\bar{X}}\\,e^{-n\\lambda}$. This has the same proportional form as\n",
    "the $\\texttt{Gamma}(n\\bar{X}+1,n)$ distribution.\n",
    "Consequently, we consider a prior distribution in the general form\n",
    "\\begin{eqnarray}\n",
    "\\lambda & ~\\sim~ & \\mathtt{Gamma}(\\alpha,\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\alpha$ is the shape parameter and $\\beta$ is the rate parameter.\n",
    "Note that the improper Jeffreys' prior above could notionally be thought of as $\\texttt{Gamma}(\\frac{1}{2},0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a4c90",
   "metadata": {},
   "source": [
    "For this gamma distribution, it is \n",
    "[known](https://en.wikipedia.org/wiki/Gamma_distribution\n",
    "\"Wikipedia: Gamma distribution\")\n",
    "that the mean and variance are given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_\\lambda~=~\\frac{\\alpha}{\\beta}\\,, \n",
    "& ~~~ & \\sigma^2_\\lambda~=~\\frac{\\alpha}{\\beta^2}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "It then follows that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[X] & ~=~ & \\mu_\\lambda~=~\\frac{\\alpha}{\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}[X] & ~=~ & \\mu_\\lambda+\\sigma^2_\\lambda\n",
    "~=~\\frac{\\alpha\\,(\\beta+1)}{\\beta^2}\\,.\n",
    "\\end{eqnarray}\n",
    "Not only does this allow for over-dispersion for $\\alpha>0$, but the two hyper-parameters $\\alpha$ and $\\beta$\n",
    "provide sufficient freedom to flexibly model both the sample mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f69cf",
   "metadata": {},
   "source": [
    "### Negative binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43524b7",
   "metadata": {},
   "source": [
    "For a single trial, the explicit form of the mixture distribution is derived as\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid\\alpha,\\beta) & ~=~ & \\int_0^\\infty p(X=k\\mid\\lambda)\\,p(\\lambda\\mid\\alpha,\\beta)\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\int_0^\\infty e^{-\\lambda}\\frac{\\lambda^k}{k!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\lambda^{\\alpha-1}\\,e^{-\\beta\\lambda}\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\frac{\\beta^\\alpha}{k!\\,\\Gamma(\\alpha)}\n",
    "\\int_0^\\infty \\lambda^{\\alpha+k-1}\\,e^{-(\\beta+1)\\lambda}\\,d\\lambda\n",
    "%\\\\& = &\n",
    "%\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,\\frac{\\beta^\\alpha}{(\\beta+1)^{\\alpha+k}}\n",
    "%\\int_0^\\infty \\frac{(\\beta+1)^{\\alpha+k}}{\\Gamma(\\alpha+k)}\\,\\lambda^{\\alpha+k-1}\\,e^{-(\\beta+1)\\lambda}\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,\\frac{\\beta^\\alpha}{(\\beta+1)^{\\alpha+k}}\\,,\n",
    "\\end{eqnarray}\n",
    "since the final integrand is proportional to the \n",
    "$\\mathtt{Gamma}(\\alpha+k,\\beta+1)$\n",
    "distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95d767",
   "metadata": {},
   "source": [
    "In order to show that this is just the real-valued (*Polya* distribution) version of the \n",
    "[*negative binomial*](https://en.wikipedia.org/wiki/Negative_binomial_distribution \"Wikipedia: Negative binomial distribution\") \n",
    "distribution,\n",
    "we define $p$ to be the probability of a *stopping* event (i.e. one out of the required number $\\alpha$ of such events) and $q$ to be the probability of a *non-stopping* event,\n",
    "namely\n",
    "\\begin{eqnarray}\n",
    "p ~\\doteq~ \\frac{\\beta}{\\beta+1}\\,, & ~~~\\mbox{and}~~~ & q ~\\doteq~ 1-p~=~\\frac{1}{\\beta+1}\\,,\n",
    "\\end{eqnarray}\n",
    "whereupon\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid\\alpha,p) & ~=~ & \n",
    "\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,q^k\\,p^\\alpha\\,.\n",
    "\\end{eqnarray}\n",
    "This represents the overall probability of observing a sequence of $k+\\alpha$ Bernoulli trials,\n",
    "where the sequence was terminated as soon as $\\alpha$ stopping events occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57bec1",
   "metadata": {},
   "source": [
    "In the traditional integer-valued (*Pascal* distribution) form, $\\alpha>0$ is replaced by $r\\in\\mathbb{Z}^{>0}$, giving\n",
    "\\begin{eqnarray}\n",
    "p(X=k\\mid r,p) & ~=~ & \n",
    "\\frac{(r+k-1)!}{k!\\,(r-1)!}\\,q^k\\,p^r~=~\\binom{k+r-1}{k}\\,q^k\\,p^r\\,.\n",
    "\\end{eqnarray}\n",
    "If we terminate the sequence of trials after just $r=1$ stopping events, then this reduces to the\n",
    "*geometric* distribution, i.e. $\\mathtt{Geom}(p)\\equiv \\mathtt{NegBinom}(1,p)$.\n",
    "Furthermore, $X\\sim\\mathtt{NegBinom}(r,p)$ corresponds to the sum of $r$ independent geometric counts, i.e.\n",
    "$X=X_1+X_2+\\ldots+X_r$ with $X_i\\sim\\mathtt{Geom}(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5ec97c",
   "metadata": {},
   "source": [
    "Finally, for a sequence of $n$ conditionally independent counts, the joint mixture distribution is\n",
    "\\begin{eqnarray}\n",
    "p_c(X_1,\\ldots,X_n\\mid\\alpha,\\beta) & ~=~ & \\int_0^\\infty p(X_1,\\ldots,X_n\\mid\\lambda)\\,p(\\lambda\\mid\\alpha,\\beta)\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\int_0^\\infty \\frac{e^{-n\\lambda}\\,\\lambda^{n\\bar{X}}}{\\prod_{i=1}^n X_i!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\lambda^{\\alpha-1}\\,e^{-\\beta\\lambda}\\,d\\lambda\n",
    "\\\\& = &\n",
    "\\frac{\\Gamma(\\alpha+n\\bar{X})}{\\Gamma(\\alpha)\\,\\prod_{i=1}^n X_i!}\\,\\frac{\\beta^\\alpha}{(\\beta+n)^{\\alpha+n\\bar{X}}}\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, for *unconditionally* independent counts sampled with *different* values of $\\lambda$, the joint mixture distribution is just\n",
    "\\begin{eqnarray}\n",
    "p_u(X_1,\\ldots,X_n\\mid\\alpha,\\beta) & ~=~ &\n",
    "\\prod_{i=1}^n\\frac{\\Gamma(\\alpha+X_i)}{X_i!}\\cdot\\frac{q^{n\\bar{X}}\\,p^{n\\alpha}}{\\Gamma(\\alpha)^n}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90566ee5",
   "metadata": {},
   "source": [
    "### Gamma posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef6881",
   "metadata": {},
   "source": [
    "For a sequence of $n$ conditionally independent counts, the posterior distribution for the Poisson rate $\\lambda$ is\n",
    "given by\n",
    "\\begin{eqnarray}\n",
    "p(\\lambda\\mid X_1,\\ldots,X_n,\\alpha,\\beta) & ~=~ &\n",
    "\\frac{p(X_1,\\ldots,X_n\\mid\\lambda)\\,p(\\lambda\\mid\\alpha,\\beta)}{p_c(X_1,\\ldots,X_n\\mid\\alpha,\\beta)}\n",
    "\\\\& = &\n",
    "\\left.\n",
    "\\frac{e^{-n\\lambda}\\,\\lambda^{n\\bar{X}}}{\\prod_{i=1}^n X_i!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\lambda^{\\alpha-1}\\,e^{-\\beta\\lambda}\n",
    "\\right/\n",
    "\\frac{\\Gamma(\\alpha+n\\bar{X})}{\\Gamma(\\alpha)\\,\\prod_{i=1}^n X_i!}\\,\\frac{\\beta^\\alpha}{(\\beta+n)^{\\alpha+n\\bar{X}}}\n",
    "\\\\& = &\n",
    "\\frac{(\\beta+n)^{\\alpha+n\\bar{X}}}{\\Gamma(\\alpha+n\\bar{X})}\\,\\lambda^{\\alpha+n\\bar{X}-1}\\,e^{-(\\beta+n)\\lambda}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\lambda\\mid X_1,\\ldots,X_n & ~\\sim~ & \\texttt{Gamma}(\\alpha+n\\bar{X},\\beta+n)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41043f01",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2b3bc",
   "metadata": {},
   "source": [
    "We now return to the sequence of unconditionally independent observations.\n",
    "The log-likelihood is therefore given by\n",
    "\\begin{eqnarray}\n",
    "L(\\boldsymbol{\\theta};X_1,\\ldots,X_n) & ~\\doteq~ &\n",
    "\\ln p_u(X_1,\\ldots,X_n\\mid\\alpha,\\beta)\n",
    "\\\\& ~=~ &\n",
    "\\sum_{i=1}^n\\left\\{\\ln\\Gamma(\\alpha+X_i)-\\ln X_i!\\right\\}\n",
    "+n\\bar{X}\\ln q+n\\alpha\\ln p-n\\ln\\Gamma(\\alpha)\\,.\n",
    "\\end{eqnarray}\n",
    "Now, since\n",
    "\\begin{eqnarray}\n",
    "p~\\doteq~\\frac{\\beta}{\\beta+1} & ~~~\\Rightarrow~~~ &\n",
    "\\beta~\\doteq~\\frac{p}{q}\\,,\n",
    "\\end{eqnarray}\n",
    "we may consider the parameterisation as either\n",
    "$\\boldsymbol{\\theta}=(\\alpha,\\beta)$ or\n",
    "$\\boldsymbol{\\theta}'=(\\alpha,p)$.\n",
    "Taking the gradient with respect to $p$ then gives\n",
    "\\begin{eqnarray}\n",
    "\\nabla_p L & ~=~ &\n",
    "\\frac{n\\alpha}{p}-\\frac{n\\bar{X}}{1-p}\n",
    "~=~\\frac{n\\alpha-n(\\alpha+\\bar{X})p}{pq}\\,,\n",
    "\\end{eqnarray}\n",
    "such that  the maximum likelihood estimate\n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}'=(\\hat{\\alpha}_\\texttt{ML},\n",
    "\\hat{p}_\\texttt{ML})$ satisfies\n",
    "\\begin{eqnarray}\n",
    "\\hat{p}_\\texttt{ML} & ~=~ & \n",
    "\\frac{\\hat{\\alpha}_\\texttt{ML}}{\\hat{\\alpha}_\\texttt{ML}+\\bar{X}}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa7dcf",
   "metadata": {},
   "source": [
    "Next, we take the gradient with respect to $\\alpha$, which gives\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\alpha L & ~=~ & \n",
    "\\sum_{i=1}^{n}\\psi(\\alpha+X_i)+n\\ln p-n\\psi(\\alpha)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the maximum likelihood estimate satisfies the nonlinear\n",
    "equation\n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\psi(\\hat{\\alpha}_\\texttt{ML}+X_i)\n",
    "-\\psi(\\hat{\\alpha}_\\texttt{ML})\n",
    "+\\ln\\hat{p}_\\texttt{ML} & ~=~ & 0\\,.\n",
    "\\end{eqnarray}\n",
    "Now, we note that $\\hat{p}_\\texttt{ML}=g(\\hat{\\alpha}_\\texttt{ML})$,\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "g(\\alpha) ~=~ \\frac{\\alpha}{\\alpha+\\bar{X}}\n",
    "& ~~~\\Rightarrow~~~ &\n",
    "g'(\\alpha)~=~\\frac{\\bar{X}}{(\\alpha+\\bar{X})^2}\n",
    "~=~\\frac{pq}{\\alpha}\\,,\n",
    "\\end{eqnarray}\n",
    "using $p\\doteq g(\\alpha)$ everywhere.\n",
    "Hence,  we may treat the nonlinear equation as $f(\\hat{\\alpha}_\\texttt{ML})=0$, where\n",
    "$n\\,f(\\alpha)\\doteq\\nabla_\\alpha L(\\alpha,p)$ with $p=g(\\alpha)$.\n",
    "As a result, we may then iteratively solve this equation via \n",
    "Newton's method, namely\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\alpha}' & ~=~ & \\hat{\\alpha}-\\frac{f(\\hat{\\alpha})}{f'(\\hat{\\alpha})}\\,,\n",
    "\\end{eqnarray}\n",
    "using the derivative\n",
    "\\begin{eqnarray}\n",
    "n\\,f'(\\alpha) & ~\\doteq~ & \n",
    "\\nabla_\\alpha^2 L(\\alpha,p)+\n",
    "g'(\\alpha)\\,\\nabla_p\\nabla_\\alpha L(\\alpha,p)\n",
    "\\\\\n",
    "\\Rightarrow f'(\\alpha)\n",
    "& = &\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\psi'(\\alpha+X_i)\n",
    "-\\psi'(\\alpha)\n",
    "+\\frac{q}{\\alpha}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74959a64",
   "metadata": {},
   "source": [
    "Finally, in terms of the original rate parameter $\\beta$, we obtain\n",
    "the maximum likelihood estimate\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\beta}_\\texttt{ML} & ~\\doteq~ &\n",
    "\\frac{\\hat{p}_\\texttt{ML}}{1-\\hat{p}_\\texttt{ML}}\n",
    "~=~\\frac{\\hat{\\alpha}_\\texttt{ML}}{\\bar{X}}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f4da2",
   "metadata": {},
   "source": [
    "### Serial correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c5350",
   "metadata": {},
   "source": [
    "Suppose that some total number $Y$ of events is observed over a given period of time.\n",
    "Further suppose that this time period is now partitioned into $n$ \n",
    "consecutive (but not necessarily equal) intervals, such that\n",
    "\\begin{eqnarray}\n",
    "Y & ~=~ & X_1+X_2+\\cdots+X_n~\\doteq~n\\bar{X}\\,,\n",
    "\\end{eqnarray}\n",
    "where $X_t$ denotes the number of events observed in the $t$th interval.\n",
    "If these intervals are independent, then it follows that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}[Y] & ~=~ & \\mathbb{V}\\left[\\sum_{t=1}^{n}X_t\\right]\n",
    "~=~\\sum_{t=1}^{n}\\mathbb{V}[X_t]\\,.\n",
    "\\end{eqnarray}\n",
    "However, there are situations in which over-dispersion occurs, such that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{V}\\left[\\sum_{t=1}^{n}X_t\\right]\n",
    "& ~=~ &\n",
    "\\sum_{t=1}^{n}\\mathbb{V}[X_t]+2\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\n",
    "\\mathtt{Cov}[X_i,X_j]\n",
    "~>~\\sum_{t=1}^{n}\\mathbb{V}[X_t]\\,,\n",
    "\\end{eqnarray}\n",
    "from which it follows that the intervals are not independent.\n",
    "Typically, we find that $\\mathtt{Cov}[X_i,X_j]>0$ for all\n",
    "$i,j=1,2,\\ldots,n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f153cde",
   "metadata": {},
   "source": [
    "To set the scene, let us first allow for possible differences between intervals\n",
    "by assuming that each interval has some arbitrary (but prespecified) event rate, say $\\lambda_t$ for each $t=1,2,\\ldots,n$.\n",
    "For convenience, let \n",
    "$\\boldsymbol{\\lambda}\\doteq(\\lambda_1,\\lambda_2,\\ldots,\\lambda_n)$\n",
    "collectively represent these underlying rates.\n",
    "Furthermore, let\n",
    "$\\vec{\\mathbf{X}}_t\\doteq(X_1,X_2,\\ldots,X_t)$\n",
    "denote a sequence (or subsequence) of $t$ consecutive counts, where, by convention, $\\vec{\\mathbf{X}}_{0}\\doteq()$ is the empty or null tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71dffdd",
   "metadata": {},
   "source": [
    "There are now several ways to model the correlation between intervals.\n",
    "One way is to consider temporal causality, such that $X_2$ depends upon\n",
    "the observed $X_1$, and $X_3$ depends upon $X_2$, et cetera.\n",
    "This leads either to a Markov chain or, more generally, to a Bayesian network\n",
    "that embodies the sequential nature of $\\vec{\\mathbf{X}}_n$, e.g.\n",
    "\\begin{eqnarray}\n",
    "p(\\vec{\\mathbf{X}}_n\\mid\\boldsymbol{\\lambda}) & ~\\doteq~ &\n",
    "\\prod_{t=1}^{n}p(X_t\\mid\\lambda_t,\\vec{\\mathbf{X}}_{t-1})\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\vec{\\mathbf{X}}_{0}\\doteq()$ indicates no previous dependencies.\n",
    "Unfortunately, this approach leads to increasingly complex conditional distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f88568",
   "metadata": {},
   "source": [
    "Another approach is to consider that the intervals, which are\n",
    "dependent, are in fact *conditionally* independent given\n",
    "some latent or hidden parameter, say $\\gamma$.\n",
    "The corresponding conditional model is then given by\n",
    "\\begin{eqnarray}\n",
    "p(\\vec{\\mathbf{X}}_n\\mid\\boldsymbol{\\lambda},\\gamma) & ~\\doteq~ &\n",
    "\\prod_{t=1}^{n}p(X_t\\mid\\lambda_t,\\gamma)\\,,\n",
    "\\end{eqnarray}\n",
    "where the corresponding generative model first samples $\\gamma$ and then samples $\\vec{\\mathbf{X}}_n$ with fixed $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55c715",
   "metadata": {},
   "source": [
    "We now extend our \n",
    "[earlier](#Poisson-data-likelihood \"Section: Poisson data likelihood\") \n",
    "assumption that each $X_t$ is Poisson distributed to include conditional dependence upon $\\gamma$ (and $\\lambda_t$) via\n",
    "\\begin{eqnarray}\n",
    "p(X_t\\mid\\lambda_t,\\gamma) & ~\\doteq~ &\n",
    "e^{-\\gamma\\lambda_t}\\frac{(\\gamma\\lambda_t)^{X_t}}{X_t!}\\,,\n",
    "\\end{eqnarray}\n",
    "which has conditional mean and variance given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{X_t\\mid\\gamma}~\\doteq~\\mathbb{E}[X_t\\mid\\boldsymbol{\\lambda},\\gamma]~=~\\gamma\\lambda_t\\,, \n",
    "& ~~~ &\n",
    "\\sigma^2_{X_t\\mid\\gamma}\\doteq\\mathbb{V}[X_t\\mid\\boldsymbol{\\lambda},\\gamma]~=~\\gamma\\lambda_t\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "\n",
    "Next, we borrow our \n",
    "[earlier](#Gamma-prior-distribution \"Section: Gamma prior distribution\")\n",
    "assumption of a gamma prior to obtain $\\gamma\\sim\\mathtt{Gamma}(\\alpha,\\beta)$.\n",
    "The marginal distribution for each $X_t$ is thus\n",
    "\\begin{eqnarray}\n",
    "p(X_t\\mid\\lambda_t) & ~=~ &\n",
    "\\int_0^\\infty\n",
    "e^{-\\gamma\\lambda_t}\\frac{(\\gamma\\lambda_t)^{X_t}}{X_t!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\gamma^{\\alpha-1}\\,\n",
    "e^{-\\beta\\gamma}\\,d\\gamma\n",
    "\\\\& = &\n",
    "\\frac{\\lambda_t^{X_t}}{X_t!}\\,\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\n",
    "\\int_0^\\infty\n",
    "\\gamma^{\\alpha+X_t-1}\\,e^{-\\gamma(\\beta+\\lambda_t)}\\,d\\gamma\n",
    "\\\\& = &\n",
    "\\frac{\\lambda_t^{X_t}}{X_t!}\\,\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\n",
    "\\frac{\\Gamma(\\alpha+X_t)}{(\\beta+\\lambda_t)^{\\alpha+X_t}}\n",
    "\\\\& = &\n",
    "\\frac{\\Gamma(\\alpha+X_t)}{X_t!\\,\\Gamma(\\alpha)}\\,\n",
    "\\left(\\frac{\\beta}{\\beta+\\lambda_t}\\right)^\\alpha\\,\n",
    "\\left(\\frac{\\lambda_t}{\\beta+\\lambda_t}\\right)^{X_t}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which is again a \n",
    "[negative binomial](#Negative-binomial-distribution\n",
    "\"Section: Negative binomial distribution\")\n",
    "distribution\n",
    "with a probability $p_t\\doteq\\frac{\\beta}{\\beta+\\lambda_t}$ of a non-countable\n",
    "event and a probability $q_t\\doteq\\frac{\\lambda_t}{\\beta+\\lambda_t}$ of a countable event.\n",
    "The marginal mean number of countable events is then\n",
    "\\begin{eqnarray}\n",
    "\\mu_{X_t} & ~\\doteq~ & \n",
    "\\mathbb{E}[X_t\\mid\\boldsymbol{\\lambda}]~=~\n",
    "\\mathbb{E}[\\mu_{X_t\\mid\\gamma}\\mid\\boldsymbol{\\lambda}]\n",
    "\\\\& = &\n",
    "\\mathbb{E}[\\gamma\\lambda_t\\mid\\boldsymbol{\\lambda}]\n",
    "~=~\\lambda_t\\,\\mu_\\gamma\n",
    "\\\\& = &\n",
    "\\frac{\\alpha\\lambda_t}{\\beta}\n",
    "~=~\\frac{\\alpha\\,q_t}{p_t}\\,,\n",
    "\\end{eqnarray}\n",
    "and the marginal variance is\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{X_t} & ~\\doteq~ & \n",
    "\\mathbb{V}[X_t\\mid\\boldsymbol{\\lambda}]\n",
    "~=~\n",
    "\\mathbb{V}[\\mu_{X_t\\mid\\gamma}\\mid\\boldsymbol{\\lambda}]\n",
    "+\\mathbb{E}[\\sigma^2_{X_t\\mid\\gamma}\\mid\\boldsymbol{\\lambda}]\n",
    "\\\\& = &\n",
    "\\mathbb{V}[\\gamma\\lambda_t\\mid\\boldsymbol{\\lambda}]\n",
    "+\\mathbb{E}[\\gamma\\lambda_t\\mid\\boldsymbol{\\lambda}]\n",
    "\\\\& = &\n",
    "\\lambda_t^2\\,\\sigma^2_\\gamma+\\lambda_t\\,\\mu_\\gamma\n",
    "~=~\\frac{\\alpha\\lambda_t^2}{\\beta^2}+\\frac{\\alpha\\lambda_t}{\\beta}\n",
    "%~=~\\frac{\\lambda_t\\,\\mu_\\gamma}{p_t}\n",
    "\\\\& = &\n",
    "\\frac{\\alpha\\,(\\beta+\\lambda_t)\\,\\lambda_t}{\\beta^2}\n",
    "~=~\\frac{\\alpha\\,q_t}{p_t^2}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Lastly, the dependence between the $i$th and $j$th intervals is measured by\n",
    "\\begin{eqnarray}\n",
    "\\mathtt{Cov}[X_i,X_j] & ~\\doteq~ &\n",
    "\\mathbb{E}[X_i\\,X_j\\mid\\boldsymbol{\\lambda}]\n",
    "-\\mathbb{E}[X_i\\mid\\boldsymbol{\\lambda}]\n",
    "\\,\\mathbb{E}[X_j\\mid\\boldsymbol{\\lambda}]\n",
    "\\\\& = & \n",
    "\\mathbb{E}[\\,\n",
    "\\mathbb{E}[X_i\\,X_j\\mid\\boldsymbol{\\lambda},\\gamma]\\mid\\boldsymbol{\\lambda}\n",
    "]\n",
    "-\\mathbb{E}[X_i\\mid\\boldsymbol{\\lambda}]\n",
    "\\,\\mathbb{E}[X_j\\mid\\boldsymbol{\\lambda}]\n",
    "\\\\& = &\n",
    "\\mathbb{E}[\\gamma\\lambda_i\\,\\gamma\\lambda_j\\mid\\boldsymbol{\\lambda}]\n",
    "-\\lambda_i\\mu_\\gamma\\,\\lambda_j\\mu_\\gamma\n",
    "\\\\& = &\n",
    "\\lambda_i\\lambda_j\\,(\\sigma^2_\\gamma+\\mu_\\gamma^2)\n",
    "-\\lambda_i\\lambda_j\\,\\mu_\\gamma^2~=~\\lambda_i\\lambda_j\\sigma^2_\\gamma\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82663485",
   "metadata": {},
   "source": [
    "Finally, let us return to the total number $Y=X_1+\\cdots+X_n$ of\n",
    "countable events.\n",
    "The conditional joint distribution of the interval counts is given by\n",
    "\\begin{eqnarray}\n",
    "p(\\vec{\\mathbf{X}}_n\\mid\\boldsymbol{\\lambda},\\gamma) & = &\n",
    "\\prod_{t=1}^{n}\n",
    "e^{-\\gamma\\lambda_t}\\frac{(\\gamma\\lambda_t)^{X_t}}{X_t!}\n",
    "~=~\n",
    "\\gamma^{n\\bar{X}}\\,e^{-n\\bar{\\lambda}\\gamma}\\,\n",
    "\\prod_{t=1}^{n}\\frac{\\lambda_t^{X_t}}{X_t!}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where we have defined $n\\bar{\\lambda}\\doteq\\lambda_1+\\lambda_2+\\cdots+\\lambda_n$\n",
    "for convenience. Hence, the conditional distribution for $Y$ is\n",
    "given by\n",
    "\\begin{eqnarray}\n",
    "p(Y=k\\mid\\boldsymbol{\\lambda},\\gamma) & ~=~ &\n",
    "\\underset{\\mid X_1+\\cdots+X_n=k}{\\sum_{X_1=0}^\\infty\\cdots\\sum_{X_n=0}^\\infty}\n",
    "\\gamma^{k}\\,e^{-n\\bar{\\lambda}\\gamma}\\,\n",
    "\\prod_{t=1}^{n}\\frac{\\lambda_t^{X_t}}{X_t!}\n",
    "~=~\n",
    "e^{-n\\bar{\\lambda}\\gamma}\\,\\frac{(n\\bar{\\lambda}\\gamma)^k}{k!}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which follows from the multinomial expansion of $(\\lambda_1+\\cdots+\\lambda_n)^k$. Consequently, this is just a Poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821bc33",
   "metadata": {},
   "source": [
    "As an aside, suppose that we know the total $Y=k$ and want to determine the distribution of the interval counts. Then observe that\n",
    "\\begin{eqnarray}\n",
    "p(\\vec{\\mathbf{X}}\\mid Y=k,\\boldsymbol{\\lambda},\\gamma)\n",
    "& ~=~ &\n",
    "\\frac{p(\\vec{\\mathbf{X}},Y=k\\mid\\boldsymbol{\\lambda},\\gamma)}\n",
    "{p(Y=k,\\boldsymbol{\\lambda},\\gamma)}\n",
    "\\\\& = &\n",
    "\\frac{\\gamma^{k}\\,e^{-n\\bar{\\lambda}\\gamma}\\,\n",
    "\\prod_{t=1}^{n}\\frac{\\lambda_t^{X_t}}{X_t!}}\n",
    "{e^{-n\\bar{\\lambda}\\gamma}\\,\\frac{(n\\bar{\\lambda}\\gamma)^k}{k!}}\n",
    "\\\\& = &\n",
    "\\frac{k!}{\\prod_{t=1}^{n}X_t!}\n",
    "\\prod_{t=1}^{n}\\left(\\frac{\\lambda_t}{n\\bar{\\lambda}}\\right)^{X_t}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since $k=X_1+\\cdots+X_n$. This is just the multinomial distribution,\n",
    "where $\\frac{\\lambda_t}{\\lambda_1+\\cdots+\\lambda_n}$ is the probability\n",
    "of assigning any single one of the $k$ counts to the $t$th interval at random.\n",
    "Note that this result is independent of the conditional parameter $\\gamma$.\n",
    "Hence, we could equally have taken the ratio of the marginal distributions\n",
    "instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc9dd8",
   "metadata": {},
   "source": [
    "The marginal distribution for $Y$ is now given by\n",
    "\\begin{eqnarray}\n",
    "p(Y=k\\mid\\boldsymbol{\\lambda}) & ~=~ &\n",
    "\\int_0^\\infty\n",
    "e^{-n\\bar{\\lambda}\\gamma}\\frac{(n\\bar{\\lambda}\\gamma)^k}{k!}\\,\n",
    "\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,\\gamma^{\\alpha-1}\\,\n",
    "e^{-\\beta\\gamma}\\,d\\gamma\n",
    "\\\\& = &\n",
    "\\frac{(n\\bar{\\lambda})^k\\,\\beta^\\alpha}{k!\\,\\Gamma(\\alpha)}\\,\n",
    "\\int_0^\\infty\n",
    "\\gamma^{\\alpha+k-1}\\,e^{-(\\beta+n\\bar{\\lambda})\\gamma}\\,d\\gamma\n",
    "\\\\& = &\n",
    "\\frac{(n\\bar{\\lambda})^k\\,\\beta^\\alpha}{k!\\,\\Gamma(\\alpha)}\\,\n",
    "\\frac{\\Gamma(\\alpha+k)}{(\\beta+n\\bar{\\lambda})^{\\alpha+k}}\n",
    "\\\\&=&\n",
    "\\frac{\\Gamma(\\alpha+k)}{k!\\,\\Gamma(\\alpha)}\\,\n",
    "\\left(\\frac{\\beta}{\\beta+n\\bar{\\lambda}}\\right)^\\alpha\\,\n",
    "\\left(\\frac{n\\bar{\\lambda}}{\\beta+n\\bar{\\lambda}}\\right)^k\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "This is just a negative binomial distribution with the overall\n",
    "probabilities of non-countable and countable events given by\n",
    "\\begin{eqnarray}\n",
    "\\bar{p}~\\doteq~\\frac{\\beta}{\\beta+n\\bar{\\lambda}}\n",
    "& ~\\;\\;\\Rightarrow~\\;\\; &\n",
    "\\bar{q}~=~1-\\bar{p}~\\doteq~\\frac{n\\bar{\\lambda}}{\\beta+n\\bar{\\lambda}}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. The marginal mean count is thus\n",
    "\\begin{eqnarray}\n",
    "\\mu_Y & ~=~ & \\frac{\\alpha\\bar{q}}{\\bar{p}}\n",
    "~=~\\frac{n\\bar{\\lambda}\\alpha}{\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "and the marginal variance is\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_Y & ~=~ & \\frac{\\alpha\\bar{q}}{\\bar{p}^2}\n",
    "~=~\\frac{n\\bar{\\lambda}\\alpha\\,(\\beta+n\\bar{\\lambda})}{\\beta^2}\n",
    "~=~\\frac{n\\bar{\\lambda}\\alpha}{\\beta}+\\frac{n^2\\bar{\\lambda}^2\\alpha}{\\beta^2}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In comparison, the sum of variances of the marginal interval counts is\n",
    "\\begin{eqnarray}\n",
    "\\sum_{t=1}^{n}\\sigma^2_{X_t} & ~=~ &\n",
    "\\sum_{t=1}^{n}\\frac{\\alpha\\lambda_t\\,(\\beta+\\lambda_t)}{\\beta^2}\n",
    "~=~\\frac{n\\bar{\\lambda}\\alpha}{\\beta}+\\frac{n\\overline{\\lambda^2}\\alpha}{\\beta^2}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $n\\overline{\\lambda^2}\\doteq\\lambda_1^2+\\cdots+\\lambda_n^2$.\n",
    "The excess variance is then\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_Y - \\sum_{t=1}^{n}\\sigma^2_{X_t} & ~=~ &\n",
    "\\frac{\\alpha}{\\beta^2}\\,\\left[(\\lambda_1+\\cdots+\\lambda_n)^2\n",
    "-(\\lambda_1^2+\\cdots+\\lambda_n^2)\\right]\n",
    "\\\\& = &\n",
    "\\frac{2\\alpha}{\\beta^2}\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\lambda_i\\lambda_j\n",
    "~=~2\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\mathtt{Cov}[X_i,X_j]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since $\\sigma^2_\\gamma=\\frac{\\alpha}{\\beta^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd8431",
   "metadata": {},
   "source": [
    "## Beta-Binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c4b70",
   "metadata": {},
   "source": [
    "The binomial distribution may be derived as a sum of $n$ independent\n",
    "Bernoulli variables. Hence, we may generalise the\n",
    "[beta-Bernoulli](#Beta-Bernoulli-distribution \"Section: Beta-Bernoulli distribution\") distribution to the binomial case.\n",
    "One rationale for doing so is to model  data that are empirically\n",
    "over-dispersed with respect to the binomial distribution.\n",
    "This is similar to our use of the\n",
    "[gamma-Poisson](#Gamma-Poisson-distribution \"Section: Gamma-Poisson distribution\") distribution to handle over-dispersion with respect to the\n",
    "Poisson distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca6fd8",
   "metadata": {},
   "source": [
    "### Beta prior distribution (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18e766",
   "metadata": {},
   "source": [
    "Once again, we utilise the general\n",
    "[beta](#Beta-prior-distribution \"Section: Beta prior distribution\")\n",
    "prior of\n",
    "\\begin{eqnarray}\n",
    "\\theta & ~\\sim~ & \\mathtt{Beta}(\\alpha,\\beta)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for which the mean and variance are given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_\\theta & ~\\doteq~ & \\mathbb{E}[\\theta\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\alpha}{\\alpha+\\beta}\\,,\n",
    "\\\\\n",
    "\\sigma^2_\\theta & ~\\doteq~ & \\mathbb{V}[\\theta\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2\\,(\\alpha+\\beta+1)}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "[Recall](https://en.wikipedia.org/wiki/Beta_distribution\n",
    "\"Wikipedia: Beta distribution\") that the beta density function is\n",
    "\\begin{eqnarray}\n",
    "p(\\theta\\mid\\alpha,\\beta) & ~=~ &\n",
    "\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\\,\n",
    "\\theta^{\\alpha-1}\\,(1-\\theta)^{\\beta-1}\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f7a33",
   "metadata": {},
   "source": [
    "### Binomial data likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5eda3f",
   "metadata": {},
   "source": [
    "The binomial distribution abstracts from each observed Bernoulli sequence of $n$ trials by disregarding the order and simply counting the total number of\n",
    "special events of interest (e.g. successes, or failures) which occurred. It happens that such sequences are *exchangeable*, meaning that every perturbation of an observed\n",
    "sequence has the *same* probability of occurrence. Thus, there are\n",
    "$n\\choose x$ exchangeable sequences all having exactly $x$ special events and $n-x$ non-special events, in any order.\n",
    "\n",
    "Thus, let the count $X$ follow a binomial distribution, namely\n",
    "\\begin{eqnarray}\n",
    "X\\mid\\theta & ~\\sim~ & \\texttt{Binom}(n,\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\theta$ is the probability of an event of interest.\n",
    "In our sporting match scenario, we might let $n$ be the total allowable\n",
    "number of attempted scoring shots in a fixed period of time.\n",
    "Then $\\theta$ might be the probability \n",
    "that any given attempt actually succeeds. Thus, $X$ would give the total score\n",
    "of a given team for the match.\n",
    "\n",
    "The likelihood of a given score is then\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,\\theta) & ~=~ & {n\\choose x}\\,\\theta^x\\,(1-\\theta)^{1-x}\\,,\n",
    "\\end{eqnarray}\n",
    "which gives a conditional mean and variance of\n",
    "\\begin{eqnarray}\n",
    "\\mu_{X\\mid\\theta} & ~\\doteq~ & \\mathbb{E}[X\\mid n,\\theta]\n",
    "~=~n\\,\\theta\\,,\n",
    "\\\\\n",
    "\\sigma^2_{X\\mid\\theta} & ~\\doteq~ & \\mathbb{V}[X\\mid n,\\theta]\n",
    "~=~ n\\,\\theta\\,(1-\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Observe that $\\sigma^2_{X\\mid\\theta}<\\mu_{X\\mid\\theta}$ for all $\\theta\\in(0,1)$.\n",
    "Thus the binomial distribution is under-dispersed in comparison to\n",
    "the Poisson distribution, for which \n",
    "$\\sigma^2_{X\\mid\\lambda}=\\mu_{X\\mid\\lambda}=\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a81ff",
   "metadata": {},
   "source": [
    "### Beta-binomial mixture distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e625d",
   "metadata": {},
   "source": [
    "To allow for over-dispersion, we permit the probability $\\theta$\n",
    "to first be drawn from the beta prior before the start of each match, and then to be held constant for the duration of the match.\n",
    "The law of \n",
    "[iterated expectations](https://en.wikipedia.org/wiki/Law_of_total_expectation\n",
    "\"Wikipedia: Law of total expectation\") then gives\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & ~\\doteq~ & \\mathbb{E}[X] ~=~\n",
    "\\mathbb{E}[\\mu_{X\\mid\\theta}\\mid\\alpha,\\beta]\n",
    "\\\\\n",
    "& ~=~ & \\mathbb{E}[n\\,\\theta\\mid\\alpha,\\beta]~=~n\\mu_\\theta\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, the \n",
    "law of \n",
    "[iterated variances](https://en.wikipedia.org/wiki/Law_of_total_variance\n",
    "\"Wikipedia: Law of total variance\")\n",
    "gives\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X & ~\\doteq~ & \\mathbb{V}[X] ~=~\n",
    "\\mathbb{V}[\\mu_{X\\mid n,\\theta}\\mid\\alpha,\\beta]\n",
    "+\n",
    "\\mathbb{E}[\\sigma^2_{X\\mid n,\\theta}\\mid\\alpha,\\beta]\n",
    "\\\\\n",
    "& ~=~ & \n",
    "\\mathbb{V}[n\\,\\theta\\mid\\alpha,\\beta]\n",
    "+\n",
    "\\mathbb{E}[n\\,\\theta\\,(1-\\theta)\\mid\\alpha,\\beta]\n",
    "\\\\\n",
    "& ~=~ & n^2\\sigma^2_\\theta+n\\,\\mu_\\theta-n\\,(\\sigma^2_\\theta+\\mu_\\theta^2)\n",
    "\\\\\n",
    "& = &\n",
    "n\\,\\mu_\\theta\\,(1-\\mu_\\theta)+n(n-1)\\,\\sigma^2_\\theta\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that if $\\sigma^2_\\theta=0$ then $\\theta=\\mu_\\theta$ and the variance reduces to the usual binomial variance of $\\sigma^2_X=n\\,\\theta\\,(1-\\theta)$.\n",
    "Hence, for $\\sigma^2_\\theta>0$ the beta-binomial allows for additional\n",
    "variance over the binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c8d47",
   "metadata": {},
   "source": [
    "The \n",
    "[beta-binomial](https://en.wikipedia.org/wiki/Beta-binomial_distribution\n",
    "\"Wikipedia: Beta-binomial distribution\")\n",
    "mixture distribution for a single score $X$ is now derived as\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,\\alpha,\\beta) & ~=~ & \\int_0^1 p(x\\mid n,\\theta)\\,p(\\theta\\mid\\alpha,\\beta)\\,d\\theta\n",
    "\\\\& = &\n",
    "\\int_0^1 {n\\choose x}\\theta^x(1-\\theta)^{n-x}\\,\n",
    "\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\n",
    "\\,\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\,d\\theta\n",
    "\\\\& = &\n",
    "{n\\choose x}\\,\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\n",
    "\\int_0^1 \\theta^{\\alpha+x-1}(1-\\theta)^{\\beta+n-x-1}\\,d\\theta\n",
    "\\\\& =  &\n",
    "{n\\choose x}\\,\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\n",
    "\\,\\frac{\\Gamma(\\alpha+x)\\,\\Gamma(\\beta+n-x)}{\\Gamma(\\alpha+\\beta+n)}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since the final integrand is proportional to the $\\texttt{Beta}(\\alpha+x,\\beta+n-x)$ distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed03a9",
   "metadata": {},
   "source": [
    "Let us now define $z!\\doteq\\Gamma(z+1)$ for real-valued $z\\in[0,\\infty)$.\n",
    "Then the beta-binomial distribution becomes\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid n,\\alpha,\\beta) & ~=~ &\n",
    "\\frac{n!\\,(\\alpha+\\beta-1)!\\,(\\alpha+x-1)!\\,(\\beta+n-x-1)!}\n",
    "{x!\\,(n-x)!\\,(\\alpha-1)!\\,(\\beta-1)!\\,(\\alpha+\\beta+n-1)!}\n",
    "\\\\\n",
    "& = &\n",
    "\\frac{(\\alpha+x-1)!}{x!\\,(\\alpha-1)!}\\,\n",
    "\\frac{(\\beta+n-x-1)!}{(n-x)!\\,(\\beta-1)!}\n",
    "\\left/\\frac{(\\alpha+\\beta+n-1)!}{n!\\,(\\alpha+\\beta-1)!}\\right.\n",
    "\\\\ & = &\n",
    "{\\alpha+x-1\\choose x}\\,{\\beta+n-x-1\\choose n-x}\n",
    "\\left/{\\alpha+\\beta+n-1\\choose n}\\right.\\,.\n",
    "\\end{eqnarray}\n",
    "This is just the real-valued  version of\n",
    "the [negative hypergeometric](https://en.wikipedia.org/wiki/Negative_hypergeometric_distribution\n",
    "\"Wikipedia: Negative hypergeometric distribution\")\n",
    "distribution, i.e. a *Polya urn* distribution.\n",
    "Notionally, in the integer-valued version, the urn contains $K=n$ successes and $N-K=\\alpha+\\beta-1$ failures,\n",
    "and sampling without replacement halts as soon as $r=\\alpha$ failures have been observed, giving $k=x$ observed successes.\n",
    "\n",
    "Note that, paradoxically, $\\alpha$ is actually the prior number of *successes*\n",
    "from the $\\texttt{Beta}(\\alpha,\\beta)$ prior distribution.\n",
    "The fact that $r=\\alpha$ is also the required number of urn sampling *failures* can be explained\n",
    "(with a bit of hand waving)\n",
    "by the fact that a high value of $\\alpha$ would imply that a high number $x$ of successes would be expected to occur\n",
    "before $\\alpha$ failures were finally observed.\n",
    "This makes sence since, from the beta distribution, a high value of $\\alpha$ would lead to a high expected probability of observing a success in any given\n",
    "Bernoulli trial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f3cd8",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24764d9b",
   "metadata": {},
   "source": [
    "[1] George E. P. Box and George C. Tiao (1973), \"*Bayesian Inference in Statistical Analysis*\", John Wiley & Sons.\n",
    "\n",
    "[2] Jouni Kerman (2011), \"*Neutral noninformative and informative conjugate beta and gamma prior distributions*\", Electron. J. Statist. 5: 1450-1470."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
