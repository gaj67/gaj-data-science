{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddd0507",
   "metadata": {},
   "source": [
    "# Appendix C: Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f7904",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a quick introduction to the mechanics of generalised linear models (GLMs) for regression,\n",
    "first introduced by Nelder and Wedderburn [[1]](#Citations \"Citation [1]: Generalized Linear Models\"). We extend the notion to generalised nonlinear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d2be0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588e123",
   "metadata": {},
   "source": [
    "### Probability distribution functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8caa0",
   "metadata": {},
   "source": [
    "We consider a multi-dimensional or scalar (i.e. uni-dimensional) variate $X$ on domain $\\mathcal{X}$. Let $X$ have an underlying probability distribution function (PDF) $p$ parameterised by $\\boldsymbol{\\theta}$, satisfying non-negativity, i.e.\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})~\\ge~0 && \\forall\\mathbf{x}\\in\\mathcal{X}\\,,\n",
    "\\end{eqnarray}\n",
    "and a total probability of unity, i.e.\n",
    "\\begin{eqnarray}\n",
    "\\int_\\mathcal{X}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}| & = & 1\\,,\n",
    "\\end{eqnarray}\n",
    "where $|d\\mathbf{x}|$ is taken to be an infinitesimal volume or length in $\\mathcal{X}$. Note that for discrete variates the constraint\n",
    "is instead\n",
    "\\begin{eqnarray}\n",
    "\\sum_{\\mathbf{x}\\in\\mathcal{X}}p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) & = & 1\\,.\n",
    "\\end{eqnarray}\n",
    "We shall henceforth consider only continuous variates for convenience, but the resulting derivations will also hold \n",
    "in the discrete case by replacing integration with summation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72dd798",
   "metadata": {},
   "source": [
    "When considered as general functions, PDFs have a variety of properties and constraints. For instance, for a continuous distribution, the integral represents the area under the curve. As a consequence of the non-negativity constraint, this places limits on the values of $p(\\mathbf{x}\\mid\\boldsymbol{\\theta})$. For example, if the domain $\\mathcal{X}$ has no finite upper bound, then $p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) \\rightarrow 0$ as $\\mathbf{x}\\rightarrow\\infty$. Similarly,\n",
    "$p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) \\rightarrow 0$ as $\\mathbf{x}\\rightarrow -\\infty$ if $\\mathcal{X}$\n",
    "does not have a finite lower bound. Similar conditions hold for spatial derivatives with respect to $\\mathbf{x}$ at the extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c11338",
   "metadata": {},
   "source": [
    "Consider now derivatives with respect to the parameter $\\boldsymbol{\\theta}$, denoted by the gradient vector operator\n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\doteq\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}$, which we take to be a column vector. Similarly, second derivatives are denoted by the *Hessian* matrix operator\n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}\\doteq\\frac{\\partial^2}{\\partial\\boldsymbol{\\theta}\\partial\\boldsymbol{\\theta}^{T}}$.\n",
    "[Later](#Parameter-transformations \"Section: Parameter transformations\") \n",
    "we shall also require the first and second derivatives with respect to some transformation\n",
    "$\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$ of the parameters, denoted\n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}$ and \n",
    "$\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}$, respectively.\n",
    "In general, unless we specifically need to distinguish between these two cases, we may drop the subscript and assume the results hold for all parameterisations.\n",
    "\n",
    "Now, considering the total probability constraint above, since the derivatives are with respect to\n",
    "$\\boldsymbol{\\theta}$ or $\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$ and not $\\mathbf{x}$, it follows that\n",
    "taking first derivatives of both sides gives\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\int_\\mathcal{X}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}| & = &\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla} p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\\mathbf{0}\\,,\n",
    "\\end{eqnarray}\n",
    "and taking second derivatives gives\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}\\int_\\mathcal{X}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}| & = &\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T} p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\\mathbf{O}\\,.\n",
    "\\end{eqnarray}\n",
    "We shall use these results in the\n",
    "[next](#Expectations-and-log-likelihoods \"Section: Log-likelihoods and expectations\") section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619a3cb",
   "metadata": {},
   "source": [
    "Note that there are various fields of stochatic modelling, such as Maximum Entropy, that require use of the general properties of PDFs in order to construct specific PDFs that fit given theoretical or practical requirements. For the rest of this doccument, however, we shall assume that the form of the PDF has been specified in advance. The properties we require then relate instead to fitting the PDF to observed data via regression modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe3393",
   "metadata": {},
   "source": [
    "### Expectations and log-likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dee07",
   "metadata": {},
   "source": [
    "We assume the \n",
    "[*law of the unconscious statistician*](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician \"en.wikipedia.org\"), and take the expectation of arbitrary function $\\mathbf{f}(X, \\boldsymbol{\\theta})$ to be given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_X\\left[\\mathbf{f}(X, \\boldsymbol{\\theta})\\mid\\boldsymbol{\\theta}\\right] & \\doteq & \n",
    "\\int_\\mathcal{X}\\mathbf{f}(\\mathbf{x}, \\boldsymbol{\\theta})\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that in general we may drop the subscript when it is clear with respect to which variate we are taking the expectation.\n",
    "Also note that since the expectation is the weighted mean of the function $\\mathbf{f}$, we often denote this for convenience  as\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_\\mathbf{f}(\\boldsymbol{\\theta}) & \\doteq & \\mathbb{E}\\left[\\mathbf{f}\\mid\\boldsymbol{\\theta}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, note that function $\\mathbf{f}$ may in general be scalar, vector, matrix or even tensor valued.\n",
    "However, we henceforth typically assume, without loss of generality, some scalar function $f$ (unless otherwise stated), since the expectation of a vector is a vector of scalar expectations, and likewise for matrices and tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2650fd3",
   "metadata": {},
   "source": [
    "Thus, taking the gradient of the expectation, we see that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\,\\mathbb{E}\\left[f\\mid\\boldsymbol{\\theta}\\right] & = & \n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "+\n",
    "\\int_\\mathcal{X}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\& = &\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "+\n",
    "\\int_\\mathcal{X}f(\\mathbf{x}, \\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}\\ln p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}f\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\n",
    "\\mathbb{E}\\left[f\\,\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where, for convenience, we have defined the log-likelihood $L$ as\n",
    "\\begin{eqnarray}\n",
    "L(\\boldsymbol{\\theta}; X) & \\doteq & \\ln p(X\\mid\\boldsymbol{\\theta})\\,.\n",
    "\\end{eqnarray}\n",
    "Note that if we instead used a vector function $\\mathbf{f}$, then we would have a choice of either the scalar gradient\n",
    "$\\boldsymbol{\\nabla}^T\\mathbf{f}$ or the matrix gradient $\\boldsymbol{\\nabla}\\mathbf{f}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4e016",
   "metadata": {},
   "source": [
    "Suppose now, as an example, that we take the constant function \n",
    "$f\\equiv 1~\\Rightarrow\\boldsymbol{\\nabla}f\\equiv\\mathbf{0}$.\n",
    "Then we immediately deduce that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right] & = & \\mathbf{0}\\,.\n",
    "\\end{eqnarray}\n",
    "This is one of the useful\n",
    " results from Kendall and Stuart [[2]](#Citations \"Citation [2]: The Advanced Theory of Statistics\").\n",
    "It is of interest to derive this result direcly.\n",
    "We begin by taking the gradient of $L$, namely\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}L & = & \n",
    "\\boldsymbol{\\nabla}\\ln p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "~=~\\frac{\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, taking the expectation of the gradient, we therefore obtain\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\int_\\mathcal{X}\n",
    "\\frac{\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "~=~\\mathbf{0}\\,,\n",
    "\\end{eqnarray}\n",
    "as before. The last part follows from the\n",
    "[previous](#Probability-distribution-functions \"Section: Probability distribution functions\") section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3d9f8",
   "metadata": {},
   "source": [
    "Similarly, taking the Hessian of $L$ gives\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L & = & \n",
    "\\boldsymbol{\\nabla}\\left\\{\n",
    "\\frac{\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\right\\}\n",
    "~=~\\frac{\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "}\n",
    "{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})^2}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, taking the expectation results in\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\int_\\mathcal{X}\n",
    "\\frac{\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "}\n",
    "{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})^2}\n",
    "\\,\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\\n",
    "& = & \n",
    "\\int_\\mathcal{X}\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "- \\int_\\mathcal{X}\n",
    "\\frac{\\boldsymbol{\\nabla}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,\\frac{\\boldsymbol{\\nabla}^{T}p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}{p(\\mathbf{x}\\mid\\boldsymbol{\\theta})}\n",
    "\\,p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,|d\\mathbf{x}|\n",
    "\\\\\n",
    "& = &\n",
    "\\mathbf{O}-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which again follows from the [previous](#Probability-distribution-functions \"Section: Probability distribution functions\") section. We shall require these results later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56266a0c",
   "metadata": {},
   "source": [
    "Finally, we note that the expected value of the log-likelihood itself is given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[L\\mid\\boldsymbol{\\theta}\\right] & = & \n",
    "\\int_\\mathcal{X} p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\\,\\ln p(\\mathbf{x}\\mid\\boldsymbol{\\theta})\n",
    "\\,|d\\mathbf{x}|~\\doteq~ -H(X\\mid\\boldsymbol{\\theta})\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $H(X\\mid\\boldsymbol{\\theta})$ is just the information-theoretic entropy of the distribution measured in *nats*. Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}H(X\\mid\\boldsymbol{\\theta}) & = &\n",
    "-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "-\n",
    "\\mathbb{E}\\left[L\\,\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~ -\\mathbb{E}\\left[L\\,\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which follows from the derivation above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fed659",
   "metadata": {},
   "source": [
    "### Parameter transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907b4e6",
   "metadata": {},
   "source": [
    "Suppose now that we wish to take derivatives, not with respect to the \n",
    "[PDF](#Probability-distribution-functions \"Section: Probability distribution functions\") \n",
    "parameters $\\boldsymbol{\\theta}$, but with respect to some other reparameterisation, say $\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\theta)$.\n",
    "For this purpose, we consider the chain rules, namely that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}~=~\n",
    "\\frac{\\partial\\boldsymbol{\\eta}^T}{\\partial\\boldsymbol{\\theta}}\\frac{\\partial}{\\partial\\boldsymbol{\\eta}}\\,,\n",
    "& \\;\\;\\;\\mbox{and}~ &\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\eta}}~=~\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}\\,.\n",
    "\\end{eqnarray}\n",
    "For convenience, we define \n",
    "$\\mathbf{J}_\\boldsymbol{\\eta}\\doteq\\frac{\\partial\\boldsymbol{\\eta}^T}{\\partial\\boldsymbol{\\theta}}$\n",
    "to be the *Jacobian* matrix of the transformation $\\boldsymbol{\\eta}(\\theta)$. It then follows from the first chain rule that the gradients are related via\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}~=~\\mathbf{J}_\\boldsymbol{\\eta}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\n",
    "& ~\\Rightarrow~ &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}~=~\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\,,\n",
    "\\end{eqnarray}\n",
    "and thus we deduce from the second chain rule that $\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\doteq\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}$. Note that $\\mathbf{J}_\\boldsymbol{\\eta}$ and $\\mathbf{J}_\\boldsymbol{\\eta}^{-1}$ are only truly matrix inverses of each other if both\n",
    "$\\boldsymbol{\\theta}$ and $\\boldsymbol{\\eta}$ have the same dimensions, otherwise we shall treat them symbolically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d02617",
   "metadata": {},
   "source": [
    "The relationship between the Hessians is more involved. Firstly, we take the transpose of the second chain rule  to obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\eta}^T} & = &\n",
    "\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\\,,\n",
    "\\end{eqnarray}\n",
    "where the symbol \"$\\cdot$\" indicates the position of the argument.\n",
    "Next, we then apply the second chain rule directly to this result, thereby obtaining\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial^2}{\\partial\\boldsymbol{\\eta}\\,\\partial\\boldsymbol{\\eta}^T} & = &\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}\n",
    "\\left\\{\n",
    "\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "\\right\\}\n",
    "\\\\& = &\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\n",
    "\\left\\{\n",
    "\\frac{\\partial^2\\,\\cdot}{\\partial\\boldsymbol{\\theta}\\,\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "+\\left(\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}\\right)\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "\\right\\}\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, we replace the last derivative\n",
    "$\\frac{\\partial}{\\partial\\boldsymbol{\\theta}}$\n",
    "via the first chain rule, to obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial^2}{\\partial\\boldsymbol{\\eta}\\,\\partial\\boldsymbol{\\eta}^T} & = &\n",
    "\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\n",
    "\\left\\{\n",
    "\\frac{\\partial^2\\,\\cdot}{\\partial\\boldsymbol{\\theta}\\,\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^T}\n",
    "+\\left(\\frac{\\partial\\,\\cdot}{\\partial\\boldsymbol{\\theta}^T}\\,\n",
    "\\frac{\\partial\\boldsymbol{\\eta}^T}{\\partial\\boldsymbol{\\theta}}\\right)\\odot\n",
    "\\frac{\\partial^2\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}\\,\\partial\\boldsymbol{\\eta}^T}\n",
    "\\right\\}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that the last term is the dot product \"$\\odot$\" of a (row) vector with a *tensor*, i.e. a column \"vector\" in which each element is \n",
    "itself a matrix. \n",
    "Consequently, in terms of the gradient operators and Jacobian matrices, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T} & = &\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}(\\cdot)\\,\n",
    "\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\n",
    "+\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\left(\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^T(\\cdot)\\,\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}\\right)\\odot\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\\,.\n",
    "\\end{eqnarray}\n",
    "Also note that various specialisations of this relationship occur depending upon both the parameters and the reparameterisation.\n",
    "For example, in the case of scalar $\\theta$ and scalar $\\eta$, the identity simplifies to\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial^2}{\\partial\\eta^2} & = & \\left(\\frac{\\partial\\theta}{\\partial\\eta}\\right)^2\n",
    "\\frac{\\partial^2}{\\partial\\theta^2} +\n",
    "\\frac{\\partial^2\\theta}{\\partial\\eta^2}\\,\\frac{\\partial}{\\partial\\theta}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1408a",
   "metadata": {},
   "source": [
    "Now, the general identity will be a bit complex to apply in practice.\n",
    "However, we find that there is a simpler approximation when we specifically consider the log-likelihood $L$, such that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L & = &\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\n",
    "+\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\left(\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\\,\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}\\right)\\odot\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\boldsymbol{\\theta}\\,.\n",
    "\\end{eqnarray}\n",
    "In particular, taking the expecation of both sides, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\n",
    "& = &\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\,\n",
    "\\left[\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\right]^{T}\n",
    "+\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\left(\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\,\\mathbf{J}_\\boldsymbol{\\eta}\\right)\\odot\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\boldsymbol{\\theta}\n",
    "\\\\&=&\n",
    "-\\frac{\\partial\\boldsymbol{\\theta}^T}{\\partial\\boldsymbol{\\eta}}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\,\n",
    "\\frac{\\partial\\boldsymbol{\\theta}}{\\partial\\boldsymbol{\\eta}^{T}}\n",
    "~=~ -\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which follows from a [previous](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\") section,\n",
    "where we derived that $\\mathbf{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]=\\mathbf{0}$\n",
    "and\n",
    "$\\mathbb{E}\\left[\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]=\n",
    "-\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\n",
    "\\boldsymbol{\\nabla}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]$.\n",
    "This approximation to the Hessian of the log-likelihood will be used in the\n",
    "[next](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da760047",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a1a17",
   "metadata": {},
   "source": [
    "Consider a stochastic sampling process that produces a sequence of (arbitrary) $n$ independent, identically distributed variables,\n",
    "$X_1, X_2, \\ldots, X_n$. Then the sample average is defined as\n",
    "\\begin{eqnarray}\n",
    "\\langle X\\rangle ~\\doteq~\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}X_i\\,.\n",
    "\\end{eqnarray}\n",
    "More generally, the sample average of an arbitrary function $\\mathbf{f}(X,\\ldots)$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mathbf{f}\\rangle(\\ldots) & \\doteq &\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{f}( X_i,\\ldots)\\,,\n",
    "\\end{eqnarray}\n",
    "where the ellipsis \"$\\ldots$\" represents arbitrary parameters that do not vary with $X$.\n",
    "Due to the linearity of the operator $\\langle\\cdot\\rangle$, we henceforth typically consider a scalar function $f$\n",
    "(unless otherwise stated), since the sample mean of a vector is a vector of scalar sample means, and likewise for\n",
    "matrices and tensors.\n",
    "Similarly, it also follows from the linearity of the various other operators\n",
    "that the parameter gradient and Hessian of the sample mean obey\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}\\langle f\\rangle ~=~\n",
    "\\left\\langle\\boldsymbol{\\nabla}f\\right\\rangle &\\;\\;\\mbox{and}\\;&\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}\\langle f\\rangle ~=~\n",
    "\\left\\langle\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}f\\right\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "respectively, and the expectation obeys\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\langle f\\rangle\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\left\\langle\\mathbb{E}\\left[f\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\n",
    "~=~\\mathbb{E}\\left[f\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since the variates $X_i$ are here assumed to be independent and identically distributed (IID).\n",
    "We shall relax this last restriction [later](#Regression-modelling \"Section: Regression modelling\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8b768",
   "metadata": {},
   "source": [
    "We therefore see that the sample-mean log-likelihood $\\langle L\\rangle$ satisfies\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}\\langle L\\rangle\\mid\\boldsymbol{\\theta}\\right] & = &\n",
    "\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\n",
    "~=~\\mathbf{0}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with the last result obtained from a [previous](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\") section.\n",
    "This result motivates the maximum likelihood approach, which is to determine the estimate $\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$ that satisfies\n",
    "$\\langle\\boldsymbol{\\nabla}L\\rangle(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})=\\mathbf{0}$,\n",
    "if such a solution exists.\n",
    "Under mild conditions of convexity,\n",
    "$\\langle L\\rangle(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$ is a local maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dae87e",
   "metadata": {},
   "source": [
    "The maximum-likelihood parameters\n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$ themselves are usually found iteratively via an update scheme of the form\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta}' & = & \\boldsymbol{\\theta}+\\Delta\\boldsymbol{\\theta}\\,,\n",
    "\\end{eqnarray}\n",
    "where all requisite quantities are evaluated at the current estimate $\\boldsymbol{\\theta}$ of the parameters.\n",
    "Then the parameter increment $\\Delta\\boldsymbol{\\theta}$ itself is usually computed either via\n",
    "a direct gradient method, e.g. gradient ascent\n",
    "\\begin{eqnarray}\n",
    "\\Delta\\boldsymbol{\\theta} & \\doteq & \\rho\\,\\boldsymbol{\\nabla}\\langle L\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "or via a modified gradient method, e.g. the Newton-Raphson method\n",
    "\\begin{eqnarray}\n",
    "\\Delta\\boldsymbol{\\theta} & \\doteq & \n",
    "-\\left[\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}\\langle L\\rangle\\right]^{-1}\\,\n",
    "\\boldsymbol{\\nabla}\\langle L\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that, in either case, the iterations will halt when \n",
    "$\\Delta\\boldsymbol{\\theta}=\\mathbf{0}$, which occurs when\n",
    "the gradient of the sample-mean log-likelihood vanishes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45cdcf",
   "metadata": {},
   "source": [
    "In practice, we usually apply the Newton-Raphson scheme by solving the linear equation\n",
    "\\begin{eqnarray}\n",
    "-\\langle\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}L\\rangle\\,\\Delta\\boldsymbol{\\theta} & = & \n",
    "\\langle\\boldsymbol{\\nabla}L\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "However, the Hessian is often difficult to compute, and so an approximation is typically used.\n",
    "Hence, following the reasoning from the\n",
    "[previous](#Parameter-transformations \"Section: Parameter transformations\") section,\n",
    "we take the expectation of the left-hand side only (since the expectation of the right-hand side is always zero).\n",
    "This allows us to compute an approximate update as the solution to\n",
    "\\begin{eqnarray}\n",
    "-\\left\\langle\\mathbb{E}\\left[\n",
    "\\boldsymbol{\\nabla}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\\,\\Delta\\boldsymbol{\\theta} \n",
    "& = & \n",
    "\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\n",
    "\\,\\Delta\\boldsymbol{\\theta} ~=~\n",
    "\\langle\\boldsymbol{\\nabla}L\\rangle\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which has the advantage of only requiring knowledge about the gradient of the log-likelihood.\n",
    "Note that some other gradient update schemes also use approximations to the Hessian. For example, the LBFGS algorithm computes an approximate Hessian matrix from (multiple) previous estimates of the gradient,\n",
    "effectively approximating the expectation itself by a temporal average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710b12b",
   "metadata": {},
   "source": [
    "Finally, note that we might more generally consider some \n",
    "[parameter transformation](#Parameter-transformations \"Section: Parameter transformations\") \n",
    "$\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$, and therefore the transformed parameter update\n",
    "$\\Delta\\boldsymbol{\\eta}$ would follow from the above formulae by\n",
    "taking all gradients and Hessians with respect to $\\boldsymbol{\\eta}$ rather than $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea62110",
   "metadata": {},
   "source": [
    "## Exponential families"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4f8db",
   "metadata": {},
   "source": [
    "Since [PDFs](#Probability-distribution-functions \"Section: Probability distribution functions\") are required to be\n",
    "non-negative, it follows that they may be expressed as exponentials. Different exponential forms lead to different families of distributions.\n",
    "Of particular interest is a [general family](#General-form \"Section: General form\")\n",
    "of distributions having linearly-additive log-likelihoods, and also a more \n",
    "[specialised family](#Seperable-dependencies \"Section: Seperable dependencies\"),\n",
    "misleading called \"**the**\" exponential family,\n",
    "having bilinear or separable dependencies between the variates and the parameters. These distributions are discussed in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ce121",
   "metadata": {},
   "source": [
    "### General form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7b31",
   "metadata": {},
   "source": [
    "Clearly, a [PDF](#Probability-distribution-functions \"Section: Probability distribution functions\")\n",
    "$p(\\mathbf{x}\\mid\\boldsymbol{\\theta})$ must be the exponential of its log-likelihood\n",
    "$L(\\boldsymbol{\\theta};\\,\\mathbf{x})$. Considered as an additive model with variate $X$, the log-likelihood will in general have constant terms, terms in $X$ but not $\\boldsymbol{\\theta}$, terms in $\\boldsymbol{\\theta}$ but not $X$,\n",
    "and terms containing interactions between $X$ and $\\boldsymbol{\\theta}$. Hence, the general log-likelihood takes the form\n",
    "\\begin{eqnarray}\n",
    "L(\\boldsymbol{\\theta};\\,X) & = & \\ln h(X)-\\ln Ƶ(\\boldsymbol{\\theta})+s(X,\\boldsymbol{\\theta})\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where any constant terms may be placed in either or both of $h(X)$ or $Ƶ(\\boldsymbol{\\theta})$, but the\n",
    "interaction function $s(X,\\boldsymbol{\\theta})$ may contain neither constant terms, nor terms only in $X$,\n",
    "nor terms only in $\\boldsymbol{\\theta}$.\n",
    "Consequently, the general probability distribution is specified by\n",
    "\\begin{eqnarray}\n",
    "p(\\mathbf{x}\\mid\\boldsymbol{\\theta}) & = & \n",
    "\\frac{h(\\mathbf{x})\\,e^{s(\\mathbf{x},\\boldsymbol{\\theta})}}\n",
    "     {Ƶ(\\boldsymbol{\\theta})}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $Ƶ(\\boldsymbol{\\theta})$ is now seen tio be the normalising *partition* function defined by\n",
    "\\begin{eqnarray}\n",
    "Ƶ(\\boldsymbol{\\theta}) & \\doteq & \n",
    "\\int_\\mathcal{X}h(\\mathbf{x})\\,e^{s(\\mathbf{x},\\boldsymbol{\\theta})}\n",
    "\\,|d\\mathbf{x}|\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ec89a",
   "metadata": {},
   "source": [
    "It now [follows](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\") \n",
    "from the parameter gradient of the log-likelihood $L$ that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}L(\\boldsymbol{\\theta}; X) & = & \\boldsymbol{\\nabla}s(X,\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}\\ln Ƶ(\\boldsymbol{\\theta})\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\mid\\boldsymbol{\\theta}\\right] & = & \n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "-\\boldsymbol{\\nabla}\\ln Ƶ~=~\\mathbf{0}\n",
    "\\\\\n",
    "\\Rightarrow \n",
    "\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s} & ~\\doteq~ &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}\\ln Ƶ\n",
    "\\\\\n",
    "\\Rightarrow \\boldsymbol{\\nabla}L & = & \\boldsymbol{\\nabla}s-\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, it [follows](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\")\n",
    "from the Hessian of the log-likelihood that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}L(\\boldsymbol{\\theta}; X) & = & \n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s(X,\\boldsymbol{\\theta})\n",
    "-\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ(\\boldsymbol{\\theta})\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "-\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ\n",
    "& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "+\\mathbb{E}\\left[\\left(\\boldsymbol{\\nabla}s-\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s}\\right)\\,\n",
    "\\left(\\boldsymbol{\\nabla}s-\\boldsymbol{\\mu}_{\\small\\boldsymbol{\\nabla}s}\\right)^T\n",
    "\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\\\\n",
    "\\Rightarrow \\boldsymbol{\\Sigma}_{\\small\\boldsymbol{\\nabla}s}\n",
    "& ~\\doteq~ &\n",
    "\\texttt{Var}\\left[\\boldsymbol{\\nabla}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}\\ln Ƶ\n",
    "-\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}\\boldsymbol{\\nabla}^{T}s\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8544eff",
   "metadata": {},
   "source": [
    "Note that these identities hold for gradients and Hessians with respect to both the distributional parameters\n",
    "$\\boldsymbol{\\theta}$ and also any arbitrary\n",
    "[reparameterisation](#Parameter-transformations \"Section: Parameter transformations\")\n",
    "$\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$.\n",
    "Consequently, we may always define a new variate of the form \n",
    "$Y\\doteq\\boldsymbol{\\nabla}s(X,\\boldsymbol{\\theta})$, such that\n",
    "its mean is given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_{Y} & ~=~ &\n",
    "\\mathbb{E}\\left[Y\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}\\ln Ƶ\\,,\n",
    "\\end{eqnarray}\n",
    "and its variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_Y\n",
    "& ~=~ &\n",
    "\\texttt{Var}\\left[Y\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}^{}\\boldsymbol{\\mu}_{Y}^{T}\n",
    "-\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}^{}Y^T\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "estimate $\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$\n",
    "may therefore be obtained iteratively via the approximate Newton-Raphson update\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\nabla}L\\,\\boldsymbol{\\nabla}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\\,\\Delta\\boldsymbol{\\theta} ~=~\n",
    "\\langle\\boldsymbol{\\nabla}L\\rangle\n",
    "& ~\\Rightarrow~ &\n",
    "\\left\\langle\\boldsymbol{\\Sigma}_Y\\right\\rangle\\,\\Delta\\boldsymbol{\\theta}\n",
    "~=~\n",
    "\\left\\langle Y-\\boldsymbol{\\mu}_Y\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The iterations halt when $\\Delta\\boldsymbol{\\theta}=\\mathbf{0}$, at which point\n",
    "the sample mean $\\left\\langle Y\\right\\rangle$ equals the expected mean\n",
    "$\\hat{\\boldsymbol{\\mu}}_Y=\\boldsymbol{\\mu}_Y(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$.\n",
    "Note that under the transformation $\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$,\n",
    "we may also obtain updates for $\\boldsymbol{\\eta}$ via $\\Delta\\boldsymbol{\\eta}$,\n",
    "culminating in the maximum-likelihood etimate\n",
    "$\\hat{\\boldsymbol{\\eta}}_\\texttt{ML}=\\boldsymbol{\\eta}(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e3e49",
   "metadata": {},
   "source": [
    "### Seperable dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1b191",
   "metadata": {},
   "source": [
    "We now consider a specialisation of the [general form](#General-form \"Section: General form\") which had (semi-)arbitrary,\n",
    "scalar interaction term $s(X,\\boldsymbol{\\theta})$.\n",
    "The essential idea is that the interactions between the variate $X$ and the\n",
    "parameters $\\boldsymbol{\\theta}$ are now multiplicatively separable, i.e. specified via a product. \n",
    "The simplest product form is \n",
    "$s(X,\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^{T}X$. However, more generally the nonlinear product\n",
    "$s(X,\\boldsymbol{\\theta})=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})^{T}\\mathbf{u}(X)$ is also valid. \n",
    "Note that the vector function $\\boldsymbol{\\eta}(\\cdot)$ may be thought of as defining the\n",
    "*natural* parameterisation $\\boldsymbol{\\eta}=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})$ of the distribution.\n",
    "Also, note that we now have the special property that $\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,s(X,\\boldsymbol{\\theta})=\\mathbf{u}(X)$,\n",
    "such that $Y_\\boldsymbol{\\eta}\\doteq\\mathbf{u}(X)$ may be thought of as the natural *variates*\n",
    "of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8661fdf",
   "metadata": {},
   "source": [
    "Note that distributions having the form $s(\\mathbf{x},\\boldsymbol{\\theta})=\\boldsymbol{\\eta}(\\boldsymbol{\\theta})^{T}\\mathbf{u}(\\mathbf{x})$ are regarded as belonging to **the** *exponential family*. This is a somewhat misleading and presumptuous term, given that other forms of $s(\\mathbf{x},\\boldsymbol{\\theta})$ exist, and other forms of log-likelihood not in the general form also exist, i.e. members of a more general exponential family that are not in \"the\" exponential family.\n",
    "Also note that distributions having the bilinear form $s(\\mathbf{x},\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^{T}\\mathbf{x}$\n",
    "are regarded as members of the *natural* exponential family, since the natural parameters $\\boldsymbol{\\eta}$\n",
    "are just the ordinary parameters $\\boldsymbol{\\theta}$. The other stipluation is (apparently) that we also must have the\n",
    "identity function\n",
    "$\\mathbf{u}(\\mathbf{x})=\\mathbf{x}$ to be in the natural exponential family. It is unclear what categorisation should be given to distributions having natural parameters but for which $\\mathbf{u}(\\mathbf{x})\\neq\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e46a7",
   "metadata": {},
   "source": [
    "As noted above, the special property of \"the\" exponential family is that, taking gradients with respect to\n",
    "the natural parameters\n",
    "$\\boldsymbol{\\eta}$, we have\n",
    "\\begin{eqnarray}\n",
    "Y_\\boldsymbol{\\eta}~=~\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,s(X,\\boldsymbol{\\theta})~=~\\mathbf{u}(X)\n",
    "& ~\\Rightarrow~ &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,Y_\\boldsymbol{\\eta}^{T}~=~\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\,s~=~\\mathbf{O}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "It then follows from the [previous](#General-form \"Section: General form\") section that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta}) & ~=~ & \n",
    "\\mathbb{E}\\left[Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right] ~=~ \n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\ln Z(\\boldsymbol{\\theta})\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta}) & ~=~ &\n",
    "\\texttt{Var}\\left[ Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}\\ln Z(\\boldsymbol{\\theta})\n",
    "~=~\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\,\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}^{T}(\\boldsymbol{\\theta})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "As a simplification, we may use the [results](#Parameter-transformations \"Section: Parameter transformations\") that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}~=~\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\ln Z\\,, \n",
    "& \\;\\;~\\mbox{and}~\\;\\; &\n",
    "\\boldsymbol{\\Sigma}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta})~=~\n",
    "\\mathbf{J}_\\boldsymbol{\\eta}^{-1}\\,\n",
    "~\\boldsymbol{\\nabla}_\\boldsymbol{\\theta}\\,\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}^{T}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the exact [Newton-Raphson](#Maximum-likelihood-estimation \"Section: #Maximum likelihood estimation\") update\n",
    "of the natural parameters $\\boldsymbol{\\eta}$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\Sigma}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta})\\,\\Delta\\boldsymbol{\\eta} & = & \n",
    "\\langle Y_\\boldsymbol{\\eta}\\rangle-\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}(\\boldsymbol{\\theta})\\,,\n",
    "\\end{eqnarray}\n",
    "such that the \n",
    " [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "estimate $\\hat{\\boldsymbol{\\eta}}_\\texttt{ML}=\\boldsymbol{\\eta}(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})$\n",
    "satisfies $\\hat{\\boldsymbol{\\mu}}_{\\small Y_\\boldsymbol{\\eta}}\n",
    "=\\boldsymbol{\\mu}_{\\small Y_\\boldsymbol{\\eta}}(\\hat{\\boldsymbol{\\theta}}_\\texttt{ML})=\\langle Y_\\boldsymbol{\\eta}\\rangle$.\n",
    " We shall see from a later \n",
    "[example](#Beta-distribution \"Section: Beta distribution\") that $\\langle Y_\\boldsymbol{\\eta}\\rangle$ are the sufficient statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e430e",
   "metadata": {},
   "source": [
    "### Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611f0f5",
   "metadata": {},
   "source": [
    "Consider a match between two teams, say team A and team B. Suppose further that, after considering all the evidence, our model proposes a probability $\\theta$ of team A winning. In practice, team A may either win or lose the match, or even draw the match, which we shall deal with later. Hence, we let $X=1$ indicate that team A actually won the match, and let $X=0$ indicate that team A lost the match. The variate $X$ then follows the Bernoulli distribution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & = & \\theta^{x}\\,(1-\\theta)^{1-x}\n",
    "~=~\\frac{e^{x\\ln\\frac{\\theta}{1-\\theta}}}{(1-\\theta)^{-1}}\\,.\n",
    "\\end{eqnarray}\n",
    "We therefore observe that the Bernoulli distribution is a member of \"the\" \n",
    "[exponential family](#Seperable-dependencies \"Section: Seperable dependencies\") \n",
    "with natural parameter\n",
    "\\begin{eqnarray}\n",
    "\\eta & \\doteq & \\ln\\frac{\\theta}{1-\\theta}~\\doteq~\\sigma^{-1}(\\theta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\sigma^{-1}(\\cdot)$ is the *logit* function, and its inverse is the  *logistic* (sigmoid)\n",
    "function $\\sigma(\\eta)\\doteq(1+e^{-\\eta})^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20438b",
   "metadata": {},
   "source": [
    "We also see that the utility function is just the identity, $u(x)=x$, \n",
    "such that $X$ is the natural variate.\n",
    "Lastly, observe that the partition function is given by\n",
    "\\begin{eqnarray}\n",
    "Ƶ(\\theta)~=~(1-\\theta)^{-1} & ~\\Rightarrow~ & \\nabla_\\theta\\ln Ƶ~=~\\frac{1}{1-\\theta}\\,.\n",
    "\\end{eqnarray}\n",
    "Given the reparameterisation $\\eta=\\sigma^{-1}(\\theta)$, we also note that the\n",
    "[Jacobian](#Parameter-transformations \"Section: Parameter transformations\") of this transformation is given by\n",
    "\\begin{eqnarray}\n",
    "J_\\eta & = & \\nabla_\\theta\\,\\eta~=~\\frac{1}{\\theta\\,(1-\\theta)}\n",
    "\\\\\n",
    "\\Rightarrow \\nabla_\\eta\\ln Ƶ & = & J_\\eta^{-1}\\nabla_\\theta\\ln Ƶ~=~\\theta\\,,\n",
    "\\end{eqnarray}\n",
    "from which it [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that\n",
    "the distributional mean is given by\n",
    "\\begin{eqnarray}\n",
    "\\mu & = & \\mathbb{E}[X\\mid\\theta]~=~\\nabla_\\eta\\ln Ƶ~=~\\theta\\,.\n",
    "\\end{eqnarray}\n",
    "We now see that the logit function $\\sigma^{-1}(\\cdot)$ is the\n",
    "natural [link function](#Regression-modelling \"Section: Regression modelling\")\n",
    "for the Bernoulli distribution, since $\\eta=\\sigma^{-1}(\\mu)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9479744",
   "metadata": {},
   "source": [
    "Similarly, we observe that\n",
    "\\begin{eqnarray}\n",
    "\\nabla_\\eta^2\\ln Ƶ(\\theta)~=~\\nabla_\\eta\\,\\theta~=~J_\\eta^{-1}~=~\\theta\\,(1-\\theta)\\,.\n",
    "\\end{eqnarray}\n",
    "It therefore [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that\n",
    "the distributional variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_X^2 & = & \\texttt{Var}[X\\mid\\theta]~=~\\theta\\,(1-\\theta)~=~\\mu\\,(1-\\mu)\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the Bernoulli distribution (or its variate $X$) is *heteroscedastic*, since the variance is not constant\n",
    "but is instead a function of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f416143",
   "metadata": {},
   "source": [
    "It also [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that the\n",
    "[maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate of the mean\n",
    "is given by \n",
    "\\begin{eqnarray}\n",
    "\\hat{\\mu}_\\texttt{ML} & = & \\hat{\\theta}_\\texttt{ML}~=~\\langle X\\rangle\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253e1a0",
   "metadata": {},
   "source": [
    "We now return to the issue of how we deal with draws. It turns out that there are a number of good reasons to treat a draw as being half-a-win and half-a-loss.\n",
    "Hence, we take the weighted log-likelihood for a draw as being given by\n",
    "\\begin{eqnarray}\n",
    "L_\\texttt{draw}(\\theta) & \\doteq & \\frac{1}{2}\\ln p(X=1\\mid\\theta)+\\frac{1}{2}\\ln p(X=0\\mid\\theta)\n",
    "\\\\& =  &\n",
    "\\frac{1}{2}\\ln\\theta+\\frac{1}{2}\\ln\\,(1-\\theta)\n",
    "\\\\& = &\n",
    "\\ln\\left[\\theta^{\\frac{1}{2}}\\,(1-\\theta)^{\\frac{1}{2}}\\right]\n",
    "~=~\\ln p(X={}^{\\frac{1}{2}}\\mid\\theta)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we may numerically treat draws as having the value $X=\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ef957",
   "metadata": {},
   "source": [
    "Finally, we make use of the fact that $1-\\sigma(\\eta)=\\sigma(-\\eta)$, and therefore observe that we may reparameterise the Bernoulli distribution \n",
    "in terms of its natural parameter $\\eta$ as\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\eta) & = & \\sigma(-\\eta)\\,e^{\\eta x}\\,,\n",
    "\\end{eqnarray}\n",
    "which puts it into the natural exponential family. Consequently, it appears that the membership (or non-membership) of a probability distribution in a given exponential subfamily is largely determined by its parameterisation.\n",
    "Also note that under this reparameterisation, the mean and variance are now given by\n",
    "\\begin{eqnarray}\n",
    "\\mu~=~\\mathbb{E}[X\\mid\\eta]~=~\\sigma(\\eta)\\,,\n",
    "&\\;\\;\\;& \n",
    "\\sigma^2_X~=~\\texttt{Var}[X\\mid\\eta]~=~\\sigma(\\eta)\\,\\sigma(-\\eta)\\,,\n",
    "\\end{eqnarray}\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d692fd",
   "metadata": {},
   "source": [
    "### Beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771a982",
   "metadata": {},
   "source": [
    "In contrast to the [previous](#Bernoulli-distribution \"Section: Bernoulli distribution\") section,\n",
    "suppose that instead of the match between teams A and B having a fixed probability $\\theta$ of team A winning, the\n",
    "probability, now denoted by the variate $X$, is itself sampled from another distribution. For example, $X$ might be drawn from the Beta distribution\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{x^{\\alpha-1}\\,(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}\n",
    "~=~\\frac{[x(1-x)]^{-1}\\,e^{\\alpha\\ln x+\\beta\\ln(1-x)}}{B(\\alpha,\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "This distribution is also in \"the\"\n",
    "[exponential family](#Seperable-dependencies \"Section: Seperable dependencies\"), with natural parameters \n",
    "$\\boldsymbol{\\theta}=[\\alpha,\\beta]^T$, \n",
    "natural variates\n",
    "$\\mathbf{u}(X)=[Y_\\alpha,Y_\\beta]^T=[\\ln X,\\ln (1-X)]^{T}$, and\n",
    "partition function $Ƶ(\\boldsymbol{\\theta})$ given by\n",
    "\\begin{eqnarray}\n",
    "B(\\alpha,\\beta) & \\doteq & \\frac{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\Gamma(\\cdot)$ is the *gamma* function.\n",
    "It therefore [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that\n",
    "the mean of $Y_\\alpha=\\ln X$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{\\small Y_\\alpha} & ~=~ & \\mathbb{E}\\left[\\ln X\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial}{\\partial\\alpha}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi(\\alpha)-\\psi(\\alpha+\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\psi(\\cdot)$ is the *digamma* function given by \n",
    "\\begin{eqnarray}\n",
    "\\psi(z) & \\doteq & \\frac{\\Gamma'(z)}{\\Gamma(z)}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, the mean of $Y_\\beta=\\ln (1-X)$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{\\small Y_\\beta} & ~=~ & \\mathbb{E}\\left[\\ln (1-X)\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial}{\\partial\\beta}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi(\\beta)-\\psi(\\alpha+\\beta)\\,.\n",
    "\\end{eqnarray}\n",
    "For interest sake, note that if we let $Y\\doteq\\ln\\frac{X}{1-X}=\\sigma^{-1}(X)$, then we deduce that\n",
    "\\begin{eqnarray}\n",
    "\\mu_Y & ~=~ & \\mathbb{E}\\left[Y\\mid\\alpha,\\beta\\right]~=~\n",
    "\\mu_{\\small Y_\\alpha}-\\mu_{\\small Y_\\beta} \n",
    "~=~\\psi(\\alpha)-\\psi(\\beta)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04864273",
   "metadata": {},
   "source": [
    "Similarly, we find that the variance of $Y_\\alpha$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{\\small Y_\\alpha} & ~=~ & \\texttt{Var}\\left[\\ln X\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial^2}{\\partial\\alpha^2}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi'(\\alpha)-\\psi'(\\alpha+\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\psi'(\\cdot)\\doteq\\psi_1(\\cdot)$ is the *trigamma* function given by\n",
    "\\begin{eqnarray}\n",
    "\\psi_1(z) & \\doteq & \\frac{\\Gamma(z)\\,\\Gamma''(z)-\\Gamma'(z)^2}{\\Gamma(z)^2}\\,.\n",
    "\\end{eqnarray}\n",
    "Likewise, the variance of $Y_\\beta$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{\\small Y_\\beta} & ~=~ & \\texttt{Var}\\left[\\ln(1-X)\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial^2}{\\partial\\beta^2}\\ln B(\\alpha,\\beta)\n",
    "~=~\\psi'(\\beta)-\\psi'(\\alpha+\\beta)\\,,\n",
    "\\end{eqnarray}\n",
    "and the covariance between $Y_\\alpha$ and $Y_\\beta$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\alpha,Y_\\beta} & ~=~ & \\texttt{Cov}\\left[\\ln X,\\,\\ln(1-X)\\mid\\alpha,\\beta\\right] ~=~\n",
    "\\frac{\\partial^2}{\\partial\\alpha\\partial\\beta}\\ln B(\\alpha,\\beta)\n",
    "~=~-\\psi'(\\alpha+\\beta)\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88911d0",
   "metadata": {},
   "source": [
    "It seems somewhat surprising that following the defined procedure does not immediately give us the mean and variance of the variate $X$, but instead gives the mean and variance of $\\mathbf{u}(X)$. \n",
    "In fact, it turns out that $\\langle\\ln X\\rangle$ and $\\langle\\ln(1-X)\\rangle$ provide the sufficient statistics for the \n",
    "[Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution \"Wikipedia: Beta distribution\"), \n",
    "and not $\\langle X\\rangle$. \n",
    "The mean and variance of $X$ are actually given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_X & ~=~ & \\mathbb{E}[X\\mid\\alpha,\\beta] ~=~\\frac{\\alpha}{\\alpha+\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X & ~=~ & \\texttt{Var}[X\\mid\\alpha,\\beta] ~=~\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "Note that we can also reparameterise the Beta distribution is another way. If we define $\\nu\\doteq\\alpha+\\beta$, then we see that\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X ~=~ \\frac{\\mu_X\\,(1-\\mu_X)}{\\nu+1} & \\;\\;\\Rightarrow\\;\\; &\n",
    "\\nu ~=~ \\frac{\\mu_X\\,(1-\\mu_X)}{\\sigma^2_X}-1\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\alpha~=~\\mu_X\\,\\nu\\,, & \\;\\;\\;\\; & \\beta=(1-\\mu_X)\\,\\nu\\,.\n",
    "\\end{eqnarray}\n",
    "The distribution is therefore heteroscedastic, with the variance $\\sigma_X^2$ clearly being a function of \n",
    "the mean $\\mu_X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959a457",
   "metadata": {},
   "source": [
    "Yet another reparametersiation is to retain $\\alpha$ and $\\nu$, such that the distribution becomes\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\nu) & ~=~ & \n",
    "\\frac{[x(1-x)]^{-1}\\,e^{\\alpha\\ln\\frac{x}{1-x}+\\nu\\ln(1-x)}}{B(\\alpha,\\nu-\\alpha)}\\,,\n",
    "\\end{eqnarray}\n",
    "where the natural variates are now $\\mathbf{u}(X)=[Y, \\ln(1-X)]^{T}$. We therefore obtain the mean of variate $Y$ as\n",
    "\\begin{eqnarray}\n",
    "\\mu_Y & ~=~ & \\frac{\\partial}{\\partial\\alpha}\\ln B(\\alpha,\\nu-\\alpha)~=~\\psi(\\alpha)-\\psi(\\nu-\\alpha)\\,,\n",
    "\\end{eqnarray}\n",
    "as before, and the variance of $Y$ is now also obtained as\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_Y & ~=~ & \\frac{\\partial^2}{\\partial\\alpha^2}\\ln B(\\alpha,\\nu-\\alpha)~=~\\psi'(\\alpha)+\\psi'(\\nu-\\alpha)\\,.\n",
    "\\end{eqnarray}\n",
    "This should come as no surprise, since $Y=\\ln X-\\ln(1-X)$, such that\n",
    "\\begin{eqnarray}\n",
    "\\texttt{Var}[Y\\mid\\alpha,\\beta] & ~=~ & \n",
    "\\texttt{Var}[\\ln X\\mid\\alpha,\\beta]+\\texttt{Var}[\\ln(1-X)\\mid\\alpha,\\beta]-2\\,\\texttt{Cov}[\\ln X,\\,\\ln(1-X)\\mid\\alpha,\\beta]\n",
    "\\\\& ~=~ &\n",
    "\\sigma^2_{Y_\\alpha}+\\sigma^2_{Y_\\beta}-2\\,\\sigma_{Y_\\alpha, Y_\\beta}~=~\\psi'(\\alpha)+\\psi'(\\beta)\\,.\n",
    "\\end{eqnarray}\n",
    "This little exercise demonstrates that although different parameterisations might make the various calculations either easier\n",
    "or harder to obtain, they cannot alter the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f1bc2",
   "metadata": {},
   "source": [
    "Finally, we observe that the maximum likelihood solution \n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}=[\\hat{\\alpha}_\\texttt{ML},\\hat{\\beta}_\\texttt{ML}]^{T}$\n",
    "satisfies the nonlinear system of equations\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\\ln X\\right\\rangle & = & \n",
    "\\psi(\\hat{\\alpha}_\\texttt{ML})-\\psi(\\hat{\\alpha}_\\texttt{ML}+\\hat{\\beta}_\\texttt{ML})\\,,\n",
    "\\\\\n",
    "\\left\\langle\\ln(1-X)\\right\\rangle & = & \n",
    "\\psi(\\hat{\\beta}_\\texttt{ML})-\\psi(\\hat{\\alpha}_\\texttt{ML}+\\hat{\\beta}_\\texttt{ML})\\,.\n",
    "\\end{eqnarray}\n",
    "This may be solved numerically using the Newton-Raphson method with iterative parameter updates of the form\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{}\\Delta\\alpha\\\\\\Delta\\beta\\end{array}\\right]\n",
    "& = &\n",
    "\\left[\\begin{array}{}\n",
    "\\psi'(\\alpha)-\\psi'(\\alpha+\\beta) & -\\psi'(\\alpha+\\beta)\\\\\n",
    "-\\psi'(\\alpha+\\beta) & \\psi'(\\beta)-\\psi'(\\alpha+\\beta)\n",
    "\\end{array}\\right]^{-1}\\,\n",
    "\\left[\\begin{array}{}\n",
    "\\left\\langle\\ln X\\right\\rangle - \\psi(\\alpha)+\\psi(\\alpha+\\beta)\\\\\n",
    "\\left\\langle\\ln(1-X)\\right\\rangle - \\psi(\\beta)+\\psi(\\alpha+\\beta)\n",
    "\\end{array}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c1271",
   "metadata": {},
   "source": [
    "The final issue remains about what sample values of the variate $X$ are being observed in practice? We can no longer use the\n",
    "[Bernoulli](#Bernoulli-distribution \"Section: Bernoulli distribution\") values of $X=1$ for a  win and $X=0$ for a loss,\n",
    "since now $X$ is a probability. One possibility is to note that after a match between team A and team B, we might have observed team A's score $S_A$ and team B's score $S_B$. Hence, we could use the proportion $X=\\frac{S_A}{S_A+S_B}$ as a proxy measure of the probability of team A winning a similar match against team B in the future.\n",
    "\n",
    "Note that, in general, we should not expect $S_A$ and $S_B$ to be independent, since $S_A$ should increase with team A's offensive strength, and decrease with team B's defensive strength. For example, we might expect both scores to be higher in a match between poor defenders than in a match between strong defenders. However, if we do assume independence, then taking\n",
    "$X\\sim\\texttt{Beta}(\\alpha,\\beta)$ [follows](https://en.wikipedia.org/wiki/Beta_distribution \"Wikipedia: Beta distribution\")\n",
    "from $S_A\\sim\\texttt{Gamma}(\\alpha,\\gamma)$ and $S_B\\sim\\texttt{Gamma}(\\beta,\\gamma)$,\n",
    "where $\\gamma$ is a parameter common across all teams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063dea1d",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee171e1",
   "metadata": {},
   "source": [
    "We now consider the combined situation where each match has a \n",
    "[Bernoulli-distributed](#Bernoulli-distribution \"Section: Bernoulli distribution\") outcome, $X\\sim\\texttt{Bern}(\\theta)$, but where the probability $\\theta$ iteslf is [Beta-distributed](#Beta-distribution \"Section: Beta distribution\"),\n",
    "with $\\theta\\sim\\texttt{Beta}(\\alpha,\\beta)$. It therefore follows that\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & = & \\int_0^1 p(x\\mid\\theta)\\,p(\\theta\\mid\\alpha,\\beta)\\,d\\theta\n",
    "\\\\& = &\n",
    "\\int_0^1 \\theta^x(1-\\theta)^{1-x}\\,\n",
    "\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\,d\\theta\n",
    "\\\\& = &\n",
    "\\frac{1}{B(\\alpha,\\beta)}\\int_0^1 \\theta^{\\alpha+x-1}(1-\\theta)^{\\beta+1-x-1}\\,d\\theta\n",
    "\\\\& =  &\n",
    "\\frac{B(\\alpha+x,\\beta+1-x)}{B(\\alpha,\\beta)}~=~\n",
    "\\frac{\\Gamma(\\alpha+x)\\,\\Gamma(\\beta+1-x)}{\\Gamma(\\alpha+\\beta+1)}\\,\n",
    "\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\,\\Gamma(\\beta)}\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, using the fact that $\\Gamma(z+1)=z\\,\\Gamma(z)$, we deduce that\n",
    "\\begin{eqnarray}\n",
    "p(X=1\\mid\\alpha,\\beta)~=~\\frac{\\alpha}{\\alpha+\\beta}\\,, & \\;\\;\\;\\; &\n",
    "p(X=0\\mid\\alpha,\\beta)~=~\\frac{\\beta}{\\alpha+\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "P(X=x\\mid\\alpha,\\beta) & = & \\frac{\\alpha^x\\,\\beta^{1-x}}{\\alpha+\\beta}\\,.\n",
    "\\end{eqnarray}\n",
    "We also deal with draws similarly to [before](#Bernoulli-distribution \"Section: Bernoulli distribution\"), by treating them as half-a-win and half-a-loss, with weighted log-likelihood\n",
    "\\begin{eqnarray}\n",
    "L_\\texttt{draw}(\\alpha,\\beta) & \\doteq & \\frac{1}{2}\\ln p(X=1\\mid\\alpha,\\beta)+\\frac{1}{2}\\ln p(X=0\\mid\\alpha,\\beta)\n",
    "\\\\& =  &\n",
    "\\frac{1}{2}\\ln\\frac{\\alpha}{\\alpha+\\beta}+\\frac{1}{2}\\ln\\frac{\\beta}{\\alpha+\\beta}\n",
    "\\\\& = &\n",
    "\\ln\\frac{\\alpha^{\\frac{1}{2}}\\,\\beta^{\\frac{1}{2}}}{\\alpha+\\beta}\n",
    "~=~\\ln p(X={}^{\\frac{1}{2}}\\mid\\alpha,\\beta)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we may numerically treat draws as having the value $X=\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36f749",
   "metadata": {},
   "source": [
    "We may now rewrite the distribution in the form\n",
    "\\begin{eqnarray}\n",
    "p(x\\mid\\alpha,\\beta) & ~=~ & \\frac{\\beta\\,\\left(\\frac{\\alpha}{\\beta}\\right)^x}{\\alpha+\\beta}\n",
    "~=~\\frac{e^{x\\ln\\frac{\\alpha}{\\beta}}}{1+\\frac{\\alpha}{\\beta}}\\,,\n",
    "\\end{eqnarray}\n",
    "which is in \"the\" [exponential family](#Seperable-dependencies \"Section: Seperable dependencies\") with natural parameter\n",
    "$\\eta=\\ln\\frac{\\alpha}{\\beta}$. Consequently, the reparameterised distribution\n",
    "\\begin{eqnarray}\n",
    "p(x\\mid\\eta) & \\doteq & \\frac{e^{\\eta x}}{1+e^\\eta}\\,,\n",
    "\\end{eqnarray}\n",
    "is thus in the natural exponential family, and has mean\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & \\mathbb{E}[X\\mid\\eta]~=~\\frac{d}{d\\eta}\\ln(1+e^\\eta)\n",
    "~=~\\frac{e^\\eta}{1+e^\\eta}~=~\\frac{1}{1+e^{-\\eta}}\\,.\n",
    "\\end{eqnarray}\n",
    "This is just the logistic function $\\sigma(\\cdot)$, such that\n",
    "\\begin{eqnarray}\n",
    "\\mu~=~\\sigma(\\eta) & ~\\Rightarrow~ & \\eta~=~\\sigma^{-1}(\\mu)\\,.\n",
    "\\end{eqnarray}\n",
    "The [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate\n",
    "$\\hat{\\eta}_\\texttt{ML}$ therefore satisfies\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\mu}_\\texttt{ML} & ~=~ & \\sigma(\\hat{\\eta}_\\texttt{ML})=\\langle X\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, the variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_X^2 & ~=~ & \\texttt{Var}[X\\mid\\eta]~=~\\frac{d}{d\\eta}\\frac{1}{1+e^{-\\eta}}\n",
    "~=~\\frac{e^{-\\eta}}{(1+e^{-\\eta})^2}~=~\\sigma(\\eta)\\,\\sigma(-\\eta)\\,.\n",
    "\\end{eqnarray}\n",
    "In terms of the original parameters $\\alpha$ and $\\beta$, substitution of $\\eta=\\ln\\frac{\\alpha}{\\beta}$ into the above results gives\n",
    "\\begin{eqnarray}\n",
    "\\mu~=~\\frac{\\alpha}{\\alpha+\\beta}\\,, & \\;\\;\\; & \\sigma^2_X~=~\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that although the mean $\\mu$ matches that of the\n",
    "[Beta distribution](#Beta-distribution \"Section: Beta distribution\"), the variance $\\sigma^2_X$\n",
    "now differs. In fact, the variance is given by $\\sigma^2_X=\\mu(1-\\mu)$, which matches the\n",
    "[Bernoulli distribution](#Bernoulli-distribution \"Section: Bernoulli distribution\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1df3e",
   "metadata": {},
   "source": [
    "### Gamma distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9dcaaa",
   "metadata": {},
   "source": [
    "As was noted [previously](#Beta-distribution \"Section: Beta distribution\"), in a match between team A and team B, we might observe scores $S_A$ and $S_B$, respectively. These scores are non-negative and usually, but not necessarily,\n",
    "integer valued. Let $X$ be the variate denoting a team's score.\n",
    "Then given appropriate assumptions of independence between the teams' scores, $X$ might follow the Gamma distribution:\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\alpha,\\beta) & ~=~ & \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\,x^{\\alpha-1}\\,e^{-\\beta x}\n",
    "~=~\\frac{x^{-1}e^{\\alpha\\ln x-\\beta x}}{\\beta^{-\\alpha}\\,\\Gamma(\\alpha)}\\,.\n",
    "\\end{eqnarray}\n",
    "This [PDF](#Probability-distribution-functions \"Section: Probability distribution functions\") \n",
    "is in \"the\" [exponential family](#Seperable-dependencies \"Section: Seperable dependencies\") with natural parameters\n",
    "$\\boldsymbol{\\theta}=[\\alpha,\\beta]^T$, natural variates $\\mathbf{u}(X)=[Y_\\alpha,Y_\\beta]^{T}=[\\ln X, -X]^T$, and\n",
    "partition function $Ƶ(\\boldsymbol{\\theta})$ that obeys\n",
    "\\begin{eqnarray}\n",
    "\\ln Ƶ(\\boldsymbol{\\theta}) & = & -\\alpha\\ln\\beta+\\ln\\Gamma(\\alpha)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184e95a",
   "metadata": {},
   "source": [
    "It therefore [follows](#Seperable-dependencies \"Section: Seperable dependencies\") that the means are given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\alpha} & ~=~ & \\mathbb{E}[\\ln X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial}{\\partial\\alpha}\\ln Ƶ\n",
    "~=~-\\ln\\beta+\\psi(\\alpha)\\,,\n",
    "\\\\\n",
    "\\mu_{Y_\\beta} & ~=~ & \\mathbb{E}[-X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial}{\\partial\\beta}\\ln Ƶ\n",
    "~=~-\\frac{\\alpha}{\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\psi(\\cdot)$ is the *digamma* function. We note that the distributional mean is thus given by\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & \\mathbb{E}[X\\mid\\alpha,\\beta]~=~\\frac{\\alpha}{\\beta}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, the variances are given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\alpha} & ~=~ & \\texttt{Var}[\\ln X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial^2}{\\partial\\alpha^2}\\ln Ƶ\n",
    "~=~\\psi'(\\alpha)\\,,\n",
    "\\\\\n",
    "\\sigma^2_{Y_\\beta} & ~=~ & \\texttt{Var}[-X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial^2}{\\partial\\beta^2}\\ln Ƶ\n",
    "~=~\\frac{\\alpha}{\\beta^2}\\,,\n",
    "\\end{eqnarray}\n",
    "and the covariance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma_{Y_\\alpha,Y_\\beta} & ~=~ & \\texttt{Cov}[\\ln X,-X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\partial^2}{\\partial\\alpha\\partial\\beta}\\ln Ƶ\n",
    "~=~-\\frac{1}{\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "such that the distributional variance is given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_X & ~=~ & \\texttt{Var}[X\\mid\\alpha,\\beta]\n",
    "~=~\\frac{\\alpha}{\\beta^2}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c3246",
   "metadata": {},
   "source": [
    "The [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") estimate\n",
    "$\\hat{\\boldsymbol{\\theta}}_\\texttt{ML}$ therefore satistifies the equations\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle -X\\right\\rangle~=~-\\frac{\\hat{\\alpha}_\\texttt{ML}}{\\hat{\\beta}_\\texttt{ML}}\n",
    "& ~\\Rightarrow~ & \\hat{\\beta}_\\texttt{ML}~=~\\frac{\\hat{\\alpha}_\\texttt{ML}}{\\left\\langle X\\right\\rangle}\n",
    "\\,,\n",
    "\\\\\n",
    "\\left\\langle\\ln X\\right\\rangle~=~-\\ln\\hat{\\beta}_\\texttt{ML}+\\psi(\\hat{\\alpha}_\\texttt{ML})\n",
    "& ~\\Rightarrow~ & \\ln\\left\\langle X\\right\\rangle-\\left\\langle\\ln X\\right\\rangle\n",
    "~=~\\ln\\hat{\\alpha}_\\texttt{ML}-\\psi(\\hat{\\alpha}_\\texttt{ML})\\,.\n",
    "\\end{eqnarray}\n",
    "[Apparently](https://en.wikipedia.org/wiki/Gamma_distribution \"Wikipedia: Gamma distribution\"), a good initial\n",
    "estimate of $\\hat{\\alpha}_\\texttt{ML}$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\alpha} & = & \\frac{3-s+\\sqrt{(s-3)^2+24s}}{12s}\\,,\n",
    "\\end{eqnarray}\n",
    "where $s=\\ln\\left\\langle X\\right\\rangle-\\left\\langle\\ln X\\right\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c0c8f",
   "metadata": {},
   "source": [
    "## Regression modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729a18e",
   "metadata": {},
   "source": [
    "For the case of regression modelling, we now suppose that the *response* (or *dependent*) variate $X$ is *explained* by\n",
    "exogenous (or *independent*) covariates $Z$ via some regression function $\\mathbf{f}(Z,\\boldsymbol{\\phi})$\n",
    "with regression parameters $\\boldsymbol{\\phi}$. The usual [mean regression](#Mean-regression \"Section: Mean regression\") \n",
    "model is to fit $\\mathbf{f}$ to the distribution mean $\\boldsymbol{\\mu}=\\mathbb{E}[X\\mid\\boldsymbol{\\theta}]$. We may represent this symbollically by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "The usual approach is to estimate the regression parameters $\\boldsymbol{\\phi}$ that best fit the observed data using\n",
    "some form of [least-squares](#Least-squares-regression \"Section: Least-squares regression\") approach that minimises the square\n",
    "error of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250c6c9",
   "metadata": {},
   "source": [
    "One of the issues with such an approach is that the range of the regression function $\\mathbf{f}$ is usually unconstrained, especially for linear models like $\\mathbf{f}(\\mathbf{z},\\boldsymbol{\\Phi})=\\boldsymbol{\\Phi}^{T}\\mathbf{z}$.\n",
    "However, the permissible values of the mean $\\boldsymbol{\\mu}$ are usually proscribed by the PDF, for example if $X$ represents one or more proportions.\n",
    "The key innovation of generalised linear modelling (GLM) introduced by\n",
    " Nelder and Wedderburn [[1]](#Citations \"Citation [1]: Generalized Linear Models\") was to regress, not on the mean\n",
    " $\\boldsymbol{\\mu}$ itself, but instead on parameters $\\boldsymbol{\\eta}$ related by a *link* function $\\mathbf{g}$ to the mean\n",
    "via $\\boldsymbol{\\eta}=\\mathbf{g}(\\boldsymbol{\\mu})$. We shall henceforth refer to $\\boldsymbol{\\eta}$ as *link parameters*,\n",
    "although this is not standard terminology. This new relationship corresponds to the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xrightarrow{\\mathbf{g}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "Now, for $\\mathbf{f}$ to explain $\\boldsymbol{\\mu}$, we require that the link function $\\mathbf{g}$ be invertible,\n",
    "such that the inverse relationship may be represented by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "In terms of mean regression, we could also collapse this model to become\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\theta} & \\xrightarrow{\\mathbb{E}_X} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}\\circ\\,\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "which corresponds to nonlinear least-squares regression, even with a linear regression function $\\mathbf{f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1259b81",
   "metadata": {},
   "source": [
    "We now turn to the issue of how to explain the PDF parameters $\\boldsymbol{\\theta}$ in terms of the parameters $\\boldsymbol{\\mu}$, regardless of whether the mean is directly or indirectly obtained from $\\mathbf{f}(Z,\\boldsymbol{\\phi})$.\n",
    "Now, for some simple PDFs, such as the [Bernoulli](#Bernoulli-distribution \"Section: Bernoulii distribution\")\n",
    "distribution,\n",
    "the relationship between the distribution parameters $\\boldsymbol{\\theta}$ and the mean $\\boldsymbol{\\mu}$ is invertible,\n",
    "and we may therefore deduce $\\boldsymbol{\\theta}$ from\n",
    "knowledge of $\\boldsymbol{\\mu}$. In general, however, knowing $\\boldsymbol{\\mu}$ might only give us partial\n",
    "knowledge of $\\boldsymbol{\\theta}$, as is the case, for example, with the [Beta](#Beta-distribution \"Section: Beta distribution\") distribution.\n",
    "We therefore propose that the parameters $\\boldsymbol{\\theta}$ may be partitioned into two parts,\n",
    "namely $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$, \n",
    "such that implicitly $\\boldsymbol{\\mu}=\\boldsymbol{\\mu}(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$.\n",
    "Conversely, however, we now suppose that the *independent* parameters $\\boldsymbol{\\psi}$ are not obtainable from\n",
    "$\\boldsymbol{\\mu}$ and so must be estimated separately, but that the *dependent* parameters \n",
    "$\\boldsymbol{\\varphi}$ are obtainable from $\\boldsymbol{\\mu}$, given $\\boldsymbol{\\psi}$, via some implicit\n",
    "function $\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi}, \\boldsymbol{\\mu})$,\n",
    "such that now $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu}))$.\n",
    "This inverted relationship may be represented by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\varphi} & \\xleftarrow{\\boldsymbol{\\iota}^{-1}_\\boldsymbol{\\psi}} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\iota_\\boldsymbol{\\psi}(\\cdot)$ denotes some implicit, quasi-invertible function\n",
    "such that\n",
    "$\\boldsymbol{\\mu}=\\boldsymbol{\\iota}_\\boldsymbol{\\psi}(\\boldsymbol{\\varphi})=\\boldsymbol{\\mu}(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$\n",
    "and \n",
    "$\\boldsymbol{\\varphi}=\\boldsymbol{\\iota}^{-1}_\\boldsymbol{\\psi}(\\boldsymbol{\\mu})=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42934a01",
   "metadata": {},
   "source": [
    "In summary, the generalised modelling approach may be boiled down to three essential requirements:\n",
    "\n",
    "1. The mean $\\boldsymbol{\\mu}$ of the variate $X$ is a function of the\n",
    "[PDF](#Probability-distribution-functions \"Section: Probability distribution functions\")\n",
    "parameters $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$ via the expectation\n",
    "$\\boldsymbol{\\mu}=\\mathbb{E}[X\\mid\\boldsymbol{\\theta}]$, with partial inversion given implicitly via\n",
    "$\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu})$.\n",
    "2. There exists an invertable *link* function $\\mathbf{g}$ that maps $\\boldsymbol{\\mu}$ into more \"natural\" *link* parameters\n",
    "$\\boldsymbol{\\eta}$ via $\\boldsymbol{\\eta}=\\mathbf{g}(\\boldsymbol{\\mu})$.\n",
    "3. Each value of the *response* variate $X$ is sampled from a distribution for which the corresponding link parameters $\\boldsymbol{\\eta}$ are determined by a parameterised regression function $\\mathbf{f}(Z,\\boldsymbol{\\phi})$ of an exogenous covariate $Z$. \n",
    "\n",
    "We now provide brief explanations of [mean regression](#Mean-regression \"Section: Mean regression\") \n",
    " and\n",
    "[least-squares regression](#Least-squares-regression \"Section: Least-squares regression\"),\n",
    "and then go on to expand upon the above points to derive the \n",
    "[general regression](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\")\n",
    "model, and then discuss its specialisation to a \n",
    "[linear regression](#Generalised-linear-models \"Section: Generalised linear models\")\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb855f7d",
   "metadata": {},
   "source": [
    "### Mean regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081eb27",
   "metadata": {},
   "source": [
    "Consider [again](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") \n",
    "the stochastic sampling process that produces an arbitrary length-$n$ sequence of independent variables,\n",
    "$X_1, X_2, \\ldots, X_n$. However, now we drop the requirement that these variables are identically distributed.\n",
    "Instead, we suppose that (the value of) each $X_i$ is drawn from the same \n",
    "[PDF](#Probability-distribution-functions \"Section: Probability distribution functions\")\n",
    "but with  potentially different parameters $\\boldsymbol{\\theta}_i$, with individual means\n",
    "$\\boldsymbol{\\mu}_{X_i}\\doteq\\mathbb{E}\\left[X_i\\mid\\boldsymbol{\\theta}_i\\right]$\n",
    "and variances $\\boldsymbol{\\Sigma}_{X_i}\\doteq\\texttt{Var}\\left[X_i\\mid\\boldsymbol{\\theta}_i\\right]$.\n",
    "\n",
    "\n",
    "Now, if we were allowed to sample the value of variate $X_i$ multiple times, then we would expect the values\n",
    "to be displaced about the mean $\\boldsymbol{\\mu}_i$ according to\n",
    "\\begin{eqnarray}\n",
    "X_i ~=~ \\boldsymbol{\\mu}_i+\\mathbf{e}_i\n",
    "& ~\\Rightarrow~ & \\mathbf{e}_i~=~X_i-\\boldsymbol{\\mu}_i\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In this context, the displacement $\\boldsymbol{e}_i$ is called the *noise* or the measurement *error*, \n",
    "and arises due to imprecise, stochastic measurements of the unknown mean $\\boldsymbol{\\mu}_i$.\n",
    "This error has the distributional properties\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\mathbf{e}_i\\mid\\boldsymbol{\\theta}_i\\right] & = & \\mathbf{0}\\,,\n",
    "\\\\\n",
    "\\mathbb{E}\\left[\\mathbf{e}_i\\,\\mathbf{e}_i^T\\mid\\boldsymbol{\\theta}_i\\right]\n",
    "& = & \\texttt{Var}\\left[X_i\\mid\\boldsymbol{\\theta}_i\\right]~=~\\boldsymbol{\\Sigma}_{X_i}(\\boldsymbol{\\theta}_i)\\,.\n",
    "\\end{eqnarray}\n",
    "We must keep in mind that this variance is not necessarily constant, but is generally a function of the\n",
    "distributional parameters $\\boldsymbol{\\theta}_i$, especially the\n",
    "[mean](#Seperable-dependencies \"Section: Seperable dependencies\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a83f8",
   "metadata": {},
   "source": [
    "Next, we suppose that associated with each *response* (or *dependent*) variate $X_i$ is a corresponding *exogenous*\n",
    "(or *independent*) covariate $Z_i$. We futher suppose that $Z_i$ explains the mean $\\boldsymbol{\\mu}_i$ of $X_i$ via a parameterised regression function of the form\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\mu}_i & = & \\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\mathbf{r}_i\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{r}_i$ is a called the *residual* or *error* of fit. Note that, in contrast to the measurement error, the residual arises due to error in approximating the true mean $\\boldsymbol{\\mu}_i$ with an estimating function\n",
    "$\\hat{\\boldsymbol{\\mu}}_i=\\mathbf{f}(Z_,\\boldsymbol{\\phi})$. However, since the mean $\\boldsymbol{\\mu}_i$ is actually unknown, then we may (conceptually) encode our uncertainty about its true value into another PDF, such that the mean of this PDF obeys\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\boldsymbol{\\mu}_i\\mid Z_i,\\boldsymbol{\\phi}] ~=~\n",
    "\\hat{\\boldsymbol{\\mu}}_i~=~\\mathbf{f}(Z_i,\\boldsymbol{\\phi})\n",
    "& ~\\Rightarrow~ & \n",
    "\\mathbb{E}[\\mathbf{r}_i\\mid Z_i,\\boldsymbol{\\phi}] ~=~ \\mathbf{0}\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, this PDF has a variance that measures our uncertainty, such that\n",
    "\\begin{eqnarray}\n",
    "\\texttt{Var}[\\boldsymbol{\\mu}_i\\mid Z_i,\\boldsymbol{\\phi}] & ~=~ &\n",
    "\\mathbb{E}[\\mathbf{r}_i^{}\\,\\mathbf{r}_i^T\\mid Z_i,\\boldsymbol{\\phi}]\n",
    "~=~\\boldsymbol{\\Sigma_{\\mathbf{r}_i}}(Z_i,\\boldsymbol{\\phi})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this variance is generally a function of both the covariate $Z_i$ and the regression parameters\n",
    "$\\boldsymbol{\\phi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c565f9",
   "metadata": {},
   "source": [
    "Combining the two distributions, we now obtain the regression model\n",
    "\\begin{eqnarray}\n",
    "X_i & = & \\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\boldsymbol{\\varepsilon}_i\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\boldsymbol{\\varepsilon}_i=\\mathbf{e}_i+\\mathbf{r}_i$ combines both measurement error and fitting error, and hence may be considered as either an error or a residual. Consequently, we deduce that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\boldsymbol{\\varepsilon}_i\\mid\\boldsymbol{\\theta}_i, Z_i, \\boldsymbol{\\phi}]\n",
    "& = & \\mathbf{0}\\,,\n",
    "\\\\\n",
    "\\mathbb{E}[\\boldsymbol{\\varepsilon}_i\\,\\boldsymbol{\\varepsilon}_i^T\\mid\\boldsymbol{\\theta}_i, Z_i, \\boldsymbol{\\phi}] & = & \n",
    "\\boldsymbol{\\Sigma}_{X_i}(\\boldsymbol{\\theta}_i)\n",
    "+\\boldsymbol{\\Sigma}_{\\mathbf{r}_i}(Z_i,\\boldsymbol{\\phi})\n",
    "~\\doteq~\\boldsymbol{\\Sigma}_i(\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi})\\,,\n",
    "\\end{eqnarray}\n",
    "on the assumption that the residual $\\mathbf{r}_i$ is independent of the noise $\\mathbf{e}_i$.\n",
    "In practice, we do not know the value of $\\boldsymbol{\\Sigma}_i$, and consequently must either estimate it from\n",
    "the empirical distribution of $\\boldsymbol{\\varepsilon}_i$, or else approximate it, for example by\n",
    "$\\boldsymbol{\\Sigma}_{X_i}$, which corresponds to assuming $\\boldsymbol{\\Sigma}_{\\mathbf{r}_i}=\\mathbf{O}$,\n",
    "i.e. being extremely certain of the regression function $\\mathbf{f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88a6e0",
   "metadata": {},
   "source": [
    "### Least-squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96656835",
   "metadata": {},
   "source": [
    "Recall from the [previous](#Mean-regresssion \"Section: Mean regresssion\") section\n",
    "that we are considering the regression model\n",
    "\\begin{eqnarray}\n",
    "X_i~=~\\boldsymbol{\\mu}_i+\\mathbf{e}_i\\,, & \\;\\; \\boldsymbol{\\mu}_i~=~\\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\mathbf{r}_i\n",
    "& ~\\Rightarrow~\n",
    "X_i~=~\\mathbf{f}(Z_i,\\boldsymbol{\\phi})+\\boldsymbol{\\varepsilon}_i\\,.\n",
    "\\end{eqnarray}\n",
    "In order to fit the regression function $\\mathbf{f}$ to observed data \n",
    "$\\mathbf{X}\\doteq(X_i)_{i=1}^{n}$ and $\\mathbf{Z}\\doteq(Z_i)_{i=1}^{n}$,\n",
    "we first redefine the\n",
    "sample mean [operator](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") $\\langle\\cdot\\rangle$ to include the covariate $Z$. Consequently, \n",
    "the sample average of an arbitrary function $\\mathbf{g}(X,Z,\\ldots)$ is now given by\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mathbf{g}\\rangle(\\ldots) & \\doteq &\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{g}(X_i,Z_i,\\ldots)\\,,\n",
    "\\end{eqnarray}\n",
    "where the ellipsis \"$\\ldots$\" represents arbitrary parameters that do not vary with $X$ or $Z$.\n",
    "Note that all parameters and variables that depend upon $X_i$ and/or $Z_i$ must also be indexed in this summation,\n",
    "e.g. the residual variance $\\boldsymbol{\\Sigma}_i$. If it becomes necessary to explicitly distinguish between constants and functions of $X_i$ and $Z_i$, then we may retain the subscript, e.g. $\\langle\\boldsymbol{\\Sigma}_i\\boldsymbol{\\phi}\\rangle$. Also note that the sample mean may still be treated as a function of $\\mathbf{X}$ and $\\mathbf{Z}$, when they are considered as variables rather than known, sampled values.\n",
    "This is useful for computing expectations of sample means, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5fd40",
   "metadata": {},
   "source": [
    "The fitting process requires estimating the best value of the function parameters $\\boldsymbol{\\phi}$ that\n",
    "minimises the overall error of fit. The *ordinary* least-squares (OLS) method is to minimise the mean of the squared lengths of the residuals, namely\n",
    "$S(\\boldsymbol{\\phi})=\\left\\langle\\boldsymbol{\\varepsilon}^{T}\\boldsymbol{\\varepsilon}\\right\\rangle$.\n",
    "However, use of OLS makes some implicit assumptions, in particular that:\n",
    "1. The residuals are independent of each other.\n",
    "2. All residuals are equally important.\n",
    "3. The elements of each residual are independent of each other.\n",
    "4. The elements of each residual are equally important.\n",
    "\n",
    "Only the first assumption of residual independence really holds. In practice, if the residuals are independent, then the plot of the fitted residuals against the covariate $Z$ should appear randomly distributed. However, if a pattern appears then the particular choice of the regression function $\\mathbf{f}$ must be reconsidered. The assumption that the elements of each residual $\\boldsymbol{\\varepsilon}_i$ are independent does not hold in general, since\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\boldsymbol{\\varepsilon}_i^{T}\\,\\boldsymbol{\\varepsilon}_i\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\n",
    "& ~=~ &\n",
    "\\mathbb{E}[\n",
    "\\texttt{trace}\\left(\\boldsymbol{\\varepsilon}_i\\,\\boldsymbol{\\varepsilon}_i^T\\right)\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\n",
    "~=~\\texttt{trace}\\left(\\boldsymbol{\\Sigma}_i\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the assumptions of equal importance do not hold either, since a residual (or an element of a residual)\n",
    "with higher variance has higher uncertainty associated with its fit, and hence should be assigned less weight.\n",
    "It turns out that weighting in inverse proportion to the variance is a good idea.\n",
    "Note that use of OLS corresponds to assuming constant and equal variances of the form\n",
    "$\\boldsymbol{\\Sigma}_i=\\mathbf{I}\\,\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af18f98",
   "metadata": {},
   "source": [
    "We can solve both problems of element-wise non-independence and unequal weighting of residuals and elements by applying a so-called\n",
    "*whitening transformation* that decouples within-residual correlations and standardises the variances. This transformation takes the form\n",
    "\\begin{eqnarray}\n",
    "\\tilde{\\boldsymbol{\\varepsilon}}_i & ~\\doteq~ & \n",
    "\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\boldsymbol{\\varepsilon}_i\n",
    "~=~\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\left[X_i-\\mathbf{f}(Z_i,\\boldsymbol{\\phi})\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "such that \n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[\\tilde{\\boldsymbol{\\varepsilon}}_i^{T}\\,\\tilde{\\boldsymbol{\\varepsilon}}_i\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\n",
    "& ~=~ &\n",
    "\\texttt{trace}\\left(\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\,\\mathbb{E}[\n",
    "\\boldsymbol{\\varepsilon}_i\\,\\boldsymbol{\\varepsilon}_i^T\n",
    "\\mid\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi}]\\,\n",
    "\\boldsymbol{\\Sigma}_i^{-\\frac{1}{2}}\\right)\n",
    "~=~\\texttt{trace}\\left(\\mathbf{I}\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the *weighted* least-squares (WLS) method is to minimise\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}) & ~=~ &\n",
    "\\left\\langle\\tilde{\\boldsymbol{\\varepsilon}}^{T}\\,\\tilde{\\boldsymbol{\\varepsilon}}\\right\\rangle\n",
    "~=~\\left\\langle\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]^{T}\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]\n",
    "\\right\\rangle\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6082e",
   "metadata": {},
   "source": [
    "However, we [recall](#Mean-regresssion \"Section: Mean regresssion\") that \n",
    "$\\boldsymbol{\\Sigma}_i=\\boldsymbol{\\Sigma}_i(\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi})$ is a function\n",
    "of $\\boldsymbol{\\phi}$. Consequently, WLS is typically a nonlinear problem even when $\\mathbf{f}$ is a linear function of $\\boldsymbol{\\phi}$. To overcome this difficulty, we \n",
    "use an iterative approximation where we\n",
    "evaluate $\\boldsymbol{\\Sigma}_i$ at the\n",
    "previous estimate $\\boldsymbol{\\phi}$, but evaluate $\\mathbf{f}$ at the new estimate\n",
    "$\\boldsymbol{\\phi}'$, resulting in\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}, \\boldsymbol{\\phi}') & ~=~ &\n",
    "\\left\\langle\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi}')\\right]^{T}\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\theta},Z,\\boldsymbol{\\phi})\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi}')\\right]\n",
    "\\right\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we substitute the Taylor series approximation\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{f}(Z,\\boldsymbol{\\phi}') & ~\\approx~ &\n",
    "\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "+\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\phi}' & ~=~ \\boldsymbol{\\phi}+\\Delta\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "to obtain the new approximation\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}, \\Delta\\boldsymbol{\\phi}) & ~=~ &\n",
    "\\left\\langle\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "-\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\right]^{T}\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "-\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\right]\n",
    "\\right\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, we take the gradient with respect to $\\Delta\\boldsymbol{\\phi}$ to obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_{\\Delta\\boldsymbol{\\phi}}S & ~=~ &\n",
    "-2\\,\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "-\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}(Z,\\boldsymbol{\\phi})\\,\\Delta\\boldsymbol{\\phi}\n",
    "\\right]\n",
    "\\right\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "which vanishes when\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\boldsymbol{\\nabla}^T_\\boldsymbol{\\phi}\\mathbf{f}(Z,\\boldsymbol{\\phi})\n",
    "\\right\\rangle\\,\\Delta\\boldsymbol{\\phi}\n",
    "& ~=~ &\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]\n",
    "\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "This is the nonlinear form of the *iteratively reweighted* least-squares (IRLS) method, due to the fact that the\n",
    "variance $\\boldsymbol{\\Sigma}_i(\\boldsymbol{\\theta}_i,Z_i,\\boldsymbol{\\phi})$ needs to be\n",
    "re-evaluated after every update of the parameter estimate $\\boldsymbol{\\phi}$.\n",
    "Note that the iterations will cease when $\\Delta\\boldsymbol{\\phi}=\\mathbf{0}$, at which point the \n",
    "solutions satisfies\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\Sigma}^{-1}\\,\n",
    "\\left[X-\\mathbf{f}(Z,\\boldsymbol{\\phi})\\right]\n",
    "\\right\\rangle & ~=~ & \\mathbf{0}\\,.\n",
    "\\end{eqnarray}\n",
    "This latter is just the solution to $\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}S=\\mathbf{0}$ from the original WLS formulation, on the assumption that $\\boldsymbol{\\Sigma}$ is held constant for the update and recomputed after the update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f92a",
   "metadata": {},
   "source": [
    "### Generalised nonlinear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7b62f",
   "metadata": {},
   "source": [
    "[Previously](#Mean-regresssion \"Section: Mean regresssion\"), we considered the\n",
    "regression of the mean $\\boldsymbol{\\mu}$ via some parameterised function\n",
    "$\\mathbf{f}(Z,\\boldsymbol{\\phi})$ of the covariate $Z$.\n",
    "In my opinion, the key contribution of Nelder and Wedderburn\n",
    "[[1]](#Citations \"Citation [1]: Generalized Linear Models\")\n",
    "to generalised linear modelling (GLM) lies in the fact that we may instead apply regression, not to $\\boldsymbol{\\mu}$,\n",
    "but to some more natural parameterisation $\\boldsymbol{\\eta}=\\mathbf{g}(\\boldsymbol{\\mu})$,\n",
    "where $\\mathbf{g}(\\cdot)$ is known as the *link* function.\n",
    "We may therefore depict the resulting relationships by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\varphi} & \\xrightarrow{\\boldsymbol{\\iota}_\\boldsymbol{\\psi}} & \\boldsymbol{\\mu}\n",
    "\\xrightarrow{\\mathbf{g}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,,\n",
    "\\end{eqnarray}\n",
    "which may further be interpreted as meaning that the parameters $\\boldsymbol{\\varphi}$ and $\\boldsymbol{\\phi}$ are conditionally independent given $\\eta$ (and $Z$ and $\\boldsymbol{\\psi}$).\n",
    "It therefore [follows](#Parameter-transformations \"Section: Parameter transformations\") that\n",
    "the gradient of the log-likelihood $L$ with respect to the regression parameters $\\boldsymbol{\\phi}$ takes the form\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}L & = &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\boldsymbol{\\eta}^T\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\n",
    "~=~\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}(Z,\\boldsymbol{\\phi})\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L(\\boldsymbol{\\theta}; X)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that although the last term on the right-hand side appears to be a function only of $X$ and\n",
    "$\\boldsymbol{\\theta}$, we must remember that $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$, and that $\\boldsymbol{\\varphi}$ is related to $\\boldsymbol{\\eta}=\\mathbf{f}(Z,\\boldsymbol{\\phi})$ via the graphical model above.\n",
    "In fact, in order to compute the gradient of the log-likelihood $L$ with respect to the parameters $\\boldsymbol{\\eta}$,\n",
    "we must use the fact that the link function $\\mathbf{g}$ is invertible, as represented\n",
    "by the graphical model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\varphi} & \\xleftarrow{\\boldsymbol{\\iota}_\\boldsymbol{\\psi}^{-1}} & \\boldsymbol{\\mu}\n",
    "\\xleftarrow{\\mathbf{g}^{-1}}\\boldsymbol{\\eta}\n",
    "\\xleftarrow{\\mathbf{f}_Z}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L & = &\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}\\boldsymbol{\\mu}^T\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\mu}\\boldsymbol{\\varphi}^T\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\varphi}L\n",
    "\\\\& = &\n",
    "\\left[\\frac{\\partial\\mathbf{g}^T}{\\partial\\boldsymbol{\\mu}}\\right]^{-1}\\,\n",
    "\\left[\\frac{\\partial\\boldsymbol{\\mu}^T}{\\partial\\boldsymbol{\\varphi}}\\right]^{-1}\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\varphi}L\\,.\n",
    "\\end{eqnarray}\n",
    "Implicitly, we may therefore suppose that $\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu}(Z,\\boldsymbol{\\phi}))$,\n",
    "such that \n",
    "$\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu}(Z,\\boldsymbol{\\phi})))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a53582",
   "metadata": {},
   "source": [
    "The requisite \n",
    "[expectations](#Expectations-and-log-likelihoods \"Section: Expectations and log-likelihoods\")\n",
    "are therefore given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}L\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}\\right]\n",
    "& = & \\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~ \\mathbf{0}\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}L\n",
    "\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}\\right]\n",
    "& = & \\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, the [maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\")\n",
    "estimate $\\hat{\\boldsymbol{\\phi}}_\\texttt{ML}$ may be obtained iteratively via updates of the form\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}\n",
    "\\right\\rangle\\,\\Delta\\boldsymbol{\\phi} & = &\n",
    "\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213ce2f",
   "metadata": {},
   "source": [
    "It must be noted that the natural *link* parameters $\\boldsymbol{\\eta}$ here are not necessarily the same as the natural *distributional* parameters used\n",
    "[previously](#Seperable-dependencies \"Section: Seperable dependencies\"), also denoted as $\\boldsymbol{\\eta}$. In general they are not the same.\n",
    "Despite this, we [may](#General-form \"Section: General form\")\n",
    "still introduce a new variate $Y_\\boldsymbol{\\eta}$ as a function of $X$ (and $\\boldsymbol{\\theta})$,\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L~=~Y_\\boldsymbol{\\eta}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}\n",
    "& ~\\Rightarrow~ & \n",
    "\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}~=~\\mathbb{E}\\left[Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The variance is then directly obtained as\n",
    "\\begin{eqnarray}\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\eta}} & ~=~ &\n",
    "\\texttt{Var}\\left[Y_\\boldsymbol{\\eta}\\mid\\boldsymbol{\\theta}\\right]\n",
    "~=~\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}L\\,\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\eta}^{T}L\\mid\\boldsymbol{\\theta}\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "It then follows that the update for the parameter $\\boldsymbol{\\phi}$ is given by\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\\,\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\eta}}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\mathbf{f}\n",
    "\\right\\rangle\\,\\Delta\\boldsymbol{\\phi} & = &\n",
    "\\left\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\mathbf{f}^{T}\n",
    "\\left[Y_\\boldsymbol{\\eta}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\eta}}\\right]\n",
    "\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "We may observe the similarity (and dissimilarity) with the nonlinear \n",
    "[IRLS](#Least-squares-regression \"Section: Least-squares regression\") update equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2468a6",
   "metadata": {},
   "source": [
    "Lastly, the maximum likelihood estimate $\\hat{\\boldsymbol{\\psi}}_\\texttt{ML}$\n",
    "is directly obtained via iterative updates of the form\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\\mathbb{E}\\left[\\boldsymbol{\\nabla}_\\boldsymbol{\\psi}L\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\psi}^{T}L\n",
    "\\mid\\boldsymbol{\\theta}\\right]\\right\\rangle\n",
    "\\,\\Delta\\boldsymbol{\\psi} & = &\n",
    "\\langle\\boldsymbol{\\nabla}_\\boldsymbol{\\psi}L\\rangle\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since $\\boldsymbol{\\psi}$ is an independent parameter that governs the distribution. Similarly to above, the parameter\n",
    "$\\boldsymbol{\\psi}$ does not have to be a natural distributional parameter. Regardless, we may also define the variates\n",
    "$Y_\\boldsymbol{\\psi}$ as functions of $X$, with corresponding means\n",
    "$\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\psi}}$ and covariances $\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}$,\n",
    "such that the update for parameter $\\boldsymbol{\\psi}$ becomes\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}\\right\\rangle\n",
    "\\,\\Delta\\boldsymbol{\\psi} & = &\n",
    "\\left\\langle Y_\\boldsymbol{\\psi}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\psi}}\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Despite this simplified form, recall that $\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\psi}}$ and  $\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}$ are still indirectly functions of $Z$ and $\\boldsymbol{\\phi}$\n",
    "via $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu}(Z,\\boldsymbol{\\phi})))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c6928",
   "metadata": {},
   "source": [
    "### Generalised linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343d836",
   "metadata": {},
   "source": [
    "Generalised linear modelling (GLM) now follows immediately from\n",
    "[nonlinear modelling](#Generalised-nonlinear-models \"Section: Generalised nonlinear models\").\n",
    "In theory, the most general linear model is given by\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{f}(\\mathbf{z},\\boldsymbol{\\Phi}) & \\doteq & \\boldsymbol{\\Phi}^T\\,\\mathbf{z}\\,,\n",
    "\\end{eqnarray}\n",
    "where the variate $Z$ is multi-dimensional (and may also include a constant component), and the parameters\n",
    "$\\boldsymbol{\\Phi}$ take the form of a matrix. However, we may subsequently separate $\\mathbf{f}$ into its components by\n",
    "considering independent scalar functions parameterised by each column \n",
    " of $\\boldsymbol{\\Phi}=[\\boldsymbol{\\phi}_i]$.\n",
    " We may therefore assume, without loss of generality, that the regression function takes the simple, scalar form \n",
    "\\begin{eqnarray}\n",
    "\\eta & = & f(Z,\\boldsymbol{\\phi}) ~\\doteq~ \\boldsymbol{\\phi}^T\\,Z\n",
    "~=~Z^{T}\\boldsymbol{\\phi}\\,.\n",
    "\\end{eqnarray}\n",
    "Note, however, that in the full vector case, we would still need to reconstruct $\\boldsymbol{\\eta}=[\\eta_k]$\n",
    "to obtain $\\boldsymbol{\\mu}=\\mathbf{g}^{-1}(\\boldsymbol{\\eta})$ and thus \n",
    "$\\boldsymbol{\\varphi}=\\boldsymbol{\\varphi}(\\boldsymbol{\\psi},\\boldsymbol{\\mu})$.\n",
    "\n",
    "The gradient of the log-likelihood is now given by\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}L  & ~=~ & Z\\,\\nabla_\\eta L~=~Z\\,\\left[Y_\\eta-\\mu_{Y_\\eta}\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "such that the parameter update\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\phi}' & = & \\boldsymbol{\\phi} + \\Delta\\boldsymbol{\\phi}\n",
    "\\end{eqnarray}\n",
    "is obtained via the solution of\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\sigma^2_{Y_\\eta}\\,\n",
    "ZZ^T\n",
    "\\right\\rangle\\,\\Delta\\boldsymbol{\\phi} & = &\n",
    "\\left\\langle \\left(Y_\\eta-\\mu_{Y_\\eta}\\right)\\,Z\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, applying the above matrix (on the left-hand side) directly to the updated parameters $\\boldsymbol{\\phi}'$ gives\n",
    "\\begin{eqnarray}\n",
    "\\langle\\sigma^2_{Y_\\eta}\\,ZZ^T\\rangle\\,\\boldsymbol{\\phi}' & ~=~ &\n",
    "\\langle\\sigma^2_{Y_\\eta}\\,ZZ^T\\rangle\\,\\boldsymbol{\\phi}+\\langle\\sigma^2_{Y_\\eta}\\,ZZ^T\\rangle\\,\n",
    "\\Delta\\boldsymbol{\\phi}\n",
    "\\\\& = & \\langle\\sigma^2_{Y_\\eta}\\,Z\\,\\eta\\rangle+\n",
    "\\left\\langle \\left(Y_\\eta-\\mu_{Y_\\eta}\\right)\\,Z\\right\\rangle\n",
    "\\\\&=& \\left\\langle \\left(Y_\\eta-\\mu_{Y_\\eta}+\\eta\\,\\sigma^2_{Y_\\eta}\\right)\\,Z\\right\\rangle\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "since $\\eta=Z^{T}\\boldsymbol{\\phi}$.\n",
    "Similarly, for the parameter update\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\psi}' & = & \\boldsymbol{\\psi} + \\Delta\\boldsymbol{\\psi}\\,,\n",
    "\\end{eqnarray}\n",
    "we obtain\n",
    "\\begin{eqnarray}\n",
    "\\langle\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}\\rangle\\,\\boldsymbol{\\psi}' & ~=~ &\n",
    "\\langle\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}\\rangle\\,\\boldsymbol{\\psi}+\n",
    "\\langle\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}\\rangle\\,\\Delta\\boldsymbol{\\psi}\n",
    "\\\\& = &\n",
    "\\left\\langle \n",
    "Y_\\boldsymbol{\\psi}-\\boldsymbol{\\mu}_{Y_\\boldsymbol{\\psi}}+\\boldsymbol{\\Sigma}_{Y_\\boldsymbol{\\psi}}\\,\\boldsymbol{\\psi}\n",
    "\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3940e",
   "metadata": {},
   "source": [
    "For comparison, let us now take another look at [least-squares](#Least-squares-regression \"Section: Least-squares regression\")\n",
    "regression. We start with WLS using the weighted residual $\\tilde{\\varepsilon}$ such that the square error is\n",
    "\\begin{eqnarray}\n",
    "S(\\boldsymbol{\\phi}) & ~=~ & \\left\\langle\\tilde{\\varepsilon}^2\\right\\rangle\n",
    "~=~\\left\\langle\\frac{(Y_\\eta-\\mu_{Y_\\eta})^2}{\\sigma_{Y_\\eta}^2}\\right\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we make use of IRLS by temporarily holding $\\sigma^2_{Y_\\eta}$ constant and taking the gradient of \n",
    "$\\mu=\\sigma(\\eta)$ with $\\eta=Z^T\\boldsymbol{\\phi}$,\n",
    "giving the approximate gradient\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\,S & ~\\approx~ & \n",
    "-2\\,\\left\\langle\\frac{Y_\\eta-\\mu_{Y_\\eta}}{\\sigma^2_{Y_\\eta}}\\,\\frac{\\partial\\mu}{\\partial\\eta}\\,\\frac{\\partial\\eta}{\\partial\\boldsymbol{\\phi}}\n",
    "\\right\\rangle\n",
    "~=~-2\\,\\left\\langle(Y_\\eta-\\mu_{\\small Y_\\eta})\\,\n",
    "\\frac{\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]}{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Next, we temporarily hold $\\nabla_\\eta Y_\\eta$ constant and take a second gradient of $\\mu$ to obtain\n",
    "the approximate Hessian as\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}\\,\\boldsymbol{\\nabla}_\\boldsymbol{\\phi}^{T}\\,S & ~\\approx~ & \n",
    "2\\left\\langle\\frac{\\partial\\mu}{\\partial\\eta}\\,\\frac{\\partial\\eta}{\\partial\\boldsymbol{\\phi}}\\,\n",
    "\\frac{\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]}{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z^T\\right\\rangle\n",
    "\\\\\n",
    "&=& 2\\left\\langle\n",
    "\\frac{\\left(\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]\\right)^2}\n",
    "{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\,Z^T\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Finally, we use the Newton-Raphson approach to obtain the approximate update\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\n",
    "\\frac{\\left(\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]\\right)^2}\n",
    "{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\,Z^T\\right\\rangle\n",
    "\\,\\Delta\\boldsymbol{\\phi} & ~=~ &\n",
    "\\left\\langle(Y_\\eta-\\mu_{Y_\\eta})\\,\n",
    "\\frac{\\sigma^2_{Y_\\eta}+\\mathbb{E}[\\nabla_\\eta Y_\\eta\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}]}{\\sigma^2_{Y_\\eta}}\n",
    "\\,Z\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "We observe that this [IRLS](#Least-squares-regression \"Section: Least-squares regression\") \n",
    "method only becomes identical to the \n",
    "[maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") \n",
    "approach in the special case where $\\nabla_\\eta Y_\\eta=0$, i.e.\n",
    "where $Y_\\eta$ is a natural variate and thus $\\eta$ is both a natural parameter and a link parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78776941",
   "metadata": {},
   "source": [
    "### Bernoulli regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e638201",
   "metadata": {},
   "source": [
    "We [recall](#Bernoulli-distribution \"Section: Bernoulli distribution\")\n",
    "that the Bernoulli distribution parameterised by $\\theta$ has mean $\\mu=\\theta$ and variance\n",
    "$\\sigma_X^2=\\theta\\,(1-\\theta)$. We also recall that the natural parameterisation of the distribution is\n",
    "$\\eta=\\sigma^{-1}(\\theta)$, where $\\sigma^{-1}(\\cdot)$ is the logit function, such that the natural gradient of\n",
    "the log-likelihood $L$ is $\\nabla_\\eta L=X-\\mu$. Also, since $\\mu=\\theta$, we find that \n",
    "$\\eta=\\sigma^{-1}(\\mu)$ is the natural parameterisation of the mean $\\mu$.\n",
    "Therefore, the dependent parameter is $\\varphi=\\theta$, and there is no independent parameter $\\psi$.\n",
    "\n",
    "We now assume, for convenience, the linear regression function $\\eta=Z^T\\boldsymbol{\\phi}$.\n",
    "Hence, from the [previous](#Generalised-linear-models \"Section: Generalised linear models\") section,\n",
    "we find that \n",
    "the iterative parameter update for $\\boldsymbol{\\phi}$ therefore takes either the form\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mu\\,(1-\\mu)\\,ZZ^T\\rangle\\,\\Delta\\boldsymbol{\\phi}\n",
    "& ~=~ & \\langle (X-\\mu)\\,Z\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "or\n",
    "\\begin{eqnarray}\n",
    "\\langle\\mu\\,(1-\\mu)\\,ZZ^T\\rangle\\,\\boldsymbol{\\phi}' \n",
    "& ~=~ & \\langle \\left[X-\\mu+\\eta\\,\\mu\\,(1-\\mu)\\right]\\,Z\\rangle\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\eta=Z^T\\boldsymbol{\\phi}$ and $\\mu=\\sigma(\\eta)$ are both functions of $Z$ and of the current\n",
    "regression parameter estimate $\\boldsymbol{\\phi}$.\n",
    "Also note that here $X$ is the natuiral variate, and $\\eta$ is both the natural parameter and the link parameter. Hence,\n",
    "from the [previous](#Generalised-linear-models \"Section: Generalised linear models\") section, we see that the Bernoulli\n",
    "distribution is one of the special cases where the \n",
    "[maximum likelihood](#Maximum-likelihood-estimation \"Section: Maximum likelihood estimation\") approach\n",
    "is identical to the \n",
    "[least-squares](#Least-squares-regression \"Section: Least-squares regression\") approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498d654",
   "metadata": {},
   "source": [
    "### Beta regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf7e66",
   "metadata": {},
   "source": [
    "Recall that the [Beta distribution](#Beta-distribution \"Section: Beta distribution\")\n",
    "has natural parameters $\\alpha$ and $\\beta$, with mean\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & \\frac{\\alpha}{\\alpha+\\beta}~=~\\frac{1}{1+\\frac{\\beta}{\\alpha}}\n",
    "~=~\\frac{1}{1+e^{-\\ln\\frac{\\alpha}{\\beta}}}\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the natural link parameter is given by\n",
    "\\begin{eqnarray}\n",
    "\\eta & ~=~ & \\ln\\frac{\\alpha}{\\beta}~=~\\sigma^{-1}(\\mu)\\,.\n",
    "\\end{eqnarray}\n",
    "We may invert this relationship to obtain either $\\alpha=\\beta\\,e^\\eta$ or $\\beta=\\alpha\\,e^{-\\eta}$, such that the\n",
    "required log-likelihood gradient is given by\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial\\eta} & ~=~ & \n",
    "\\frac{\\partial L}{\\partial\\alpha}\\,\\frac{\\partial\\alpha}{\\partial\\eta}\n",
    "+\\frac{\\partial L}{\\partial\\beta}\\,\\frac{\\partial\\beta}{\\partial\\eta}\n",
    "~=~\\alpha\\,(Y_\\alpha-\\mu_{Y_\\alpha})-\\beta\\,(Y_\\beta-\\mu_{Y_\\beta})\\,,\n",
    "\\end{eqnarray}\n",
    "where $Y_\\alpha=\\ln X$ and $Y_\\beta=\\ln(1-X)$ are the natural variates, and $\\mu_{Y_\\alpha}$ and\n",
    "$\\mu_{Y_\\beta}$ are their respective means. \n",
    "We may therefore define the new *link* variate $Y_\\eta$ and its mean $\\mu_{Y_\\eta}$ as\n",
    "\\begin{eqnarray}\n",
    "Y_\\eta~\\doteq~\\alpha\\,Y_\\alpha-\\beta\\,Y_\\beta & \\;\\;~\\mbox{and}~\\;\\;\n",
    "\\mu_{Y_\\eta}~=~\\alpha\\,\\mu_{Y_\\alpha}-\\beta\\,\\mu_{Y_\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Its variance is therefore\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\eta} & ~=~ & \\texttt{Var}[Y_\\eta\\mid\\alpha,\\beta]\n",
    "~=~\\alpha^2\\,\\sigma^2_{Y_\\alpha}+\\beta^2\\,\\sigma^2_{Y_\\beta}-2\\alpha\\beta\\,\\sigma_{\\small Y_\\alpha,Y_\\beta}\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the update for the [generalised linear regression](#Generalised-linear-models \"Section: Generalised linear models\")\n",
    "parameter $\\boldsymbol{\\phi}$ is just\n",
    "\\begin{eqnarray}\n",
    "\\langle\\sigma^2_{Y_\\eta}\\,Z\\,Z^T\\rangle\\,\\Delta\\boldsymbol{\\phi} & ~=~ &\n",
    "\\left\\langle \\left(Y_\\eta-\\mu_{Y_\\eta}\\right)\\,Z\\right\\rangle\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec872847",
   "metadata": {},
   "source": [
    "Note that since we cannot recover both $\\alpha$ and $\\beta$ from $\\eta$ (or $\\mu$), we must choose one of these parameters to be the independent parameter $\\psi$, and the other to be the dependent parameter $\\varphi$.\n",
    "Following Kieschnick and McCullough [[3]](#Citations \n",
    "\"Citation [3]: Regression analysis of variates observed on $(0, 1)$\"),\n",
    "we choose $\\psi=\\alpha$ and $\\varphi=\\beta=\\alpha\\,e^{-\\eta}$.\n",
    "Hence, the iterative update for $\\alpha$ takes the form\n",
    "\\begin{eqnarray}\n",
    "\\langle\\sigma^2_{Y_\\alpha}\\rangle\\,\\Delta\\alpha & = &\n",
    "\\left\\langle Y_\\alpha-\\mu_{Y_\\alpha}\\right\\rangle\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $Y_\\alpha\\doteq\\ln X$. The mean $\\mu_{Y_\\alpha}$ and variance $\\sigma^2_{Y_\\alpha}$ of\n",
    "the variate $Y_\\alpha$ may be obtained from \n",
    "the [Beta distribution](#Beta-distribution \"Section: Beta distribution\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3531a4",
   "metadata": {},
   "source": [
    "### Beta-Bernoulli regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec99e3c",
   "metadata": {},
   "source": [
    "Recall that the [Beta-Bernoulli distribution](#Beta-Bernoulli-distribution \"Section: Beta-Bernoulli distribution\")\n",
    "  has mean $\\mu$ and variance $\\sigma^2_X$ given by\n",
    "\\begin{eqnarray}\n",
    "\\mu~=~\\frac{\\alpha}{\\alpha+\\beta}\\,, & \\;\\;\\mbox{and}\\;\\; & \\sigma^2_X~=~\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively, \n",
    "along with a natural parameter $\\eta$ and link function $g(\\mu)$ given by\n",
    "\\begin{eqnarray}\n",
    "\\eta & ~=~ & \\ln\\frac{\\alpha}{\\beta}~=~\\sigma^{-1}(\\mu)\\,.\n",
    "\\end{eqnarray}\n",
    "We therefore deduce that $\\eta$ is also the natural link parameter for a logit link function.\n",
    "\n",
    "Following [Beta regression](#Beta-regression \"Section: Beta regression\"), \n",
    "we take $\\psi=\\alpha$ as the independent parameter, and $\\varphi=\\beta=\\alpha\\,e^{-\\eta}$ as the dependent parameter. Consequently, we derive that\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial\\alpha} & ~=~ &\n",
    "\\frac{\\partial\\eta}{\\partial\\alpha}\\frac{\\partial L}{\\partial\\eta}\n",
    "~=~\\frac{1}{\\alpha}\\,(X-\\mu)\\,,\n",
    "\\end{eqnarray}\n",
    "which results in a variate $Y_\\alpha$ and corresponding mean $\\mu_{Y_\\alpha}$ given by\n",
    "\\begin{eqnarray}\n",
    "Y_\\alpha~\\doteq~\\frac{X}{\\alpha}\\,, & \\;\\;\\mbox{and}\\;\\; & \n",
    "\\mu_{Y_\\alpha}~=~\\frac{\\mu}{\\alpha}~=~\\frac{1}{\\alpha+\\beta}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively.\n",
    "The variance $\\sigma^2_{Y_\\alpha}$ of $Y_\\alpha$ is then given by\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\alpha} & ~=~ & \\frac{\\sigma^2_X}{\\alpha^2}~=~\n",
    "\\frac{\\beta}{\\alpha\\,(\\alpha+\\beta)^2}\\,.\n",
    "\\end{eqnarray}\n",
    "The iterative update equation for the Beta parameter $\\alpha$ is thus\n",
    "\\begin{eqnarray}\n",
    "\\langle\\sigma^2_{Y_\\alpha}\\rangle\\,\\Delta\\alpha & ~=~ & \n",
    "\\langle Y_\\alpha-\\mu_\\alpha\\rangle\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\Delta\\alpha & = & \\alpha\\,\\frac{\\langle X-\\mu\\rangle}{\\langle\\mu(1-\\mu)\\rangle}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d43a4",
   "metadata": {},
   "source": [
    "Finally, since $\\eta$ is both the link parameter and the natural parameter, then the iterative\n",
    "[GLM](#Generalised-linear-models \"Section: Generalised linear models\")\n",
    "update for regression parameters $\\boldsymbol{\\phi}$ is just\n",
    "\\begin{eqnarray}\n",
    "\\left\\langle\\sigma^2_X\\,ZZ^T\\right\\rangle\\,\\Delta\\boldsymbol{\\phi} & ~=~ &\n",
    "\\left\\langle\\mu(1-\\mu)\\,ZZ^T\\right\\rangle\\,\\Delta\\boldsymbol{\\phi}~=~\n",
    "\\left\\langle(X-\\mu)\\,Z\\right\\rangle\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this is just the same update equation as for plain\n",
    "[Bernoulli regression](#Bernoulli-regression \"Section: Bernoulli regression\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73bb07",
   "metadata": {},
   "source": [
    "### Gamma regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12d40b",
   "metadata": {},
   "source": [
    "We [recall](#Gamma-distribution \"Section: Gamma distribution\") that the Gamma distribution has natural parameters\n",
    "$\\alpha$ and $\\beta$, and natural variates $Y_\\alpha=\\ln X$ and $Y_\\beta=-X$. The mean and variance of the distribution are\n",
    "given by\n",
    "\\begin{eqnarray}\n",
    "\\mu~=~\\frac{\\alpha}{\\beta}\\,, & \\;\\;~\\mbox{and}~\\;\\; & \\sigma^2_X~=~\\frac{\\alpha}{\\beta^2}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Following the\n",
    "[previous](#Beta-regression \"Section: Beta regression\") section, we take the link variate to be\n",
    "\\begin{eqnarray}\n",
    "\\eta~=~\\ln\\frac{\\alpha}{\\beta}~=~\\ln\\mu & ~\\Rightarrow~ & \\mu~=~e^\\eta\\,,\n",
    "\\end{eqnarray}\n",
    "which gives $\\alpha=\\beta\\;e^{\\eta}$ and $\\beta=\\alpha\\,e^{-\\eta}$ as before. Thus, we again obtain\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial\\eta} & ~=~ &\n",
    "\\frac{\\partial L}{\\partial\\alpha}\\,\\frac{\\partial\\alpha}{\\partial\\eta}\n",
    "+\\frac{\\partial L}{\\partial\\beta}\\,\\frac{\\partial\\beta}{\\partial\\eta}\n",
    "~=~\\alpha\\,(Y_\\alpha-\\mu_{Y_\\alpha})-\\beta\\,(Y_\\beta-\\mu_{Y_\\beta})\\,,\n",
    "\\end{eqnarray}\n",
    "leading to the new link variate \n",
    "\\begin{eqnarray}\n",
    "Y_\\eta & ~\\doteq~ & \\alpha\\,Y_\\alpha-\\beta\\,Y_\\beta~=~\\alpha\\,\\ln X+\\beta\\,X\\,,\n",
    "\\end{eqnarray}\n",
    "whith mean $\\mu_{Y_\\eta}$ and variance $\\sigma^2_{Y_\\eta}$ given by\n",
    "\\begin{eqnarray}\n",
    "\\mu_{Y_\\eta} & ~=~ & \\alpha\\,\\mu_{Y_\\alpha}-\\beta\\,\\mu_{Y_\\beta}\n",
    "~=~\\alpha\\,\\left[1+\\psi(\\alpha)-\\ln\\beta\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\sigma^2_{Y_\\eta} & ~=~ & \\alpha^2\\,\\sigma^2_{Y_\\alpha}+\\beta^2\\,\\sigma^2_{Y_\\beta}\n",
    "-2\\alpha\\beta\\,\\sigma_{\\small Y_\\alpha,Y_\\beta}\n",
    "~=~\\alpha\\,\\left[3+\\alpha\\psi'(\\alpha)\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f957ab",
   "metadata": {},
   "source": [
    "Now, following an [earlier](#Beta-distribution \"Section: Beta distribution\") suggestion,\n",
    "we suppose that parameter $\\beta$ is common across all observations. Hence, we take the independent parameter to be $\\psi=\\beta$, and the dependent parameter to be $\\varphi=\\alpha=\\beta\\,e^{\\eta}$.\n",
    "Consequently, the independent parameter update equation is now given by\n",
    "\\begin{eqnarray}\n",
    "\\langle\\sigma^2_{Y_\\beta}\\rangle\\,\\Delta\\beta & = &\n",
    "\\left\\langle Y_\\beta-\\mu_{Y_\\beta}\\right\\rangle\n",
    "\\\\\n",
    "\\Rightarrow \\Delta\\beta & = &\n",
    "-\\beta\\,\\frac{\\left\\langle X-\\mu\\right\\rangle}{\\langle\\mu\\rangle}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where the estimate of $\\beta$ is held constant at each iteration, but $\\mu=e^{\\eta}$ with\n",
    "$\\eta=Z^T\\boldsymbol{\\phi}$ varying.\n",
    "The linear regression parameter update equation is\n",
    "\\begin{eqnarray}\n",
    "\\langle\\sigma^2_{Y_\\eta}\\,ZZ^T\\rangle\\,\\Delta\\boldsymbol{\\phi} & ~=~ &\n",
    "\\left\\langle\\left(Y_\\eta-\\mu_{Y_\\eta}\\right)\\,Z\\right\\rangle\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a0460",
   "metadata": {},
   "source": [
    "## Regression modelling revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743136a",
   "metadata": {},
   "source": [
    "An alternative approach to deriving probabilistic regression models is offered by\n",
    "Bergtold et al. [[4]](#Citations \"Citation [4]: Bernoulli Regression Models\"). \n",
    "Under this \"*probabilistic reduction*\" framework, both the dependent variate $X\\in\\mathcal{X}$ and the independent covariate(s) $Z\\in\\mathcal{Z}$ are jointly modelled.\n",
    "For such a joint density to exist, it must be able to be factored into conditionals into two different ways, namely\n",
    "\\begin{eqnarray}\n",
    "p(X,Z\\mid\\Theta) & ~=~ & p(X\\mid\\boldsymbol{\\theta})\\,p(Z\\mid X,\\Theta)\n",
    "~=~ p(Z\\mid\\boldsymbol{\\pi})\\,p(X\\mid \\boldsymbol{\\psi},Z,\\boldsymbol{\\phi})\\,,\n",
    "\\end{eqnarray}\n",
    "where we have retained our previous notation of $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$ representing the parameters of the distribution of $X$, and $\\boldsymbol{\\phi}$ representing the regression function parameters. Here the arbitrary parameters $\\Theta$ include both $\\boldsymbol{\\theta}$ and\n",
    "$\\boldsymbol{\\psi}$, as well as the new parameters $\\boldsymbol{\\pi}$ and\n",
    "any other parameters required to define the conditional distribution of $Z$.\n",
    "\n",
    "We may now combine both factorisations together and rearrange terms to obtain\n",
    "\\begin{eqnarray}\n",
    "p(X\\mid\\boldsymbol{\\theta}) & = &\n",
    "\\frac{p(Z\\mid\\boldsymbol{\\pi})\\,p(X\\mid \\boldsymbol{\\psi},Z,\\boldsymbol{\\phi})}{p(Z\\mid X,\\Theta)}\\,.\n",
    "\\end{eqnarray}\n",
    "It is of prime importance to note that the apparent dependency of the right-hand side on $Z$ must actually cancel out, since the left-hand side is purely a function of $X$.\n",
    "The next step is to consider two distinct values, say $x_0,x_1\\in\\mathcal{X}$, and evaluating the above formula at both points. Taking the ratio then gives\n",
    "\\begin{eqnarray}\n",
    "\\frac{p(X=x_1\\mid\\boldsymbol{\\theta})}{p(X=x_0\\mid\\boldsymbol{\\theta})}\n",
    "& ~=~ & \n",
    "\\frac{p(X=x_1\\mid \\boldsymbol{\\psi},Z,\\boldsymbol{\\phi})\\,p(Z\\mid X=x_0,\\Theta)}\n",
    "{p(X=x_0\\mid \\boldsymbol{\\psi},Z,\\boldsymbol{\\phi})\\,p(Z\\mid X=x_1,\\Theta)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "The final two steps of \"probabilistic reduction\" depend upon the distributions of $X$ and $Z$, respectively, as we shall see in the following sections.\n",
    "In particular, since $\\boldsymbol{\\theta}=(\\boldsymbol{\\psi},\\boldsymbol{\\varphi})$,\n",
    "we find it convenient to assume that the conditional distribution $p(X\\mid \\boldsymbol{\\psi},Z,\\boldsymbol{\\phi})$ takes the same functional form as the marginal distribution $p(X\\mid\\boldsymbol{\\theta})$, namely that\n",
    "\\begin{eqnarray}\n",
    "p(X\\mid\\boldsymbol{\\psi},Z,\\boldsymbol{\\phi}) & ~\\doteq~ &\n",
    "p(X\\mid\\boldsymbol{\\psi},\\mathbf{F}(Z;\\boldsymbol{\\phi}))\\,,\n",
    "\\end{eqnarray}\n",
    "which corresponds to the regression model\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{\\varphi} & ~\\doteq~ & \\mathbf{F}(Z;\\boldsymbol{\\phi})\\,.\n",
    "\\end{eqnarray}\n",
    "Similarly, we also make the simplifying assumption that the conditional distribution\n",
    "$p(Z\\mid X=x_i,\\Theta)$ takes the same functional form as the marginal distribution\n",
    "$p(Z\\mid\\boldsymbol{\\pi})$. In particular, we \n",
    "suppose that $X=x_0$ acts to select specific parameters $\\boldsymbol{\\pi}_0\\in\\Theta$, and similarly $X=x_1$ selects alternative parameters $\\boldsymbol{\\pi}_1\\in\\Theta$. Consequently, we take\n",
    "\\begin{eqnarray}\n",
    "p(Z\\mid X=x_i,\\Theta) & ~\\doteq~ & p(Z\\mid\\boldsymbol{\\pi}_i)\\,,\n",
    "\\end{eqnarray}\n",
    "for $i=0,1$. Under these conditions, the ratio formula above reduces to\n",
    "\\begin{eqnarray}\n",
    "\\frac{p(X=x_1\\mid\\boldsymbol{\\theta})}{p(X=x_0\\mid\\boldsymbol{\\theta})}\n",
    "& ~=~ &\n",
    "\\left.\n",
    "\\frac{p(X=x_1\\mid \\boldsymbol{\\psi},\\mathbf{F}(Z;\\boldsymbol{\\phi}))}{p(Z\\mid\\boldsymbol{\\pi}_1)}\n",
    "\\right/\n",
    "\\frac{p(X=x_0\\mid \\boldsymbol{\\psi},\\mathbf{F}(Z;\\boldsymbol{\\phi}))}{p(Z\\mid\\boldsymbol{\\pi}_0)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "As we noted above, the dependence on $Z$ of the right-hand side must vanish in practice.\n",
    "Consequently, the correct choice of $\\mathbf{F}(Z,\\boldsymbol{\\phi})$ is determined\n",
    "directly by the assumed (marginal) distribution of $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59913880",
   "metadata": {},
   "source": [
    "### Bernoulli regression (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc599b76",
   "metadata": {},
   "source": [
    "The [Bernoulli distribution](#Bernoulli-distribution \"Section: Bernoulli distribution\")\n",
    "has domain $\\mathcal{X}=\\{0,1\\}$ and (marginal) density\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid\\theta) & ~=~ & \\theta^x\\,(1-\\theta)^{1-x}\\,.\n",
    "\\end{eqnarray}\n",
    "It therefore makes sense for the conditional distribution to take a similar form, namely\n",
    "\\begin{eqnarray}\n",
    "p(X=x\\mid Z,\\boldsymbol{\\phi}) & ~=~ & F(Z;\\boldsymbol{\\phi})^x\\,[1-F(Z;\\boldsymbol{\\phi})]^{1-x}\\,.\n",
    "\\end{eqnarray}\n",
    "Note that since the mean of the Bernoulli distribution is just $\\mu=\\theta$, this is equivalent\n",
    "to the [mean regression](#Mean-regression \"Section: Mean regression\") model\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & F(Z;\\boldsymbol{\\phi})\\,.\n",
    "\\end{eqnarray}\n",
    "We now take $x_0=0$ and $x_1=1$, such that the first ratio formula reduces to\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\theta}{1-\\theta} & ~=~ & \n",
    "\\frac{F(Z;\\boldsymbol{\\phi})\\,p(Z\\mid X=0,\\Theta)}\n",
    "{[1-F(Z;\\boldsymbol{\\phi})]\\,p(Z\\mid X=1,\\Theta)}\n",
    "\\\\\n",
    "\\Rightarrow \\ln\\frac{F(Z;\\boldsymbol{\\phi})}{1-F(Z;\\boldsymbol{\\phi})}\n",
    "& ~=~ & \\ln\\frac{\\theta}{1-\\theta} \n",
    "+ \\ln\\frac{p(Z\\mid X=1,\\Theta)}{p(Z\\mid X=0,\\Theta)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "We recognise the left-hand side as the logit transform $\\sigma^{-1}(\\cdot)$ of $F(Z;\\boldsymbol{\\phi})$,\n",
    "and hence the Bernoulli regression model takes the logistic form of\n",
    "\\begin{eqnarray}\n",
    "\\mu & ~=~ & F(Z;\\boldsymbol{\\phi}) ~=~\\sigma(f(Z;\\boldsymbol{\\phi}))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with\n",
    "\\begin{eqnarray}\n",
    "f(Z;\\boldsymbol{\\phi}) & ~\\doteq~ &\n",
    "\\ln\\frac{\\theta}{1-\\theta} + \\ln\\frac{p(Z\\mid X=1,\\Theta)}{p(Z\\mid X=0,\\Theta)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Bergtold et al. [[4]](#Citations \"Citation [4]: Bernoulli Regression Models\") make the observation that\n",
    "the first term on the right-hand side corresponds to a constant in the regression model, and that the\n",
    "correct form of regression on the covariate(s) $Z$ follows directly from the \n",
    "assumed conditional distribution $p(Z\\mid X,\\Theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786f98f",
   "metadata": {},
   "source": [
    "### Beta-distributed covariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc3d94",
   "metadata": {},
   "source": [
    "Let us suppose that a scalar covariate $Z$ represents a probability or proportion on the domain\n",
    "$\\mathcal{Z}=(0,1)$. For convenience, we might assume that each observed value of $Z$ is drawn from\n",
    "a [Beta distribution](#Beta-distribution \"Section: Beta distribution\").\n",
    "In the particular case that the response variate $X$ is \n",
    "[Bernoulli distributed](#Bernoulli-distribution \"Section: Bernoulli distribution\"), we assume that\n",
    "\\begin{eqnarray}\n",
    "p(Z=z\\mid X=x,\\Theta) & ~\\doteq~ & P(Z=z\\mid\\alpha_x,\\beta_x) ~=~ \n",
    "\\frac{z^{\\alpha_x-1}\\,(1-z)^{\\beta_x-1}}{B(\\alpha_x,\\beta_x)}\\,.\n",
    "\\end{eqnarray}\n",
    "The corresponding terms for [Bernoulli regression](#Bernoulli-regression-(again) \"Section: Bernoulli regression (again)\") are therefore obtained from\n",
    "\\begin{eqnarray}\n",
    "\\ln\\frac{p(Z=z\\mid\\alpha_1,\\beta_1)}{p(Z=z\\mid\\alpha_0,\\beta_0)}\n",
    "& ~=~ &\n",
    "(\\alpha_1-\\alpha_0)\\ln z+(\\beta_1-\\beta_0)\\ln(1-z)-\\ln\\frac{B(\\alpha_1,\\beta_1)}{B(\\alpha_0,\\beta_0)}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "such that the appropriate regression function is given by\n",
    "\\begin{eqnarray}\n",
    "f(Z;\\boldsymbol{\\phi}) & ~=~ & \\phi_0+\\phi_1\\,\\ln Z+\\phi_2\\,\\ln(1-Z)\\,.\n",
    "\\end{eqnarray}\n",
    "The corresponding predictive model is therefore given by\n",
    "\\begin{eqnarray}\n",
    "p(X=1\\mid Z,\\boldsymbol{\\phi}) & ~=~ & \\sigma(\\phi_0+\\phi_1\\,\\ln Z+\\phi_2\\,\\ln(1-Z))\\,.\n",
    "\\end{eqnarray}\n",
    "In the special case where we have theoretical reasons to suppose that $\\phi_2=-\\phi_1$, \n",
    "i.e. $\\alpha_0+\\beta_0=\\alpha_1+\\beta_1$,\n",
    "this\n",
    "reduces to the simpler form of\n",
    "\\begin{eqnarray}\n",
    "p(X=1\\mid Z,\\boldsymbol{\\phi}) & ~=~ & \\sigma(\\phi_0+\\phi_1\\,\\sigma^{-1}(Z))\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7293b",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95acf0",
   "metadata": {},
   "source": [
    "[1] J. A. Nelder and R. W. M. Wedderburn (1972), \"*Generalized Linear Models*\", J. Royal Stat. Soc. Series A, Vol. 135, No. 3, pp. 370-384.\n",
    "\n",
    "[2] M. G. Kendall and A. Stuart (1967), \"*The Advanced Theory of Statistics*\", 2nd ed., Vol. 2.\n",
    "\n",
    "[3] R. Kieschnick and B. D. McCullough (2003), \"*Regression analysis of variates observed on $(0, 1)$*\", Statistical Modelling 3(3):193-213. [[PDF]](https://journals.sagepub.com/doi/10.1191/1471082X03st053oa \"journals.sagepub.com\")\n",
    "\n",
    "[4] J. S. Bergtold, A. Spanos and E. Onukwugha (2010), \"*Bernoulli Regression Models: Revisiting the\n",
    "Specification of Statistical Models with Binary Dependent Variables*\",\n",
    "J. Choice Modelling 3(2), pp 1-28."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
