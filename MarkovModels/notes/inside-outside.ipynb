{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4a90c3",
   "metadata": {},
   "source": [
    "# Inside-Outside Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616e2d9",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f54a67",
   "metadata": {},
   "source": [
    "The seminal paper on the inside-outside algorithm is \n",
    "[(Baker)](#References \"Reference [1]: Trainable grammars for speech recognition\").\n",
    "However, \n",
    "[(Lari and Young)](#References \"Reference [3]:\n",
    "Applications of stochastic context-free grammars using the Inside-Outside algorithm\")\n",
    "claim that Baker neither specified the actual algorithm nor gave an application\n",
    "(I haven't yet found my copy of (Baker) to verify).\n",
    "\n",
    "My re-derivation here of the inside-outside algorithm (from first principles) is mostly based on \n",
    "[(Lari and Young)](#References \"Reference [2]: The estimation of stochastic context-free grammars using the inside–outside algorithm\").\n",
    "However, they normalise\n",
    "their rule probabilities differently (as discussed in a \n",
    "[later](#Rule-conditioning \"Section: Rule conditioning\") section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21245e",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b74791",
   "metadata": {},
   "source": [
    "Consider a stochastic process that outputs an arbitrary-length\n",
    "sequence $\\mathbf{Y}$ of discrete symbols, where $\\mathbf{Y}$ may be considered as a vector random variable. \n",
    "In the language domain, each symbol might be a character\n",
    "(e.g. alphanumeric, space or punctuation), or a word with the sequence of words forming\n",
    "a sentence (with the inclusion of spacing and punctuation in natural language). \n",
    "In general, we let each symbol be a token from a finite set \n",
    "$\\mathcal{Y}=\\{\\nu_1,\\nu_2,\\ldots\\}$ of discrete tokens.\n",
    "\n",
    "Thus, we suppose that we have observed an entire sequence $\\mathbf{y}=(y_1,y_2,\\ldots)$ of such tokens.\n",
    "Note that once we know that $|\\mathbf{y}|=T$, say, then $\\mathbf{y}\\in\\mathcal{Y}^T$,\n",
    "and we may consider either the complete\n",
    "sequence $\\mathbf{y}=\\mathbf{y}_{1:T}\\doteq(y_1,y_2,\\ldots,y_T)$, or any contiguous subsequence\n",
    "$\\mathbf{y}_{s:t}\\doteq(y_s,y_{s+1},\\ldots,y_t)$ of tokens.\n",
    "Given $\\mathbf{y}$, there are two main problems of interest.\n",
    "\n",
    "One problem is to construct the (or a) most probable \n",
    "[parse](#Bottom-up-parsing-model \"Section: Bottom-up parsing model\") \n",
    "structure $\\mathbf{s}$\n",
    "that best explains this sequence, given a grammar $\\mathcal{G}$ of plausible substructures or rules. \n",
    "Thus, if we let $\\mathbf{S}$ be a vector random variable representing each possible\n",
    "parse structure $\\mathbf{s}$, then parsing seeks $\\mathbf{s}$ to maximise \n",
    "$P(\\mathbf{S}=\\mathbf{s}\\mid\\mathbf{Y}=\\mathbf{y})$.\n",
    "\n",
    "The other problem of interest is to induce the grammar $\\mathcal{G}$\n",
    "directly from a corpus $\\mathbb{Y}=(\\mathbf{y}^{(1)},\\mathbf{y}^{(2)},\\ldots)$ of observed sequences,\n",
    "using a [generative](#Top-down-generative-model \"Section: Top-down generative model\")\n",
    "model of the form $P(\\mathbf{S}=\\mathbf{s},\\mathbf{Y}=\\mathbf{y})$.\n",
    "This is the purpose of the\n",
    "*inside-outside* algorithm - to automatically extract the various rules and their probabilities for a stochastic, context-free grammar in Chomsky normal form.\n",
    "Consequently, only unary rules may produce tokens, and unary rules must produce only tokens.\n",
    "This means that unary *type-raising* rules are forbidden (which also simplifies parsing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e08af",
   "metadata": {},
   "source": [
    "### Rule conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7253e",
   "metadata": {},
   "source": [
    "We assume that all rules are conditioned on the antecedent, and that summing over all possible\n",
    "consequents results in a value of unity.\n",
    "Thus, a stochastic unary rule of the form\n",
    "$\\mathtt{A}\\rightarrow\\mathtt{B}$\n",
    "has probability $P(\\mathtt{B}\\mid\\mathtt{A})$, such that $\\sum_{b}P(b\\mid\\mathtt{A})=1$. \n",
    "Likewise, a top-down binary rule of the form\n",
    "$\\mathtt{A}\\rightarrow\\mathtt{B}\\oplus\\mathtt{C}$\n",
    "has probability\n",
    "$P(\\mathtt{B},\\mathtt{C}\\mid\\mathtt{A})$, such that\n",
    "$\\sum_{b,c}P(b,c\\mid\\mathtt{A})=1$. \n",
    "Conversely, the bottom-up (reversed) binary rule $\\mathtt{B}\\oplus\\mathtt{C}\\rightarrow\\mathtt{A}$\n",
    "has probability\n",
    "$P(\\mathtt{A}\\mid\\mathtt{B},\\mathtt{C})$, such that $\\sum_{a}P(a\\mid\\mathtt{B}\\oplus\\mathtt{C})=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac824cf2",
   "metadata": {},
   "source": [
    "In contrast, [(Lari and Young)](#References \"Reference [2]: The estimation of stochastic context-free grammars using the inside–outside algorithm\")\n",
    "normalise their generative rules to satisfy\n",
    "$\\sum_d P(A\\rightarrow d)+\\sum_{b,c}P(A\\rightarrow b\\oplus c)=1$.\n",
    "This makes sense in a \n",
    "[generative](#Top-down-generative-model \"Section: Top-down generative model\")\n",
    "process **prior** to observation, since once the process has entered an antecedent state $\\texttt{A}$, any of its consequents (unary token or binary states) could potentially be generated. \n",
    "\n",
    "However, the situation changes **after** we have observed sequence $\\mathbf{y}$, since we now have more information. For example, if the grammar $\\mathcal{G}$ always starts with a single, unique\n",
    "[root state](#Root-states \"Section: Root states\") $\\texttt{S}$ and we know that $|\\mathbf{y}|>1$, then\n",
    "it is inconsistent to consider any unary rules $\\mathtt{S}\\rightarrow\\nu_m$.\n",
    "In other words, for observed sequence $\\mathbf{y}$ and potential\n",
    "[parse](#Bottom-up-parsing-model \"Section: Bottom-up parsing model\")\n",
    "structure $\\mathbf{s}$, we must have both $P(\\mathbf{S}=\\mathbf{s},\\mathbf{Y}=\\mathbf{y})=0$\n",
    "and $P(\\mathbf{S}=\\mathbf{s}\\mid\\mathbf{Y}=\\mathbf{y})=0$ \n",
    "whenever $\\mathbf{s}$ is inconsistent with $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156978e",
   "metadata": {},
   "source": [
    "Put another way, the assumption of Chomsky normal form means that each token must be spanned by a unary rule,\n",
    "and every (contiguous) subsequence of two or more tokens must be spanned by a binary rule. Since the decision of a unary or binary rule is deterministic given the span $s:t$, it no longer makes sense to ignore this conditioning information. Hence, we make unary and binary rules separately sum to unity over the consequents given the antecedent, instead of making them collectively sum to unity like (Lari and Young).\n",
    "\n",
    "In practice, either normalisation scheme will work the same once we condition on $\\mathbf{y}$, e.g.\n",
    "$P(\\mathbf{S}=\\mathbf{s}\\mid\\mathbf{Y}=\\mathbf{y})$ and its various marginalisations over components of\n",
    "$\\mathbf{S}$. However, the choice affects the viability of \n",
    "[parsing](#Parsing-Versus-Generation \"Section: Parsing Versus Generation\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f7c58",
   "metadata": {},
   "source": [
    "### Comparison with a HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae406614",
   "metadata": {},
   "source": [
    "Before proceeding to the details of the inside-outside algorithm, let us first\n",
    "briefly summarise some basic details about modelling a sequence with a hidden Markov model (HMM).\n",
    "Consider a simple Markov chain transitioning from stage $t$ to stage $t+1$, for $t=1,2,\\ldots,\\infty$,\n",
    "such as that shown in the figure below for the sentence \"*The cat sat on the mat.*\". \n",
    "<img src=\"HMM_sentence.png\" title=\"Hidden Markov Model of a sentence\" width=\"50%\">\n",
    "\n",
    "Let the hidden state at stage $t$ be represented by the variable $S_t$, taking one of a finite set of possible values $\\mathcal{S}=\\{\\sigma_1,\\sigma_2,\\ldots\\}$.\n",
    "For the simple HMM shown above, the allowable transitions between stages $t$ and $t+1$ have probabilities\n",
    "$P(S_{t+1}=\\sigma_j\\mid S_t=\\sigma_i)$, for $\\sigma_i,\\sigma_j\\in\\mathcal{S}$. These transition probabilities are typically assumed to be \n",
    "invariant with respect to the stages, and are usually represented by the matrix $\\mathbf{A}=[a_{ij}]$,\n",
    "such that $a_{ij}\\doteq P(S_{t+1}=\\sigma_j\\mid S_t=\\sigma_i)$\n",
    "and $\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}a_{ij}=1$.\n",
    "Hence, the stage-invariant transitions\n",
    "correspond to context-free rules of the form $\\sigma_i\\rightarrow\\sigma_j$.\n",
    "\n",
    "Similarly, let the output at stage $t$ be represented by the variable $Y_t\\in\\mathcal{Y}$.\n",
    "This output is generated by the HMM process via the state $S_t$\n",
    "with probability $P(Y_t=\\nu_m\\mid S_t=\\sigma_i)$\n",
    "for $\\sigma_i\\in\\mathcal{S}$ and $\\nu_m\\in\\mathcal{Y}$.\n",
    "Once again, these *emission* probabilities are assumed to be invariant with respect to the stage, and are\n",
    "usually\n",
    "represented by the matrix $\\mathbf{B}=[b_{im}]$, such that $b_{im}\\doteq P(Y_t=\\nu_m\\mid S_t=\\sigma_i)$\n",
    "and $\\sum_{m=1}^{\\left|\\mathcal{Y}\\right|}b_{im}=1$.\n",
    "Hence, the stage-invariant emissions\n",
    "correspond to context-free rules of the form $\\sigma_i\\rightarrow\\nu_m$.\n",
    "\n",
    "Given these transition and emission probabilities, the HMM is a generative model that specifies the joint probabilities\n",
    "$P(\\mathbf{S}=\\mathbf{s},\\mathbf{Y}=\\mathbf{y})$. Since the hidden states $\\mathbf{s}$ are unknown in general,\n",
    "the key quantity is the likelihood of the observed sequence, namely\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}=\\mathbf{y}) & = & \\sum_{\\mathbf{s}\\in\\mathcal{S}^{|\\mathbf{y}|}}\n",
    "P(\\mathbf{S}=\\mathbf{s},\\mathbf{Y}=\\mathbf{y})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "An efficient method for calculating this probability is offered by\n",
    "the *forward-backward* algorithm.\n",
    "The forward pass along the sequence $\\mathbf{y}_{1:T}$ recursively computes\n",
    "joint probabilities of the form\n",
    "\\begin{eqnarray}\n",
    "\\alpha_t(i) & \\doteq & P(\\mathbf{Y}_{1:t}=\\mathbf{y}_{1:t},S_t=\\sigma_i)\\,,\n",
    "\\end{eqnarray}\n",
    "for $t=1,2,\\ldots,T$.\n",
    "Conversely, the backward pass recursively computes\n",
    "conditional probabilities of the form\n",
    "\\begin{eqnarray}\n",
    "\\beta_t(i) & \\doteq & P(\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T}\\mid S_t=\\sigma_i)\\,,\n",
    "\\end{eqnarray}\n",
    "for $t=T,T-1,\\ldots,1$, where $\\mathbf{y}_{t:s}=()$ for $t>s$ by definition.\n",
    "\n",
    "The required Markov property is that the past (i.e. $\\mathbf{y}_{1:t}$)\n",
    "is conditionally independent of the future (i.e. $\\mathbf{y}_{t+1:T}$) given\n",
    "the present (i.e. $S_t=\\sigma_i$). It follows that the likelihood of the\n",
    "observed sequence $\\mathbf{y}_{1:T}$ is\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}) & = &\n",
    "\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_t=\\sigma_i)\n",
    "\\\\& = &\n",
    "\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{1:t}=\\mathbf{y}_{1:t},S_t=\\sigma_i)\\,\n",
    "P(\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T}\\mid S_t=\\sigma_i)\n",
    "\\\\& = &\n",
    "\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\alpha_t(i)\\,\\beta_t(i)\\,,\n",
    "\\end{eqnarray}\n",
    "for any and every $t=1,2,\\ldots,T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6467a8",
   "metadata": {},
   "source": [
    "The standard forward formulation computes $\\alpha_t(i)$ via the recursion\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{t}(i) & = & \n",
    "P(\\mathbf{Y}_{1:t}=\\mathbf{y}_{1:t},S_t=\\sigma_i)\n",
    "\\\\& = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{1:t-1}=\\mathbf{y}_{1:t-1},S_{t-1}=\\sigma_j)\\,\n",
    "P(S_t=\\sigma_i\\mid S_{t-1}=\\sigma_j)\\,P(Y_t=y_t\\mid S_t=\\sigma_i)\n",
    "\\\\& = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\\alpha_{t-1}(j)\\,a_{ji}\\,b_{i,y_{t}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for $t=2,3,\\ldots,T$. The initial step is given by\n",
    "\\begin{eqnarray}\n",
    "\\alpha_1(i) & = & P(Y_1=y_1,S_1=\\sigma_i) ~=~ \\iota_i\\,b_{i,y_1}\\,,\n",
    "\\end{eqnarray}\n",
    "where the *initial* state probabilities\n",
    "$\\iota_i\\doteq P(S_1=\\sigma_i)$ satisfy $\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\\iota_i=1$.\n",
    "\n",
    "Similarly, the standard backward formulation computes $\\beta_t(i)$ via the recursion\n",
    "\\begin{eqnarray}\n",
    "\\beta_t(i) & = & \n",
    "P(\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T}\\mid S_t=\\sigma_i)\n",
    "\\\\& = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(S_{t+1}=\\sigma_j\\mid S_t=\\sigma_i)\\,P(Y_{t+1}=y_{t+1}\\mid S_{t+1}=\\sigma_j)\\,\n",
    "P(\\mathbf{Y}_{t+2:T}=\\mathbf{y}_{t+2:T}\\mid S_{t+1}=\\sigma_j)\n",
    "\\\\& = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}a_{ij}\\,b_{j,y_{t+1}}\\,\\beta_{t+1}(j)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for $t=T-1,T-2,\\ldots,1$.\n",
    "Note that this standard formulation allows each sequence to potentially continue forever,\n",
    "permitted by the weak constraint that $\\beta_T(i)=1$ for every $\\sigma_i\\in\\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae9b07",
   "metadata": {},
   "source": [
    "However, when modelling complete sequences, such as those obtained from sentences of a grammar,\n",
    "it is necessary to include the fact that such sequences have both an implicit start, \n",
    "here already represented via $\\iota_i$,\n",
    "and also an implict end.\n",
    "For this purpose, we introduce start-of-sequence and end-of-sequence \n",
    "[markers](#Marker-states \"Section: Marker states\"), $\\triangleleft$ and\n",
    "$\\triangleright$ respectively, such that the initial probabilities are now \n",
    "$\\iota_i\\doteq P(S_1=\\sigma_1\\mid S_0=\\triangleleft)$.\n",
    "Additionally, we introduce the *final* or *termination* probabilities \n",
    "$\\tau_i\\doteq P(S_{T+1}=\\triangleright\\mid S_T=\\sigma_i)$, with the new constraint that\n",
    "$\\beta_T(i)=\\tau_i$.\n",
    "\n",
    "Note that in some formulations, the pseudo-states $\\triangleleft$ and $\\triangleright$ are implicitly added \n",
    "into both the state space $\\mathcal{S}$ and the transition matrix $\\mathbf{A}$. However, I prefer to explicitly keep them separate.\n",
    "Hence, let us generalise the final probabilities to be stage-invariant, such that\n",
    "$\\tau_i\\doteq P(S_{t+1}=\\triangleright\\mid S_t=\\sigma_i)$ is now the probability that the subsequence\n",
    "$\\mathbf{y}_{1:t}$ terminates immediately after stage $t$ from state $S_t=\\sigma_i$.\n",
    "Conversely, let $\\bar{\\tau}_i\\doteq 1-\\tau_i$ be the probability that the subsequence $\\mathbf{y}_{1:t}$ does\n",
    "not terminate immediately after stage $t$. Note that if a subsequence does not terminate with stage $t$, then there **must** be a further transition to the next stage $t+1$.\n",
    "Hence, for the purposes of sequence generation, the introduction of termination probabilities allows a stochastic choice\n",
    "after every stage of whether or not to terminate the sequence.\n",
    "\n",
    "Consequently, the forward pass now takes the form\n",
    "\\begin{eqnarray}\n",
    "\\alpha_t(i) & = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\\alpha_{t-1}(j)\\,\\bar{\\tau}_j\\,a_{ji}\\,b_{i,y_{t}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for $t=2,\\ldots,T$, and the backward pass takes the form\n",
    "\\begin{eqnarray}\n",
    "\\beta_t(i) & = & \n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\\bar{\\tau}_i\\,a_{ij}\\,b_{j,y_{t+1}}\\,\\beta_{t+1}(j)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for $t=T-1,\\ldots,1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46216766",
   "metadata": {},
   "source": [
    "## Binary Hierarchical Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131d031",
   "metadata": {},
   "source": [
    "### Top-down generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce1f38",
   "metadata": {},
   "source": [
    "We now generalise the notion of a one-dimensional \n",
    "[HMM](#Comparison-with-a-HMM \"Section: Comparison with a HMM\")\n",
    "to a two-dimensional structure\n",
    "representing a hierarchy of hidden states.\n",
    "As shown in the figure below, we consider a *generative* process \n",
    "that outputs a sequence of tokens (e.g. a sentence) from the grammar $\\mathcal{G}$.\n",
    "<img src=\"generated_sentence.png\" title=\"Hierarchical, generative model of a sentence\" width=\"40%\">\n",
    "\n",
    "Here we restrict $\\mathcal{G}$ to a context-free grammar using only unary and binary rules.\n",
    "The binary rules take the\n",
    "top-down form $\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$ for $\\sigma_i,\\sigma_j\\sigma_k\\in\\mathcal{S}$,\n",
    "where $\\sigma_i$ is the *parent* state, and $\\sigma_j$ and $\\sigma_k$ are the\n",
    "*left child* and *right child* states, respectively.\n",
    "Similarly, the unary rules take the form $\\sigma_i\\rightarrow\\nu_m$ for\n",
    "$\\sigma_i\\in\\mathcal{S}$ and $\\nu_m\\in\\mathcal{Y}$, where $\\sigma_i$ is again the parent state, and\n",
    "now $\\nu_m$ is the child token.\n",
    "Thus, the final *derivation* is a binary tree from a \n",
    "[root](#Root-states \"Section: Root states\") state $S_{1:T}$, \n",
    "through [intermediate](#Intermediate-states \"Section: Intermediate states\") states $S_{s:t}$,\n",
    "down to [leaf](#Leaf-states \"Section: Leaf states\") states $S_{t:t}\\doteq S_t$,\n",
    "spanning the tokens $\\mathbf{y}_{1:T}$.\n",
    "\n",
    "If we let the *collection* $\\mathbf{S}_{1:T}$ of variables loosely represent\n",
    "the entire hierarchy of hidden states, then the derivation shown above has a joint probability\n",
    "that factors in top-down fashion as\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{S}_{1:T}=\\mathbf{s},\\mathbf{Y}_{1:T}=\\mathbf{y}) & = &\n",
    "P(S_{1:6}=\\mathtt{S})\\,\n",
    "P(S_{1:2}=\\mathtt{NP},S_{3:6}=\\mathtt{VP}\\mid S_{1:6}=\\mathtt{S})\\,\n",
    "P(S_1=\\mathtt{D},S_2=\\mathtt{N}\\mid S_{1:2}=\\mathtt{NP})\n",
    "\\\\&&{}\\times\n",
    "P(Y_1=\\texttt{The}\\mid S_1=\\mathtt{D})\\,\n",
    "P(Y_2=\\texttt{cat}\\mid S_2=\\mathtt{N})\\,\n",
    "P(S_{3}=\\mathtt{V},S_{4:6}=\\mathtt{PP}\\mid S_{3:6}=\\mathtt{VP})\n",
    "\\\\&&{}\\times\n",
    "P(Y_3=\\texttt{sat}\\mid S_3=\\mathtt{V})\\,\n",
    "P(S_4=\\mathtt{P},S_{5:6}=\\mathtt{NP}\\mid S_{4:6}=\\mathtt{PP})\n",
    "P(Y_4=\\texttt{on}\\mid S_4=\\mathtt{P})\n",
    "\\\\&&{}\\times\n",
    "P(S_5=\\mathtt{D},S_5=\\mathtt{N}\\mid S_{5:6}=\\mathtt{NP})\\,\n",
    "P(Y_5=\\texttt{the}\\mid S_5=\\mathtt{D})\\,\n",
    "P(Y_6=\\texttt{mat}\\mid S_6=\\mathtt{N})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that in comparison to the corresponding [HMM](#Comparison-with-a-HMM \"Section: Comparison with a HMM\")\n",
    "model, the hierarchical model lacks strong left-to-right dependencies\n",
    "between consecutive leaf states. However, the binary rules do implicitly constrain the left and right\n",
    "child states to span adjacent subsequences of tokens, such that right-to-left combinations are forbidden.\n",
    "This issue is re-examined in a \n",
    "[later](#Sequential-Dependencies \"Section: Sequential Dependencies\")\n",
    "section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37fecb",
   "metadata": {},
   "source": [
    "### Bottom-up parsing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39278c4",
   "metadata": {},
   "source": [
    "Conversely to the top-down, [generative](#Top-down-generative-model \"Section: Top-down generative model\")\n",
    "process that produces tokens,\n",
    "there is also notionally a corresponding bottom-up, *parsing* process from tokens to \n",
    "[root](#Root-states \"Section: Root states\") state. \n",
    "Loosely, we suppose that the top-down rules of grammar $\\mathcal{G}$ may be reversed\n",
    "to become unary rules of the form $\\nu_m\\rightarrow\\sigma_i$ for $\\nu_m\\in\\mathcal{Y}$\n",
    "and $\\sigma_i\\in\\mathcal{S}$, and binary rules of the form \n",
    "$\\sigma_j\\oplus\\sigma_k\\rightarrow\\sigma_i$ for $\\sigma_i,\\sigma_j,\\sigma_k\\in\\mathcal{S}$.\n",
    "Note that in this reversed form, $\\sigma_i$ has become a child state, and $\\sigma_j$ and $\\sigma_k$\n",
    "have become parent states. In order to retain consistency with the generative process, we shall call $\\sigma_i$\n",
    "the *head* state regardless of the direction, e.g.\n",
    "$\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$\n",
    "or $\\sigma_j\\oplus\\sigma_k\\rightarrow\\sigma_i$.\n",
    "\n",
    "We now suppose that the contiguous sequence $\\mathbf{y}_{1:T}$ of tokens\n",
    "can be *parsed* via a series of bottom-up unary and binary rules into a single structure, specifically a binary tree called a *parse* tree. Since the only difference between the top-down\n",
    "[derivation](#Top-down-generative-model \"Section: Top-down generative model\")\n",
    "and the bottom-up parse tree is the direction of the edges (i.e. the rules), then we may loosely\n",
    "refer to both structures (ignoring direction) as a *parse*. Hence, if there is a state\n",
    "$S_{s:t}$ spanning the subsequence $\\mathbf{y}_{s:t}$ of tokens, we may say that $\\mathbf{y}_{s:t}$\n",
    "has a *subparse* with head state $S_{s:t}$.\n",
    "\n",
    "Note, however, that directionality of the rules is important when computing probabilities, since \n",
    "$P_\\mathtt{gen}(\\texttt{A}\\rightarrow\\texttt{B})\\neq P_\\mathtt{parse}(\\texttt{B}\\rightarrow\\texttt{A})$ in general,\n",
    "due to the former being $P(\\texttt{B}\\mid\\texttt{A})$ and the latter being $P(\\texttt{A}\\mid\\texttt{B})$.\n",
    "\n",
    "The parse tree for our example sentence is shown in the figure below.\n",
    "<img src=\"parsed_sentence.png\" title=\"Hierarchical parsing model of a sentence\" width=\"40%\">\n",
    "\n",
    "This parse has conditional probability\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{S}_{1:T}=\\mathbf{s}\\mid\\mathbf{Y}_{1:T}=\\mathbf{y}) & = &\n",
    "P(S_1=\\mathtt{D}\\mid Y_1=\\texttt{The})\\,P(S_2=\\mathtt{N}\\mid Y_2=\\texttt{cat})\\,\n",
    "P(S_{1:2}=\\mathtt{NP}\\mid S_1=\\mathtt{D},S_2=\\mathtt{N})\n",
    "\\\\&&{}\\times\n",
    "P(S_5=\\mathtt{D}\\mid Y_5=\\texttt{the})\\,P(S_6=\\mathtt{N}\\mid Y_6=\\texttt{mat})\\,\n",
    "P(S_{5:6}=\\mathtt{NP}\\mid S_5=\\mathtt{D},S_6=\\mathtt{N})\n",
    "\\\\&&{}\\times\n",
    "P(S_4=\\mathtt{P}\\mid Y_4=\\texttt{on})\\,\n",
    "P(S_{4:6}=\\mathtt{PP}\\mid S_4=\\mathtt{P},S_{5:6}=\\mathtt{NP})\n",
    "\\\\&&{}\\times\n",
    "P(S_3=\\mathtt{V}\\mid Y_3=\\texttt{cat})\\,\n",
    "P(S_{3:6}=\\mathtt{VP}\\mid S_3=\\mathtt{V},S_{4:6}=\\mathtt{PP})\n",
    "\\\\&&{}\\times\n",
    "P(S_{1:6}=\\mathtt{S}\\mid S_{1:2}=\\mathtt{NP},S_{3:6}=\\mathtt{VP})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that the order of combining tokens and head states in bottom-up token parsing\n",
    "differs, in general, from the order of generating head states and tokens in a top-down fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba424ba",
   "metadata": {},
   "source": [
    "### Inside structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2a5cc",
   "metadata": {},
   "source": [
    "The *inside* pass is analogous to the *backward* pass of a \n",
    "[HMM](#Comparison-with-a-HMM \"Section: Comparison with a HMM\"). \n",
    "We assume that a\n",
    "subsequence $\\mathbf{y}_{s:t}$ can be parsed into a substructure with given head state\n",
    "$S_{s:t}$. Unlike proper parsing, however, we are not concerned with the actual\n",
    "substructure so obtained. Instead, we only wish to measure the overall likelihood that\n",
    "one or more such subparses exist. This probability is given by\n",
    "\\begin{eqnarray}\n",
    "\\beta_{s:t}(i) & \\doteq &\n",
    "P(\\mathbf{Y}_{s:t}=\\mathbf{y}_{s:t}\\mid S_{s:t}=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "Clearly, in the special case where $s=t$, this reduces to\n",
    "\\begin{eqnarray}\n",
    "\\beta_{t:t}(i) & = &\n",
    "P(Y_t=y_t\\mid S_{t}=\\sigma_i)\\,,\n",
    "\\end{eqnarray}\n",
    "where we define $S_t\\doteq S_{t:t}$ for convenience.\n",
    "Under the assumption that the latter distribution is invariant\n",
    "to the position $t$ within the sequence, we define\n",
    "\\begin{eqnarray}\n",
    "    P(Y_t=\\nu_m\\mid S_{t}=\\sigma_i) & \\doteq & \n",
    "    P(\\sigma_i\\rightarrow\\nu_m)~=~\n",
    "    b_{im}\\,, \n",
    "\\end{eqnarray}\n",
    "where $\\sum_{m=1}^{\\left|\\mathcal{Y}\\right|}b_{im}=1$ for all \n",
    "$i=1,2,\\ldots,\\left|\\mathcal{S}\\right|$.\n",
    "Hence, $\\mathbf{B}=[b_{im}]$ is just the emission matrix from the corresponding HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b0162",
   "metadata": {},
   "source": [
    "The more general case, however, is that $s<t$, for which there exists at least one $s\\le r<t$ such that\n",
    "the substructure for subsequence $\\mathbf{y}_{s:t}$ may be further partitioned into\n",
    "two adjacent substructures, one for subsequence $\\mathbf{y}_{s:r}$ and the other\n",
    "for subsequence $\\mathbf{y}_{r+1:t}$. In order for this to be plausible,\n",
    "the former subsequence must have some head state $S_{s:r}=\\sigma_j$, and the latter must have some head state $S_{r+1:t}=\\sigma_k$. \n",
    "Consequently, there must exist at least one (top-down) rule of the\n",
    "form $\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$ that satisfies the partitioning\n",
    "$S_{s:t}\\rightarrow S_{s:r}\\oplus S_{r+1:t}$ for \n",
    "$\\mathbf{y}_{s:t}=\\mathbf{y}_{s:r}\\odot\\mathbf{y}_{r+1:t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd4a67",
   "metadata": {},
   "source": [
    "Now, we noted above that we are not interested in the specific detail\n",
    "of a particular parse.\n",
    "Hence,\n",
    "we obscure all such substructural detail by summing over the probability of each subparse, for all permissible values of \n",
    "the partition variable $r$, and weight each substructure by the plausibility of the values of $j$ and $k$.\n",
    "Thus, we obtain the recursive relation\n",
    "\\begin{eqnarray}\n",
    "\\beta_{s:t}(i) & = & \n",
    "P(\\mathbf{Y}_{s:t}=\\mathbf{y}_{s:t}\\mid S_{s:t}=\\sigma_i)\\,.\n",
    "\\\\& = &\n",
    "\\sum_{r=s}^{t-1}\n",
    "P(\\mathbf{Y}_{s:r}=\\mathbf{y}_{s:r},\\mathbf{Y}_{r+1:t}=\\mathbf{y}_{r+1:t}\\mid S_{s:t}=\\sigma_i)\\,.\n",
    "\\\\& = &\n",
    "\\sum_{r=s}^{t-1}\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{s:r}=\\mathbf{y}_{s:r}\\mid S_{s:r}=\\sigma_j)\\,\n",
    "P(\\mathbf{Y}_{r+1:t}=\\mathbf{y}_{r+1:t}\\mid S_{r+1:t}=\\sigma_k)\\,\n",
    "\\\\&&{}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\times\n",
    "P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k\\mid S_{s:t}=\\sigma_i)\n",
    "\\\\& = &\n",
    "\\sum_{r=s}^{t-1}\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k\\mid S_{s:t}=\\sigma_i)\\,\n",
    "\\beta_{s:r}(j)\\,\\beta_{r+1:t}(k)\\,,\n",
    "\\end{eqnarray}\n",
    "for $s<t$.\n",
    "Under the assumption that the probabilities of the various binary rules are\n",
    "invariant to the positions within the sequence, we define\n",
    "\\begin{eqnarray}\n",
    "P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k\\mid S_{s:t}=\\sigma_i)\n",
    "& \\doteq & P(\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k)\n",
    "~=~a_{ijk}\\,,\n",
    "\\end{eqnarray}\n",
    "where \n",
    "$\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}a_{ijk}=1$ for $i=1,2,\\ldots,\\left|\\mathcal{S}\\right|$.\n",
    "Observe that $\\mathbf{A}=[a_{ijk}]$ is now a three-dimensional tensor, in constrast to the\n",
    "two-dimensional transition matrix of the corresponding HMM.\n",
    "\n",
    "Consequently, we obtain\n",
    "\\begin{eqnarray}\n",
    "\\beta_{s:t}(i) & = & \n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "a_{ijk}\n",
    "\\sum_{r=s}^{t-1}\n",
    "\\beta_{s:r}(j)\\,\\beta_{r+1:t}(k)\\,,\n",
    "\\end{eqnarray}\n",
    "for $s<t$. Observe that the subscripts of $\\beta$ go from a narrower range to a wider range, and hence the inside pass corresponds to a *bottom-up* pass.\n",
    "This is the direction that token parsing takes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b174b0f",
   "metadata": {},
   "source": [
    "### Outside structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88e0c13",
   "metadata": {},
   "source": [
    "Given an *inside* substructure for some subsequence $\\mathbf{y}_{s:t}$\n",
    "with head state $S_{s:t}=\\sigma_i$, the rest of\n",
    "the parse, spanning the remaining subsequences $\\mathbf{y}_{1:s-1}$ and $\\mathbf{y}_{t+1:T}$, forms\n",
    "the *outside* structure. Analagous to the *forward* pass of a \n",
    "[HMM](#Comparison-with-a-HMM \"Section: Comparison with a HMM\"), \n",
    "the outside structure has joint probability\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{s:t}(i) & \\doteq & \n",
    "P(\\mathbf{Y}_{1:s-1}=\\mathbf{y}_{1:s-1},\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},\n",
    "S_{s:t}=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "We observe that\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{s:t}(i)\\,\\beta_{s:t}(i) & = & \n",
    "P(\\mathbf{Y}_{1:s-1}=\\mathbf{y}_{1:s-1},\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},\n",
    "S_{s:t}=\\sigma_i)\\,\n",
    "P(\\mathbf{Y}_{s:t}=\\mathbf{y}_{s:t}\\mid S_{s:t}=\\sigma_i)\n",
    "\\\\ & = &\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}, S_{s:t}=\\sigma_i)\\,,\n",
    "\\end{eqnarray}\n",
    "for every $s\\le t$ with $s,t\\in\\{1,2,\\ldots,T\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3bd20",
   "metadata": {},
   "source": [
    "At the extreme, for $s=1$ and $t=T$, the outside probability reduces to\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{1:T}(i) & = & P(S_{1:T}=\\sigma_i)\\,,\n",
    "\\end{eqnarray}\n",
    "for $i=1,2,\\ldots,|\\mathcal{S}|$. It is usual, but not necessary, for there to be\n",
    "a single, distinguished state at the root of every complete sequence \n",
    "$\\mathbf{y}_{1:T}$\n",
    "(see the [section](#Root-states \"Section: Root states\") on root states\n",
    "for more detail)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259dc912",
   "metadata": {},
   "source": [
    "We now consider the other cases where $s>1$ and/or $t<T$.\n",
    "For $s>1$, there exists at least one $1\\le r\\le s-1$ such that we may subdivide the\n",
    "left-hand part of the outer structure to include an *inner* substructure spanning\n",
    "subsequence $\\mathbf{y}_{r:s-1}$, adjacent to the existing inner substructure for\n",
    "$\\mathbf{y}_{s:t}$. We give this new substructure some head state $\\sigma_j$, and\n",
    "combine the two adjacent, inner substructures with some rule\n",
    "$\\sigma_k\\rightarrow\\sigma_j\\oplus\\sigma_i$. \n",
    "The result of the combination is a single inner structure spanning $\\mathbf{y}_{r:t}$\n",
    "with head state $S_{r:t}=\\sigma_k$, which forms part of the inner probability\n",
    "$\\beta_{r:t}(k)$. What remains is therefore a new outer structure forming part\n",
    "of the outer probability $\\alpha_{r:t}(k)$.\n",
    "\n",
    "This 'left-hand-side' probability is thus\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{s:t}^{\\mathtt{LHS}}(i) & = &\n",
    "\\sum_{r=1}^{s-1}P(\\mathbf{Y}_{1:r-1}=\\mathbf{y}_{1:r-1},\n",
    "\\mathbf{Y}_{r:s-1}=\\mathbf{y}_{r:s-1},\n",
    "\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},S_{s:t}=\\sigma_i)\n",
    "\\\\&=&\n",
    "\\sum_{r=1}^{s-1}\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{r:s-1}=\\mathbf{y}_{r:s-1}\\mid S_{r:s-1}=\\sigma_j)\\,\n",
    "P(\\mathbf{Y}_{1:r-1}=\\mathbf{y}_{1:r-1},\n",
    "\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},S_{s:t}=\\sigma_i,S_{r:s-1}=\\sigma_j)\n",
    "\\\\&=&\n",
    "\\sum_{r=1}^{s-1}\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{r:s-1}=\\mathbf{y}_{r:s-1}\\mid S_{r:s-1}=\\sigma_j)\\,\n",
    "P(\\mathbf{Y}_{1:r-1}=\\mathbf{y}_{1:r-1},\n",
    "\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},S_{r:t}=\\sigma_k)\n",
    "\\\\&&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{}\\times\n",
    "P(S_{r:s-1}=\\sigma_j,S_{s:t}=\\sigma_i\\mid S_{r:t}=\\sigma_k)\n",
    "\\\\& = &\n",
    "\\sum_{r=1}^{s-1}\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\sigma_k\\rightarrow\\sigma_j\\oplus\\sigma_i)\\,\n",
    "\\beta_{r:s-1}(j)\\,\\alpha_{r:t}(k)\n",
    "\\\\& = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "a_{kji}\n",
    "\\sum_{r=1}^{s-1}\n",
    "\\alpha_{r:t}(k)\\,\\beta_{r:s-1}(j)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34824067",
   "metadata": {},
   "source": [
    "Similarly, for $t<T$ we may choose at least one $t+1\\le r\\le T$ by which to subdivide\n",
    "the right-hand side of the outer structure to include a new inner substructure\n",
    "spanning $\\mathbf{y}_{t+1:r}$ with head state $S_{t+1:r}=\\sigma_j$. This new substructure is adjacent to the substructure spanning $\\mathbf{y}_{s:t}$, and so we can\n",
    "again follow the logic described above. The resulting 'right-hand-side'\n",
    "probability is thus\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{s:t}^{\\mathtt{RHS}}(i) & = &\n",
    "\\sum_{r=t+1}^{T}P(\\mathbf{Y}_{1:s-1}=\\mathbf{y}_{1:s-1},\n",
    "\\mathbf{Y}_{t+1:r}=\\mathbf{y}_{t+1:r},\n",
    "\\mathbf{Y}_{r+1:T}=\\mathbf{y}_{r+1:T},\n",
    "S_{s:t}=\\sigma_i)\n",
    "\\\\&=&\n",
    "\\sum_{r=t+1}^{T}\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{t+1:r}=\\mathbf{y}_{t+1:r}\\mid S_{t+1:r}=\\sigma_j)\\,\n",
    "P(\\mathbf{Y}_{1:s-1}=\\mathbf{y}_{1:s-1},\n",
    "\\mathbf{Y}_{r+1:T}=\\mathbf{y}_{r+1:T},S_{s:t}=\\sigma_i,S_{t+1:r}=\\sigma_j)\n",
    "\\\\&=&\n",
    "\\sum_{r=t+1}^{r}\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{t+1:r}=\\mathbf{y}_{t+1:r}\\mid S_{t+1:r}=\\sigma_j)\\,\n",
    "P(\\mathbf{Y}_{1:s-1}=\\mathbf{y}_{1:s-1},\n",
    "\\mathbf{Y}_{r+1:T}=\\mathbf{y}_{r+1:T},S_{s:r}=\\sigma_k)\n",
    "\\\\&&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{}\\times\n",
    "P(S_{s:t}=\\sigma_i,S_{t+1:r}=\\sigma_j\\mid S_{s:r}=\\sigma_k)\n",
    "\\\\& = &\n",
    "\\sum_{r=t+1}^{T}\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\sigma_k\\rightarrow\\sigma_i\\oplus\\sigma_j)\\,\n",
    "\\beta_{t+1:r}(j)\\,\\alpha_{s:r}(k)\n",
    "\\\\& = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "a_{kij}\n",
    "\\sum_{r=t+1}^{T}\n",
    "\\alpha_{s:r}(k)\\,\\beta_{t+1:r}(j)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8973b14",
   "metadata": {},
   "source": [
    "Consequently, the total outside probability is now just\n",
    "\\begin{eqnarray}\n",
    "\\alpha_{s:t}(i) & = & \\alpha_{s:t}^\\mathtt{LHS}(i)+\\alpha_{s:t}^\\mathtt{RHS}(i)\n",
    "\\\\& = &\n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\left\\{\n",
    "a_{kji}\\sum_{r=1}^{s-1}\n",
    "\\alpha_{r:t}(k)\\,\\beta_{r:s-1}(j)\n",
    "+a_{kij}\\sum_{r=t+1}^{T}\n",
    "\\alpha_{s:r}(k)\\,\\beta_{t+1:r}(j)\n",
    "\\right\\}\\,,\n",
    "\\end{eqnarray}\n",
    "for $s>1$ or $t<T$. Observe that the subscripts on $\\alpha$ go from a wider range to a narrower range, and hence the outside pass corresponds to a *top-down* pass. This\n",
    "is the direction that token generation takes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b3530",
   "metadata": {},
   "source": [
    "### Arbitrary structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac2a0a",
   "metadata": {},
   "source": [
    "The above formulae for the inside and outside passes deliberately obscure individual\n",
    "parsing structures. However, sometimes we desire more detailed knoweledge, especially\n",
    "about which states have been used.\n",
    "\n",
    "In particular, we assume that every token sequence $\\mathbf{y}_{1:T}$ can in fact be parsed up to some root state $S_{1:T}$.\n",
    "The likelihood of an observed sequence $\\mathbf{y}_{1:T}$ is therefore given by\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}) & = &\n",
    "\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}, S_{1:T}=\\sigma_i)\n",
    "~=~\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\\alpha_{1:T}(i)\\,\\beta_{1:T}(i)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e414f",
   "metadata": {},
   "source": [
    "However, note that our notation still admits some ambiguity, since\n",
    "\\begin{eqnarray}\n",
    "\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}, S_{s:t}=\\sigma_i)\n",
    "& \\neq & P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T})\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "in general. The left-hand side of this inequality gives the joint probability of the observed\n",
    "sequence **and** the fact that the subsequence $\\mathbf{y}_{s:t}$ is spaned by some substructure with arbitrary head state $S_{s:t}$. There is no reason why\n",
    "different parses with different substructures should be equiprobable.\n",
    "Thus, with slightly less ambiguity, we define\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}, S_{s:t}=*)\n",
    "& \\doteq &\n",
    "\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T}, S_{s:t}=\\sigma_i)\n",
    "~=~\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\\alpha_{s:t}(i)\\,\\beta_{s:t}(i)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a8332",
   "metadata": {},
   "source": [
    "By similar reasoning, we note that the values of $s$ and $t$ are not particularly\n",
    "special, and so we may sum over these values instead.\n",
    "However, we must be careful to distinguish between the two modes of head state\n",
    "$S_{s:t}=\\sigma_i$. For $s=t$, we **must** have a unary rule of the form\n",
    "$\\sigma_i\\rightarrow\\nu_m$, whereas for $s<t$ we **must** have a binary rule of\n",
    "the form $\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$. \n",
    "This is the primary reason for our choice of the way we condition probabilities,\n",
    "as briefly discussed in the [introductory](#Background \"Section: Background\") section.\n",
    "\n",
    "Hence, \n",
    "using the shorthand that $S_t\\doteq S_{t:t}$, we define the unary\n",
    "case as\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{*}=\\sigma_i)\n",
    "& \\doteq & \n",
    "\\sum_{t=1}^{T}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{t}=\\sigma_i)\n",
    "~=~\n",
    "\\sum_{t=1}^{T}\n",
    "\\alpha_{t:t}(i)\\,\\beta_{t:t}(i)\\,.\n",
    "\\end{eqnarray}\n",
    "This is the joint probability of observing the sequence $\\mathbf{y}_{1:T}$\n",
    "coupled with the fact that state $\\sigma_i$ appears\n",
    "as the head state of a single token, at least once in the derivation. \n",
    "\n",
    "Similarly, the binary case is defined as\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{*:*}=\\sigma_i)\n",
    "& \\doteq & \n",
    "\\sum_{s=1}^{T-1}\n",
    "\\sum_{t=s+1}^{T}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{s:t}=\\sigma_i)\n",
    "~=~\n",
    "\\sum_{s=1}^{T-1}\n",
    "\\sum_{t=s+1}^{T}\n",
    "\\alpha_{s:t}(i)\\,\\beta_{s:t}(i)\\,.\n",
    "\\end{eqnarray}\n",
    "This is the joint probability of observing the sequence $\\mathbf{y}_{1:T}$\n",
    "coupled with the fact that state $\\sigma_i$ appears\n",
    "as a head state spanning two or more tokens, at least once in the derivation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b963c0",
   "metadata": {},
   "source": [
    "By similar reasoning, we now have\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_*=\\sigma_i,\\sigma_i\\rightarrow\\nu_m)\n",
    "& = & \\sum_{t=1}^{T}P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{t:t}=\\sigma_i,y_t=\\nu_m)\n",
    "\\\\& = &\n",
    "\\sum_{t=1}^{T}\\delta(y_t=\\nu_m)\\,P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{t:t}=\\sigma_i)\n",
    "\\\\& = & \n",
    "\\sum_{t=1}^{T}\\delta(y_t=\\nu_m)\\,\\alpha_{t:t}(i)\\,\\beta_{t:t}(i)\\,,\n",
    "\\end{eqnarray}\n",
    "for the unary case, and hence\n",
    "\\begin{eqnarray}\n",
    "P(\\sigma_i\\rightarrow\\nu_m\\mid\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_*=\\sigma_i)\n",
    "& = & \n",
    "\\frac{P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_*=\\sigma_i,\\sigma_i\\rightarrow\\nu_m)}\n",
    "{P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_*=\\sigma_i)}\n",
    "\\\\& = &\n",
    "\\frac{\\sum_{t=1}^{T}\\delta(y_t=\\nu_m)\\,\\alpha_{t:t}(i)\\,\\beta_{t:t}(i)}\n",
    "{\\sum_{t=1}^{T}\\alpha_{t:t}(i)\\,\\beta_{t:t}(i)}\n",
    "\\\\& = &\n",
    "b_{im}\\,\\frac{\\sum_{t=1}^{T}\\delta(y_t=\\nu_m)\\,\\alpha_{t:t}(i)}\n",
    "{\\sum_{t=1}^{T}\\alpha_{t:t}(i)\\,\\beta_{t:t}(i)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Clearly this is normalised to unity summing over all $\\nu_m\\in\\mathcal{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02ed7b",
   "metadata": {},
   "source": [
    "For the binary rule, we similarly have that\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{*:*}=\\sigma_i,\n",
    "\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k) & = &\n",
    "\\sum_{s=1}^{T-1}\\sum_{t=s+1}^{T}\\sum_{r=s}^{t-1}\n",
    "P(\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{s:t}=\\sigma_i,S_{s:r}=\\sigma_j,\n",
    "S_{r+1:t}=\\sigma_k)\n",
    "\\\\& = &\n",
    "\\sum_{s=1}^{T-1}\\sum_{t=s+1}^{T}\\sum_{r=s}^{t-1}\n",
    "P(\\mathbf{Y}_{s:r}=\\mathbf{y}_{s:r}\\mid S_{s:r}=\\sigma_j)\n",
    "\\\\&&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{}\\times\n",
    "P(\\mathbf{Y}_{r+1:t}=\\mathbf{y}_{r+1:t}\\mid S_{r+1:t}=\\sigma_k)\n",
    "\\\\&&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{}\\times\n",
    "P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k\\mid S_{s:t}=\\sigma_i)\n",
    "\\\\&&\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;{}\\times\n",
    "P(\\mathbf{Y}_{1:s-1}=\\mathbf{y}_{1:s-1},\\mathbf{Y}_{t+1:T}=\\mathbf{y}_{t+1:T},\n",
    "S_{s:t}=\\sigma_i)\n",
    "\\\\& = &\n",
    "\\sum_{s=1}^{T-1}\\sum_{t=s+1}^{T}\\sum_{r=s}^{t-1}\n",
    "\\beta_{s:r}(j)\\,\\beta_{r+1:t}(k)\\,P(\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k)\\,\n",
    "\\alpha_{s:t}(i)\n",
    "\\\\& = &\n",
    "a_{ijk}\\sum_{s=1}^{T-1}\\sum_{t=s+1}^{T}\\alpha_{s:t}(i)\n",
    "\\sum_{r=s}^{t-1}\\beta_{s:r}(j)\\,\\beta_{r+1:t}(k)\\,,\n",
    "\\end{eqnarray}\n",
    "and hence\n",
    "\\begin{eqnarray}\n",
    "P(\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k\\mid\n",
    "\\mathbf{Y}_{1:T}=\\mathbf{y}_{1:T},S_{*:*}=\\sigma_i) \n",
    "& = &\n",
    "a_{ijk}\\,\n",
    "\\frac{\\sum_{s=1}^{T-1}\\sum_{t=s+1}^{T}\\alpha_{s:t}(i)\n",
    "\\sum_{r=s}^{t-1}\\beta_{s:r}(j)\\,\\beta_{r+1:t}(k)}\n",
    "{\\sum_{s=1}^{T-1}\\sum_{t=s+1}^{T}\\alpha_{s:t}(i)\\,\\beta_{s:t}(i)}\\,.\n",
    "\\end{eqnarray}\n",
    "We observe that this is normalised to unity when summed over $j$ and $k$, since\n",
    "we have previously established that\n",
    "\\begin{eqnarray}\n",
    "\\beta_{s:t}(i) & = & \n",
    "\\sum_{j=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\sum_{k=1}^{\\left|\\mathcal{S}\\right|}\n",
    "a_{ijk}\n",
    "\\sum_{r=s}^{t-1}\n",
    "\\beta_{s:r}(j)\\,\\beta_{r+1:t}(k)\\,,\n",
    "\\end{eqnarray}\n",
    "for $s<t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552aa87",
   "metadata": {},
   "source": [
    "## Conditional Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c4042",
   "metadata": {},
   "source": [
    "A major role of the inside-outside algorithm is to estimate the conditional\n",
    "probabilities $\\mathbf{A}=[a_{ijk}]$ of the binary rules \n",
    "$\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$, and the conditional probabilities \n",
    "$\\mathbf{B}=[b_{im}]$ of the unary rules $\\sigma_i\\rightarrow\\nu_m$. These values are \n",
    "typically estimated from observed data via the *Expectation-Maximisation* algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48ad15",
   "metadata": {},
   "source": [
    "### Expectation-maximisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ff2a6",
   "metadata": {},
   "source": [
    "Suppose that we have observed a collection \n",
    "$\\mathbb{Y}=\\left(\\mathbf{y}^{(1)},\\mathbf{y}^{(2)},\\ldots,\\mathbf{y}^{(N)}\\right)$ of indepdendent token sequences of arbitrary lengths \n",
    "$T^{(1)}=\\left|\\mathbf{y}^{(1)}\\right|,\\ldots,T^{(N)}=\\left|\\mathbf{y}^{(N)}\\right|$.\n",
    "Next, we notionally *complete* the data by assuming that each sequence\n",
    "$\\mathbf{y}^{(d)}$ has a true but unknown parse structure loosely represented by $\\mathbf{s}^{(d)}$.\n",
    "Clearly, the allowable parses depend upon both the grammar $\\mathcal{G}$ and the\n",
    "length $T^{(d)}$ of the sequence. Hence, we take \n",
    "$\\mathbf{s}^{(d)}\\in\\boldsymbol{\\mathcal{S}}_{T^{(d)}}$.\n",
    "The complete data therefore takes the form \n",
    "$\\mathbb{C}=((\\mathbf{s}^{(d)},\\mathbf{y}^{(d)}))_{d=1}^{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0b476",
   "metadata": {},
   "source": [
    "Assuming the complete cases are independent, the overall likelihood is given by\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbb{C}\\mid\\Theta) & = & \n",
    "\\prod_{d=1}^N P(\\mathbf{s}^{(d)},\\mathbf{y}^{(d)}\\mid\\Theta)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\Theta=(\\mathbf{A},\\mathbf{B})$ represents the current parameters of the grammar.\n",
    "However, since the true parse structure $\\mathbf{s}^{(d)}$ of each\n",
    "sequence $\\mathbf{y}^{(d)}$ is unknown, we invent a notional zero/one indicator $z_{d,\\mathbf{s}}\\doteq\\delta(\\mathbf{s}=\\mathbf{s}^{(d)})$, such that\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbb{C}\\mid\\Theta) & = & \n",
    "\\prod_{d=1}^N \n",
    "\\prod_{\\mathbf{s}\\in\\boldsymbol{\\mathcal{S}}_{T^{(d)}}}\n",
    "P(\\mathbf{s},\\mathbf{y}^{(d)}\\mid\\Theta)^{z_{d,\\mathbf{s}}}\n",
    "\\\\\\Rightarrow\n",
    "L(\\Theta) & \\doteq & \\ln P(\\mathbb{C}\\mid\\Theta) ~=~ \n",
    "\\sum_{d=1}^N \n",
    "\\sum_{\\mathbf{s}\\in\\boldsymbol{\\mathcal{S}}_{T^{(d)}}}\n",
    "z_{d,\\mathbf{s}} \\ln P(\\mathbf{s},\\mathbf{y}^{(d)}\\mid\\Theta)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e45ed1",
   "metadata": {},
   "source": [
    "The expected value of the complete log-likelihood is therefore\n",
    "\\begin{eqnarray}\n",
    "Q(\\Theta,\\Theta') & \\doteq & \n",
    "\\mathbb{E}\\left[L(\\Theta)\\mid\\mathbb{Y},\\Theta'\\right]\n",
    "\\\\& = &\n",
    "\\sum_{d=1}^N \n",
    "\\sum_{\\mathbf{s}\\in\\boldsymbol{\\mathcal{S}}_{T^{(d)}}}\n",
    "\\mathbb{E}\\left[z_{d,\\mathbf{s}}\\mid\\mathbb{Y},\\Theta'\\right]\n",
    "\\ln P(\\mathbf{s},\\mathbf{y}^{(d)}\\mid\\Theta)\n",
    "\\\\& = &\n",
    "\\sum_{d=1}^N \n",
    "\\sum_{\\mathbf{s}\\in\\boldsymbol{\\mathcal{S}}_{T^{(d)}}}\n",
    "P(\\mathbf{s}\\mid\\mathbf{y}^{(d)},\\Theta')\n",
    "\\ln P(\\mathbf{s},\\mathbf{y}^{(d)}\\mid\\Theta)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\Theta'$ is a prior estimate of $\\Theta$. The \n",
    "expectation-maximisation (EM) algorithm then proceeds by choosing \n",
    "a new estimate $\\hat{\\Theta}$ such that\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\Theta} & = & \\arg\\max_\\Theta Q(\\Theta,\\Theta')\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, the generalised\n",
    "expectation-maximisation (GEM) algorithm proceeds more simply by choosing \n",
    "a new estimate\n",
    "$\\hat{\\Theta}$ such that\n",
    "\\begin{eqnarray}\n",
    "Q(\\hat{\\Theta},\\Theta') & > & Q(\\Theta',\\Theta')\\,.\n",
    "\\end{eqnarray}\n",
    "In either case, the estimate is iteratively updated via $\\Theta'\\leftarrow\\hat{\\Theta}$.\n",
    "Under mild conditions, the iterations will converge to a local optimum $\\Theta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9ea80",
   "metadata": {},
   "source": [
    "### Expected binary rule counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f65db9",
   "metadata": {},
   "source": [
    "As an example, consider now the simple sequence $\\mathbf{y}^{(d)}=(\\mathtt{John},\\mathtt{ran})$ with ground-truth parse \n",
    "$\\mathbf{s}^{(d)}=(S_{1:2}=\\mathtt{S},S_{1:1}=\\mathtt{N},S_{2:2}=\\mathtt{V})$,\n",
    "where here $\\mathtt{S}$\n",
    "represents the state of a complete sentence, $\\mathtt{N}$ represents a noun, and\n",
    "$\\mathtt{V}$ represents a verb.\n",
    "We observe that the appropriate indicator is\n",
    "\\begin{eqnarray}\n",
    "z_{d,\\mathbf{s}} & = & \n",
    "\\delta(S_{1:2}=\\mathtt{S}\\wedge S_{1:1}=\\mathtt{N}\\wedge S_{2:2}=\\mathtt{V})\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}[z_{d,\\mathbf{s}}\\mid\\mathbf{y}^{(d)},\\Theta] & = &\n",
    "P(S_{1:2}=\\mathtt{S},S_{1:1}=\\mathtt{N}, S_{2:2}=\\mathtt{V}\\mid\n",
    "\\mathbf{y}^{(d)},\\Theta)\n",
    "\\\\& = &\n",
    "P(S_{1:2}=\\mathtt{S}\\mid\\mathbf{y}^{(d)},\\Theta)\\,\n",
    "P(S_{1:1}=\\mathtt{N}, S_{2:2}=\\mathtt{V}\\mid\n",
    "S_{1:2}=\\mathtt{S},\\mathbf{y}^{(d)},\\Theta)\\,.\n",
    "\\end{eqnarray}\n",
    "The latter term gives the empirical probability of the rule\n",
    "$\\mathtt{S}\\rightarrow\\mathtt{N}\\oplus\\mathtt{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371cfc9b",
   "metadata": {},
   "source": [
    "More generally, there might be a number of opportunities for a given rule\n",
    "$\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$ to appear in parse $\\mathbf{s}^{(d)}$,\n",
    "e.g. $P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k\\mid\n",
    "S_{s:t}=\\sigma_i,\\mathbf{y}^{(d)},\\Theta)$.\n",
    "In fact, from the\n",
    "[previous](#Binary-Hierarchical-Structure \"Section: Binary Hierarchical Structure\")\n",
    "section we have\n",
    "\\begin{eqnarray}\n",
    "P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k,\n",
    "S_{s:t}=\\sigma_i,\\mathbf{Y}=\\mathbf{y}^{(d)}\\mid\\Theta)\n",
    "& = &\n",
    "a_{j,k|i}\\,\\beta^{(d)}_{s:r}(j)\\,\\beta^{(d)}_{r+1:t}(k)\\,\\alpha^{(d)}_{s:t}(i)\n",
    "\\\\\\Rightarrow\n",
    "P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k,\n",
    "S_{s:t}=\\sigma_i\\mid\\mathbf{y}^{(d)},\\Theta)\n",
    "& = &\n",
    "\\frac{a_{j,k|i}\\,\\beta^{(d)}_{s:r}(j)\\,\\beta^{(d)}_{r+1:t}(k)\\,\\alpha^{(d)}_{s:t}(i)}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\\alpha^{(d)}_{1:T^{(d)}}(i')\\,\n",
    " \\beta^{(d)}_{1:T^{(d)}}(i')}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note for interest that by summing this over $r$, $j$ and $k$ we obtain\n",
    "\\begin{eqnarray}\n",
    "P(S_{s:t}=\\sigma_i\\mid\\mathbf{y}^{(d)},\\Theta) & = &\n",
    "\\frac{\\alpha^{(d)}_{s:t}(i)\\,\\beta^{(d)}_{s:t}(i)}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\\alpha^{(d)}_{1:T^{(d)}}(i')\\,\n",
    " \\beta^{(d)}_{1:T^{(d)}}(i')}\\,,\n",
    "\\end{eqnarray}\n",
    "which is the joint probability that subsequence $\\mathbf{y}_{s:t}^{(d)}$ is spanned by a subparse\n",
    "**and** the head of the subparse is $\\sigma_i$.\n",
    "Hence, the probability that subsequence $\\mathbf{y}_{s:t}^{(d)}$ is spanned by some subparse is\n",
    "\\begin{eqnarray}\n",
    "P(S_{s:t}=*\\mid\\mathbf{y}^{(d)},\\Theta) & = &\n",
    "\\frac{\\sum_{i=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\alpha^{(d)}_{s:t}(i)\\,\\beta^{(d)}_{s:t}(i)}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\\alpha^{(d)}_{1:T^{(d)}}(i')\\,\n",
    " \\beta^{(d)}_{1:T^{(d)}}(i')}\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that $P(S_{1:T^{(d)}}=*\\mid\\mathbf{y}^{(d)},\\Theta)=1$, as required,\n",
    "since we have assumed from the outset that every observed sequence \n",
    "has been generated by the grammar $\\mathcal{G}$ and thus must be parsable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf8ad1",
   "metadata": {},
   "source": [
    "The conditional probability of observing joint states \n",
    "$(S_{s:t},S_{s:r},S_{r+1:t})=(\\sigma_i,\\sigma_j,\\sigma_k)$ for a particular substructure $s:r:t$\n",
    "now generalises over all such substructures to the expected count of observing the tuple $(\\sigma_i,\\sigma_j,\\sigma_k)$, namely\n",
    "\\begin{eqnarray}\n",
    "\\hat{c}^{(d)}_{ijk} & \\doteq &\n",
    "\\sum_{s=1}^{T^{(d)}-1}\\sum_{t=s+1}^{T^{(d)}}\\sum_{r=s}^{t-1}\n",
    "P(S_{s:r}=\\sigma_j,S_{r+1:t}=\\sigma_k,\n",
    "S_{s:t}=\\sigma_i\\mid\\mathbf{y}^{(d)},\\Theta)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this is just the maximisation step of EM, where we are implicitly using the old estimate of $\\Theta$, i.e. $\\Theta'$ without the prime, to obtain\n",
    "the new estimate $\\hat{\\Theta}$.\n",
    "\n",
    "Hence, over all observed data $\\mathbb{Y}$, we have total expected counts\n",
    "\\begin{eqnarray}\n",
    "\\hat{c}_{ijk} & \\doteq & \\sum_{d=1}^{N} \\hat{c}^{(d)}_{ijk}\\,.\n",
    "\\end{eqnarray}\n",
    "The updated estimate of binary rule probabilities, $\\mathbf{A}$, is now obtained via\n",
    "\\begin{eqnarray}\n",
    "\\hat{a}_{ijk} & = & \\frac{\\hat{c}_{ijk}}{\\hat{c}_{i\\cdot\\cdot}}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdce86",
   "metadata": {},
   "source": [
    "More efficiently, we might instead compute\n",
    "\\begin{eqnarray}\n",
    "\\check{c}^{(d)}_{ijk} & \\doteq &\n",
    "\\sum_{s=1}^{T^{(d)}-1}\\sum_{t=s+1}^{T^{(d)}}\n",
    "\\alpha^{(d)}_{s:t}(i)\n",
    "\\sum_{r=s}^{t-1}\n",
    "\\beta^{(d)}_{s:r}(j)\\,\\beta^{(d)}_{r+1:t}(k)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\check{c}_{ijk} & \\doteq &\n",
    "\\sum_{d=1}^{N}\\left\\{\n",
    "\\frac{\\check{c}^{(d)}_{ijk}}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\\alpha^{(d)}_{1:T^{(d)}}(i')\\,\n",
    " \\beta^{(d)}_{1:T^{(d)}}(i')\n",
    "}\n",
    "\\right\\}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\hat{a}_{ijk} & = & \\frac{a_{ijk}\\,\\check{c}_{ijk}}\n",
    "{\\sum_{j'=1}^{\\left|\\mathcal{S}\\right|}\n",
    " \\sum_{k'=1}^{\\left|\\mathcal{S}\\right|}\n",
    " a_{ij'k'}\\,\\check{c}_{ij'k'}}\\,.\n",
    "\\end{eqnarray}\n",
    "Observe from this formulation that any zero probability in $a_{ijk}$ remains zero\n",
    "in $\\hat{a}_{ijk}$. Consequently, *structural restrictions* in the binary rules of the grammar $\\mathcal{G}$ are preserved. In particular, the head of any binary rule\n",
    "is an [intermediate](#Intermediate-states \"Section: Intermediate states\") state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40509a",
   "metadata": {},
   "source": [
    "### Expected unary rule counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4700ef",
   "metadata": {},
   "source": [
    "Similarly to the \n",
    "[above](#Expected-binary-rule-counts \"Section: Expected binary rule counts\")\n",
    "case for binary rules, for the unary rules we deduce that\n",
    "the probability of observing the tuple $(S_t,Y_t)=(\\sigma_i,\\nu_m)$ at position $t$\n",
    "is given by\n",
    "\\begin{eqnarray}\n",
    "P(S_t=\\sigma_i,Y_t=\\nu_m\\mid\\mathbf{y}^{(d)},\\Theta) & = &\n",
    "\\frac{\\delta(y_t^{(d)}=\\nu_m)\\,\\alpha^{(d)}_{t:t}(i)\\,\\beta^{(d)}_{t:t}(i)}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\n",
    "\\alpha^{(d)}_{1:T^{(d)}}(i')\\,\\beta^{(d)}_{1:T^{(d)}}(i')\n",
    "}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, we may define the per-sequence expected counts\n",
    "\\begin{eqnarray}\n",
    "\\hat{c}^{(d)}_{im} & \\doteq &\n",
    "\\sum_{t=1}^{T^{(d)}}\n",
    "P(S_t=\\sigma_i,Y_t=\\nu_m\\mid\\mathbf{y}^{(d)},\\Theta)\\,,\n",
    "\\end{eqnarray}\n",
    "and the total expected counts\n",
    "\\begin{eqnarray}\n",
    "\\hat{c}_{im} & \\doteq & \\sum_{d=1}^{N}\\hat{c}^{(d)}_{im}\\,,\n",
    "\\end{eqnarray}\n",
    "such that the unary rule probabilities, $\\mathbf{B}$, will be updated via\n",
    "\\begin{eqnarray}\n",
    "\\hat{b}_{im} & = & \\frac{\\hat{c}_{im}}{\\hat{c}_{i\\cdot}}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5675f5",
   "metadata": {},
   "source": [
    "Now, we observe that if $y_t^{(d)}=\\nu_m$ then \n",
    "$\\beta^{(d)}_{t:t}(i)=P(Y_t=y^{(d)}_t\\mid S_t=\\sigma_i,\\Theta)=b_{m|i}$.\n",
    "Hence, we may more efficiently compute\n",
    "\\begin{eqnarray}\n",
    "\\check{c}^{(d)}_{im} & \\doteq &\n",
    "\\sum_{t=1}^{T^{(d)}}\n",
    " \\delta(y^{(d)}_t=\\nu_m)\\,\\alpha^{(d)}_{t:t}(i)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "\\check{c}_{im} & \\doteq &\n",
    "\\sum_{d=1}^N\\left\\{\n",
    "\\frac{\\check{c}^{(d)}_{im}}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\n",
    " \\alpha^{(d)}_{1:T^{(d)}}(i')\\,\\beta^{(d)}_{1:T^{(d)}}(i')\n",
    "}\n",
    "\\right\\}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "such that\n",
    "\\begin{eqnarray}\n",
    "\\hat{b}_{im} & = & \n",
    "\\frac{b_{im}\\,\\check{c}_{im}}\n",
    "{\\sum_{m'=1}^{\\left|\\mathcal{Y}\\right|}b_{im'}\\,\\check{c}_{im'}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Observe from this formulation that any zero probability in $b_{im}$ remains zero\n",
    "in $\\hat{b}_{im}$. Consequently, structural restrictions in the unary rules of the grammar $\\mathcal{G}$ are preserved. In particular, the head of a unary rule is\n",
    "a [leaf](#Leaf-states \"Section: Leaf states\") state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52a2e8",
   "metadata": {},
   "source": [
    "## Grammatical Restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e59a5",
   "metadata": {},
   "source": [
    "The inside-outside process involves the\n",
    "finite set $\\mathcal{S}=\\{\\sigma_1,\\sigma_2,\\ldots\\}$ of discrete states that form\n",
    "the *non-terminal* symbols, and the finite set $\\mathcal{Y}=\\{\\nu_1,\\nu_2,\\ldots\\}$\n",
    "of output tokens that form the *terminal* symbols. Each grammatical structure spanning a contiguous sequence\n",
    "$\\mathbf{y}$ of tokens takes the form of a binary parse tree.\n",
    "The stochastic process itself utilises binary rule probabilities, \n",
    "$\\mathbf{A}=[a_{ijk}]$,\n",
    "and unary rule probabilities, $\\mathbf{B}=[b_{im}]$, where the rules have been defined\n",
    "in a top-down fashion suitable for token generation.\n",
    "\n",
    "As developed so far, these are the only restrictions placed on the grammar.\n",
    "However, there are situations where it might be useful to exert more control over the grammar. In particular, due to the binary nature of the parse tree, we may further\n",
    "distinguish between the non-terminal states. Thus, we let\n",
    "$\\mathcal{S}=\\mathcal{S}_\\mathtt{root}\\cup\\mathcal{S}_\\mathtt{int}\\cup\n",
    "\\mathcal{S}_\\mathtt{leaf}$, with\n",
    "*root* states $\\mathcal{S}_\\mathtt{root}$, *leaf* states $\\mathcal{S}_\\mathtt{leaf}$, and *intermediate* states $\\mathcal{S}_\\mathtt{int}$. Whether or not these susets overlap depends upon the restrictions placed upon the grammar. By default, with no restrictions, we have\n",
    "$\\mathcal{S}_\\mathtt{root}=\\mathcal{S}_\\mathtt{leaf}=\\mathcal{S}_\\mathtt{int}=\n",
    "\\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cd4d0",
   "metadata": {},
   "source": [
    "### Root states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b822a5",
   "metadata": {},
   "source": [
    "The root states $\\mathcal{S}_\\mathtt{root}\\subseteq\\mathcal{S}$ \n",
    "determine which non-terminal symbols may appear at the root of the parse tree of a sequence $\\mathbf{y}_{1:T}$.\n",
    "Thus, we suppose that $\\alpha_{1:T}(i)=0$ for every $\\sigma_i\\in\\mathcal{S}\\backslash\\mathcal{S}_\\mathtt{root}$, and\n",
    "$\\alpha_{1:T}(i)>0$ for every $\\sigma_i\\in\\mathcal{S}_\\mathtt{root}$.\n",
    "\n",
    "A common usage of the inside-outside algorithm in natural language processing (NLP) is to automatically extract a grammar from a corpus of tokenised sentences. It is typical in this scenario to assume that each sentence $\\mathbf{y}_{1:T}$ begins with a single, common state, say \n",
    "$S_{1:T}=\\mathtt{S}$ with unit probability.\n",
    "\n",
    "However, what then do we make of imperative sentences like \"*Go!*\"? We observe that the\n",
    "word \"go\" here typically takes a verbal form, say $\\mathtt{V}$ for simplicity, which is clearly\n",
    "not $\\mathtt{S}$. One possibility is to include punctuation in the token vocabulary,\n",
    "such that the appropriate rule might be \n",
    "$\\mathtt{S}\\rightarrow\\mathtt{V}\\oplus\\mathtt{!}$.\n",
    "Another possibility is to allow multiple root states, such as allowing $\\mathtt{V}$\n",
    "to also indicate an imperative sentence, or allowing another explicit root state\n",
    "like $\\mathtt{S}_\\mathtt{imp}$ along with a unary rule like\n",
    "$\\mathtt{S}_\\mathtt{imp}\\rightarrow\\mathtt{go}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88841c61",
   "metadata": {},
   "source": [
    "Another issue is whether or not root states are allowed to recur internally within the parse.\n",
    "For example, \"*The cat sat.*\" and \"*The cat sat on the mat.*\" are both sentences.\n",
    "Should we allow rules of the form $\\mathtt{S}\\rightarrow\\mathtt{S}\\oplus\\mathtt{PP}$,\n",
    "where $\\mathtt{PP}$ represents a prepositional phrase? Doing so presents a problem\n",
    "for indicating prepositional attachment, e.g. in \"*I saw the man with the telescope.*\" did I use\n",
    "a telescope (verb attachment), or was the man carrying a telescope (noun attachment)?\n",
    "\n",
    "A reasonable proposal might be to only allow root states to be used once per parse,\n",
    "i.e. $\\mathcal{S}_\\mathtt{root}\\cap\\mathcal{S}_\\mathtt{int}=\\emptyset$ and\n",
    "$\\mathcal{S}_\\mathtt{root}\\cap\\mathcal{S}_\\mathtt{leaf}=\\emptyset$.\n",
    "However, what if in response to the question \"*What is the dog chasing?*\" the answer is\n",
    "\"*The cat.*\", which is a noun phrase, $\\mathtt{NP}$? If we let \n",
    "$\\mathtt{NP}\\in\\mathcal{S}_\\mathtt{root}$, then it doesn't make sense to prevent it\n",
    "from being reused multiple times, e.g. in \"*The cat sat on the mat.*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db682cd",
   "metadata": {},
   "source": [
    "### Leaf states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16198690",
   "metadata": {},
   "source": [
    "The leaf states $\\mathcal{S}_\\mathtt{leaf}\\subseteq\\mathcal{S}$ \n",
    "determine which non-terminal symbols may appear at the leaves of the parse tree of a sequence $\\mathbf{y}_{1:T}$. Leaf states are special in that they are solely responsible\n",
    "for the production of the terminal symbols $y_t\\in\\mathcal{Y}$.\n",
    "Thus, we suppose that $\\beta_{t:t}(i)=0$ for every $\\sigma_i\\in\\mathcal{S}\\backslash\\mathcal{S}_\\mathtt{leaf}$, \n",
    "which (in order to sum probabilities to unity) might require the implicit use of \n",
    "a non-observable pseudo-token, say $\\nu_0=\\square$.\n",
    "Conversely, we suppose that\n",
    "$\\beta_{t:t}(i)>0$ for every $\\sigma_i\\in\\mathcal{S}_\\mathtt{leaf}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4541a0ba",
   "metadata": {},
   "source": [
    "In practice, the assigned leaf state $S_t$ for each token $y_t$ represents a *category*,\n",
    "or (in NLP) a *part-of-speech* (POS) tag, which specifies the low-level role that token has within\n",
    "the entire parse. For example, in the NLP domain each sentence in the training corpus\n",
    "could (in principle) could be deterministically POS-tagged prior to using the inside-outside algorithm. Thus, these POS-tags could represent the leaf states of the corresponding parse.\n",
    "Alternatively, if we are willing to assume that the assigned POS-tags are always perfectly accurate (which is not true, in practice), then we could instead use the POS-tags themselves as tokens.\n",
    "\n",
    "Note that POS-tagging the corpus offers the opportunity to automatically build a stochastic mapping from the vocabulary to the POS-tags. In fact, if we subsequently dropped the assigned POS-tags prior to building the grammar, then the use of the stochastic mapping from tokens to leaf states  permits\n",
    "some error correction of incorrectly assigned POS-tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0fc435",
   "metadata": {},
   "source": [
    "As discussed in the \n",
    "[previous](#Root-states \"Section: Root states\") section,\n",
    "using special leaf states causes difficulty for one-token sequences \n",
    "(e.g. from the sentence \"*Go!*\"), since then the leaf state $S_1$ and the root state $S_{1:1}$ \n",
    "are identical, implying that \n",
    "$\\mathcal{S}_\\mathtt{leaf}\\cap\\mathcal{S}_\\mathtt{root}\\neq\\emptyset$.\n",
    "Two-token sequences (e.g. from \"*Go home!*\") are also potentially problematic, since\n",
    "then the parse tree has no intermediate states (see the\n",
    "[next](#Intermediate-states \"Section: Intermediate states\") section).\n",
    "Consequently,\n",
    "the appropriate binary rule, say $\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$,\n",
    "for a two-token sequence\n",
    "must obey $\\sigma_i\\in\\mathcal{S}_\\mathtt{root}$ but have\n",
    "$\\sigma_j,\\sigma_k\\in\\mathcal{S}_\\mathtt{leaf}$.\n",
    "This means that rules of the (templated) form\n",
    "$\\mathcal{S}_\\mathtt{root}\\rightarrow\\mathcal{S}_\\mathtt{leaf}\\oplus\n",
    "\\mathcal{S}_\\mathtt{leaf}$ must be permitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8581f6",
   "metadata": {},
   "source": [
    "### Intermediate states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1020a18",
   "metadata": {},
   "source": [
    "The intermediate states $\\mathcal{S}_\\mathtt{int}\\subseteq\\mathcal{S}$ \n",
    "are assigned to nodes in the\n",
    "binary parse tree that are neither the root node nor any of the leaf nodes.\n",
    "Recall that for a sequence of length $T=|\\mathbf{y}|$, the\n",
    "binary parse tree has $2T-1$ non-terminal nodes, with $T$ leaf nodes, $1$ root node \n",
    "(which is also the leaf node when $T=1$), and thus\n",
    "$T-\\delta(T>1)-1$ intermediate nodes.\n",
    "Consequently, for sequences of\n",
    "length $T\\ge 3$, the parse tree has one or more intermediate nodes.\n",
    "Hence, such a tree must contain at least one binary rule,\n",
    "e.g. $\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$, that results in an intermediate \n",
    "head state,\n",
    "i.e. $\\sigma_i\\in\\mathcal{S}_\\mathtt{int}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907d48b",
   "metadata": {},
   "source": [
    "For example, the three-token sentence \"*You go home!*\" will (by dropping punctuation) give rise to a parse tree having exactly one intermediate state, \n",
    "with a binary rule taking the form\n",
    "$\\mathcal{S}_\\mathtt{int}\\rightarrow\\mathcal{S}_\\mathtt{leaf}\\oplus\n",
    "\\mathcal{S}_\\mathtt{leaf}$. For the root state, the binary rule must therefore\n",
    "take either the form\n",
    "$\\mathcal{S}_\\mathtt{root}\\rightarrow\\mathcal{S}_\\mathtt{leaf}\\oplus\n",
    "\\mathcal{S}_\\mathtt{int}$, or\n",
    "$\\mathcal{S}_\\mathtt{root}\\rightarrow\\mathcal{S}_\\mathtt{int}\\oplus\n",
    "\\mathcal{S}_\\mathtt{leaf}$.\n",
    "\n",
    "At this point, it is feasible to separate leaf states from intermediate states, i.e.\n",
    "$\\mathcal{S}_\\mathtt{leaf}\\cap\\mathcal{S}_\\mathtt{int}=\\emptyset$.\n",
    "In general, each subparse would then take the form\n",
    "$\\mathcal{S}_\\mathtt{int}\\rightarrow\\left(\n",
    "\\mathcal{S}_\\mathtt{leaf}\\mid\\mathcal{S}_\\mathtt{int}\\right)\\oplus\n",
    "\\left(\\mathcal{S}_\\mathtt{leaf}\\mid\\mathcal{S}_\\mathtt{int}\\right)$.\n",
    "The corresponding general form for the root state is therefore also\n",
    "$\\mathcal{S}_\\mathtt{root}\\rightarrow\\left(\n",
    "\\mathcal{S}_\\mathtt{leaf}\\mid\\mathcal{S}_\\mathtt{int}\\right)\\oplus\n",
    "\\left(\\mathcal{S}_\\mathtt{leaf}\\mid\\mathcal{S}_\\mathtt{int}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46ff2e",
   "metadata": {},
   "source": [
    "### Marker states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59bbb59",
   "metadata": {},
   "source": [
    "Earlier sections have alluded to some of the grammatical problems caused by parsing\n",
    "very short sequences, i.e. the potential confusion of states between the subsets\n",
    "$\\mathcal{S}_\\mathtt{root}$, $\\mathcal{S}_\\mathtt{leaf}$ and $\\mathcal{S}_\\mathtt{int}$.\n",
    "However, we have overlooked another possible solution, namely the fact that we are\n",
    "parsing *complete* sequences. Notionally, each complete sequence is distinguished\n",
    "from a partial sequence (i.e. a subsequence) by having both an implicit start and\n",
    "and an implicit end.\n",
    "\n",
    "Hence, we could make the sequence start and end explicit by introducing one or more\n",
    "marker states, $\\mathcal{S}_\\mathtt{mark}$.\n",
    "Since the parse preserves token ordering, it is unclear whether we need two markers,\n",
    "e.g. $\\mathcal{S}_\\mathtt{mark}=\\{\\triangleleft,\\triangleright\\}$, or if one marker suffices,\n",
    "e.g. $\\mathcal{S}_\\mathtt{mark}=\\{\\triangleright\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb70e0b",
   "metadata": {},
   "source": [
    "Assuming the former, the imperative sentence \"*Go!*\" then becomes the sequence\n",
    "$(\\square,\\mathtt{go},\\square)$,\n",
    "where we have taken $\\nu_0=\\square$ to be a non-observable pseudo-token corresponding to both\n",
    "start state $\\triangleleft$ and end state\n",
    "$\\triangleright$.\n",
    "An appropriate parse might then utilise the rules $\\mathtt{V}_\\mathtt{nosubj}\\rightarrow\\triangleleft\\oplus\\mathtt{V}$\n",
    "and $\\mathtt{S}\\rightarrow\\mathtt{V}_\\mathtt{nosubj}\\oplus\\triangleright$.\n",
    "Alternatively, the grammar might be \n",
    "$\\mathtt{V}_\\mathtt{noobj}\\rightarrow\\mathtt{V}\\oplus\\triangleright$\n",
    "and \n",
    "$\\mathtt{S}\\rightarrow\\triangleleft\\oplus\\mathtt{V}_\\mathtt{noobj}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207c5ab",
   "metadata": {},
   "source": [
    "However, if we use only one marker, then the sentence \"*Go!*\" becomes the sequence\n",
    "$(\\mathtt{go},\\square)$, with parse $\\mathtt{S}\\rightarrow\\mathtt{V}\\oplus\\triangleright$.\n",
    "\n",
    "In either case, for longer sequences the issue arises of whether or not the markers are all allowed to be combined into intermediate states. For example, given the sequence\n",
    "$(\\square,\\mathtt{The},\\mathtt{cat},\\mathtt{sat},\\square)$\n",
    "with leaf states\n",
    "$(\\triangleleft,\\mathtt{D},\\mathtt{N},\\mathtt{V},\\triangleright)$, \n",
    "what does it mean if $(\\triangleleft,\\mathtt{D})$ may combine, or \n",
    "$(\\triangleleft,\\mathtt{NP})$? Essentially, a binary combination with a marker corresponds to a unary type-raising rule, but this would only be permitted at the ends of the sequence. \n",
    "\n",
    "We could, instead, restrict the grammar such that the markers may only be combined after all observable tokens have been parsed. Even in this restricted case,\n",
    "the final effect of type-raising means that we could now separate root states from\n",
    "leaf states and intermediate states, i.e. \n",
    "$\\mathcal{S}_\\mathtt{root}\\cap\\mathcal{S}_\\mathtt{leaf}=\\emptyset$ and\n",
    "$\\mathcal{S}_\\mathtt{root}\\cap\\mathcal{S}_\\mathtt{int}=\\emptyset$.\n",
    "For a single marker, the overall parse would then take the simpler form\n",
    "$\\mathcal{S}_\\mathtt{root}\\rightarrow\\left(\n",
    "\\mathcal{S}_\\mathtt{leaf}\\mid\\mathcal{S}_\\mathtt{int}\\right)\\oplus\\triangleright$.\n",
    "\n",
    "Note that if this final type-raising is generally undesirable, \n",
    "i.e. for $\\left|\\mathcal{S}_\\mathtt{root}\\right|>1$,\n",
    "then we could instead take two marker states\n",
    "$\\mathcal{S}_\\mathtt{mark}=\\{\\triangleleft,\\triangleright\\}$, but take $\\triangleleft$ as the single, unique\n",
    "[root](#Root-states \"Section: Root state\") state $\\mathcal{S}_\\mathtt{root}=\\{\\triangleleft\\}$\n",
    "with $\\mathcal{S}_\\mathtt{root}\\cap\\mathcal{S}_\\mathtt{leaf}=\\emptyset$.\n",
    "Consequently, the root rule simplifies to the form $\\triangleleft\\rightarrow(\\mathcal{S}_\\mathtt{leaf}|\\mathcal{S}_\\mathtt{int})\\oplus\\triangleright$.\n",
    "Such a situation is discussed further in a \n",
    "[later](\"#Sequential-Dependencies \"Section: Sequential Dependencies\")\n",
    "section. In practice, however, this is all just syntactic sugar, since the inner parse corresponds\n",
    "to $\\mathcal{S}_\\mathtt{root}=\\mathcal{S}_\\mathtt{leaf}\\cup\\mathcal{S}_\\mathtt{int}$ in the\n",
    "orginal formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec6c85",
   "metadata": {},
   "source": [
    "## Parsing Versus Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628c684",
   "metadata": {},
   "source": [
    "The rules for the inside-outside algorithm have been defined in a top-down fashion\n",
    "suitable for token generation. In particular, the unary rules take the form\n",
    "$\\sigma_i\\rightarrow\\nu_m$ for state $\\sigma_i\\in\\mathcal{S}$ and \n",
    "token $\\nu_m\\in\\mathcal{Y}$. \n",
    "Likewise, the binary rules take the form \n",
    "$\\sigma_i\\rightarrow\\sigma_j\\oplus\\sigma_k$ for \n",
    "$\\sigma_i,\\sigma_j,\\sigma_k\\in\\mathcal{S}$.\n",
    "\n",
    "However, we have repeatedly talked about forming a parse tree that spans a sequence\n",
    "$\\mathbf{y}$ of tokens. In this context, we need to map each token into the possible\n",
    "leaf states via reversed unary rules like $\\nu_m\\rightarrow\\sigma_i$. Similarly, we\n",
    "need to be able to combine adjacent states into a single state via reversed binary\n",
    "rules like $\\sigma_j\\oplus\\sigma_k\\rightarrow\\sigma_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d325bb",
   "metadata": {},
   "source": [
    "### Unary parsing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47910ebd",
   "metadata": {},
   "source": [
    "In a [previous](#Leaf-states \"Section: Leaf states\") section, we discussed the possibility of creating a mapping from tokens to leaf states via POS-tagging the training corpus. This mapping is suitable for parsing as it specifies the\n",
    "probabilities\n",
    "\\begin{eqnarray}\n",
    "P(\\nu_m\\rightarrow\\sigma_i) & \\doteq & P(S_t=\\sigma_i\\mid Y_t=\\nu_m)\n",
    "\\end{eqnarray}\n",
    " of the bottom-up unary rules.\n",
    "However, recall that for generation we instead need the probabilities\n",
    "\\begin{eqnarray}\n",
    "P(\\sigma_i\\rightarrow\\nu_m) & \\doteq & P(Y_t=\\nu_m\\mid S_t=\\sigma_i)\n",
    "~\\doteq~ \\beta_{t:t}(i)\n",
    "\\end{eqnarray}\n",
    "of the top-down rules. In order to make this conversion, we first note that\n",
    "\\begin{eqnarray}\n",
    "P(S_t=\\sigma_i, Y_t=y_t) & = &\n",
    "P(S_t=\\sigma_i\\mid Y_t=y_t)\\,P(Y_t=y_t)\n",
    "~=~P(Y_t=y_t\\mid S_t=\\sigma_i)\\,P(S_t=\\sigma_i)\\,.\n",
    "\\end{eqnarray}\n",
    "Now, in practice it is difficult to compute $P(Y_t)$, since for parsing we need to allow\n",
    "for new tokens not found in the training corpus $\\mathbb{Y}$. Hence, we instead\n",
    "replace usages of $\\beta_{t:t}(i)$ by\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\beta}_{t:t}(i) & \\doteq & \n",
    "\\frac{P(Y_t=y_t\\mid S_t=\\sigma_i)}{P(Y_t=y_t)}\n",
    "~=~\\frac{P(S_t=\\sigma_i\\mid Y_t=y_t)}{P(S_t=\\sigma_i)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "This trick works because\n",
    "it turns out that the unknown $P(Y_t)$ terms will cancel out when conditioning\n",
    "on the observed sequence $\\mathbf{y}$. Additionally, given a POS-tagged corpus, the same trick can also be used in a one-dimensional HMM to sharpen the probabilities from\n",
    "$P(S_t=\\sigma_i\\mid Y_t=y_t)$ to $P(S_t=\\sigma_i\\mid\\mathbf{Y}=\\mathbf{y})$,\n",
    "although technically the latter probabilities are not independent\n",
    "across $t=1,2,\\ldots,|\\mathbf{y}|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115d268",
   "metadata": {},
   "source": [
    "It should be noted that if we have pre-specified both $P(S_t=\\sigma_i\\mid Y_t=\\nu_m)$ and $P(S_t=\\sigma_i)$, then there is no need to estimate the matrix $\\mathbf{B}$\n",
    "of top-down unary rule probabilities. However, if we do not have a POS-tagged corpus, how do we compute these values?\n",
    "\n",
    "We recall from a \n",
    "[previous](#Expected-unary-rule-counts \"Section: Expected unary rule counts\") section\n",
    "that\n",
    "\\begin{eqnarray}\n",
    "\\hat{b}_{m|i} & = & \n",
    "\\frac{\\hat{c}_{im}}{\\hat{c}_{i\\cdot}}~=~\n",
    "\\frac{b_{m|i}\\,\\check{c}_{im}}\n",
    "{\\sum_{m'=1}^{\\left|\\mathcal{Y}\\right|}b_{m'|i}\\,\\check{c}_{im'}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $c_{im}$ is the expected joint count of $(\\sigma_i,\\nu_m)$, and $b_{m|i}$ is just our original\n",
    "emission probability $b_{im}$ repurposed to explicitly reflect the required conditionality. \n",
    "Hence, if we define $\\gamma_i\\doteq P(S_t=\\sigma_i)$, then we can estimate this\n",
    "probability via\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\gamma}_i & = & \n",
    "\\frac{\\hat{c}_{i\\cdot}}{\\hat{c}_{\\cdot\\cdot}}\n",
    "~=~\n",
    "\\frac{\\sum_{m=1}^{\\left|\\mathcal{Y}\\right|}b_{m|i}\\,\\check{c}_{im}}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\n",
    " \\sum_{m'=1}^{\\left|\\mathcal{Y}\\right|}b_{m'|i'}\\,\\check{c}_{i'm'}}\n",
    " \\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we may estimate $P(S_t=\\sigma_i\\mid Y_t=\\nu_m)$ via\n",
    "\\begin{eqnarray}\n",
    "\\hat{b}_{i|m} & = &\n",
    "\\frac{\\hat{c}_{im}}{\\hat{c}_{\\cdot m}}\n",
    "~=~\n",
    "\\frac{b_{m|i}\\,\\check{c}_{im}}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}b_{m|i'}\\,\\check{c}_{i'm}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Alternatively, if we modify the inside-outside estimation procedure to always compute and retain $\\hat{\\boldsymbol{\\Gamma}}=[\\hat{\\gamma}_i]$, then we may subsequently compute\n",
    "\\begin{eqnarray}\n",
    "\\hat{b}_{i|m} & = &\n",
    "\\frac{\\hat{c}_{im}/\\hat{c}_{\\cdot\\cdot}}{\\hat{c}_{\\cdot m}/\\hat{c}_{\\cdot\\cdot}}\n",
    "~=~\n",
    "\\frac{\\hat{b}_{m|i}\\,\\hat{\\gamma}_i}\n",
    "{\\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\\hat{b}_{m|i'}\\,\\hat{\\gamma}_{i'}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155eaf3",
   "metadata": {},
   "source": [
    "Clearly, the more fundamental quantity is that of $c_{im}$,\n",
    "or perhaps\n",
    "\\begin{eqnarray}\n",
    "b_{im} & \\doteq & P(S_t=\\sigma_i,Y_y=\\nu_m)\n",
    "~=~\\frac{c_{im}}{c_{\\cdot\\cdot}}\\,,\n",
    "\\end{eqnarray}\n",
    "from which we may compute $b_{m|i}$, $b_{i|m}$ and $\\gamma_i=b_{i\\cdot}$.\n",
    "We may also compute\n",
    "\\begin{eqnarray}\n",
    "\\delta_m & \\doteq & P(Y_t=\\nu_m)~=~b_{\\cdot m}\\,,\n",
    "\\end{eqnarray}\n",
    "which presumably should remain constant, since this can be computed directly from\n",
    "the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10917b",
   "metadata": {},
   "source": [
    "We observe that\n",
    "\\begin{eqnarray}\n",
    "\\hat{c}_{im} & = & b_{m|i}\\,\\check{c}_{im}\n",
    "~=~\\frac{c_{im}}{c_{i\\cdot}}\\,\\check{c}_{im}\n",
    "\\\\& = &\n",
    "\\frac{c_{im}/c_{\\cdot\\cdot}}{c_{i\\cdot}/c_{\\cdot\\cdot}}\\,\\check{c}_{im}\n",
    "~=~\\frac{b_{im}\\,\\check{c}_{im}}{b_{i\\cdot}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and hence\n",
    "\\begin{eqnarray}\n",
    "\\hat{b}_{im} & = &\n",
    "\\frac{\n",
    " \\frac{b_{im}\\,\\check{c}_{im}}{b_{i\\cdot}}\n",
    "}{\n",
    " \\sum_{i'=1}^{\\left|\\mathcal{S}\\right|}\n",
    " \\frac{\\sum_{m'=1}^{\\left|\\mathcal{Y}\\right|}b_{i'm'}\\,\\check{c}_{i'm'}}\n",
    " {b_{i'\\cdot}}\n",
    "}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a02f5",
   "metadata": {},
   "source": [
    "## Sequential Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1b5bf",
   "metadata": {},
   "source": [
    "It was noted in a [previous](#Top-down-generative-model \"Section: Top-down generative model\")\n",
    "section that although binary rules of the form $\\texttt{A}\\rightarrow\\texttt{B}\\oplus\\texttt{C}$\n",
    "specify a left-to-right ordering, they do not specify any dependence between states $\\texttt{C}$ and $\\texttt{B}$. This is in contrast to a \n",
    "[HMM](#Comparison-with-a-HMM \"Section: Comparison with a HMM\"),\n",
    "which does not have any hierarchical structure but instead models the sequence $\\mathbf{y}$ via\n",
    "sequential dependencies between the states $\\mathbf{s}$.\n",
    "\n",
    "In practice, this lack of sequential dependence reveals itself during parsing. For example, since \"*water*\"\n",
    "may either be a noun $\\texttt{N}$ or a verb $\\texttt{V}$, the imperative sentence \"*Water the garden!*\"\n",
    "could either have joint states $(\\texttt{V},\\texttt{D},\\texttt{N})$ or \n",
    "$(\\texttt{N},\\texttt{D},\\texttt{N})$. This was actually my test sentence for validating third-party POS taggers;\n",
    "it turned out that older POS-taggers erroneously \n",
    "interpreted the sentence as the apposition \"*Water, the garden!*\" (i.e. \"*water*\" equals \"*the garden*\"),\n",
    "due to the empirical fact that $P(S_t=\\texttt{N}\\mid Y_t=\\texttt{water})\\gg P(S_t=\\texttt{V}\\mid Y_t=\\texttt{water})$.\n",
    "This problem seems to have been fixed in modern POS taggers.\n",
    "\n",
    "For a context-free grammar, the only disambiguation between these choices comes via the relative probabilities\n",
    "of the rules $\\texttt{VP}\\rightarrow\\texttt{V}\\oplus\\texttt{NP}$ versus \n",
    "$\\texttt{NP}\\rightarrow\\texttt{N}\\oplus\\texttt{NP}$. More generally, we are trying to decide between\n",
    "a rule, say $\\texttt{A}\\rightarrow\\texttt{B}\\oplus\\texttt{C}$, \n",
    "having high probability but with a leaf state, say\n",
    "$\\texttt{C}$, having low probability given the token it spans, compared to another rule, say\n",
    "$\\texttt{A}\\rightarrow\\texttt{D}\\oplus\\texttt{E}$, having a lower probability but with the leaf state $\\texttt{D}$ having high probability with respect to the same token. The parser will tend to waste a lot of time\n",
    "trying to match rules to leaf state $\\texttt{D}$ before it gets around to leaf state $\\texttt{C}$.\n",
    "\n",
    "One way of improving this situation, as noted in a\n",
    "[previous](#Unary-parsing-rules \"Section: Unary parsing rules\") section,\n",
    "is to first sharpen the leaf state probabilities by running a HMM\n",
    "across the tokens $\\mathbf{y}$, and then computing the (albeit non-independent) posteriors \n",
    "$P(S_t\\mid\\mathbf{y})$ for $t=1,2,\\ldots,|\\mathbf{y}|$.\n",
    "Another option is to include explicit sequential dependence in the grammar $\\mathcal{G}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b09042",
   "metadata": {},
   "source": [
    "### Binary rule backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc30cf",
   "metadata": {},
   "source": [
    "Recall that the binary rule $\\texttt{A}\\rightarrow\\texttt{B}\\oplus\\texttt{C}$ has probability\n",
    "$P(\\texttt{B},\\texttt{C}\\mid\\texttt{A})$.\n",
    "Hence, we can include explicit sequential dependence in the grammar $\\mathcal{G}$ by observing that\n",
    "\\begin{eqnarray}\n",
    "P(\\texttt{B},\\texttt{C}\\mid\\texttt{A}) & = & \n",
    "P(\\texttt{B}\\mid\\texttt{A})\\,P(\\texttt{C}\\mid\\texttt{B},\\texttt{A})\\,,\n",
    "\\end{eqnarray}\n",
    "in general. Note, however, that the latter term's dependence upon both the child state $\\texttt{B}$ and\n",
    "the parent state $\\texttt{A}$ means that the grammar has become (weakly) context-sensitive.\n",
    "\n",
    "For simplicity, we could reduce the new grammar to being context-free by backing-off from\n",
    "$P(\\texttt{C}\\mid\\texttt{B},\\texttt{A})$ to $P(\\texttt{C}\\mid\\texttt{B})$,\n",
    "which is equivalent to defining\n",
    "\\begin{eqnarray}\n",
    "P(\\texttt{B},\\texttt{C}\\mid\\texttt{A}) & \\doteq &\n",
    "P_\\texttt{left}(\\texttt{B}\\mid\\texttt{A})\\,P_\\texttt{right}(\\texttt{C}\\mid\\texttt{B})\\,.\n",
    "\\end{eqnarray}\n",
    "This symbollicaly corresponds to a new rule of the form \n",
    "$\\texttt{A}\\overset{\\tiny\\texttt{left}}{\\rightarrow}\\texttt{B}\\overset{\\tiny\\texttt{right}}{\\rightarrow}\\texttt{C}$,\n",
    "where $\\texttt{A}\\overset{\\tiny\\texttt{left}}{\\rightarrow}\\texttt{B}$\n",
    "is equivalent to the \n",
    "(expansion or substitution or rewrite)\n",
    "rule $\\texttt{A}\\rightarrow\\texttt{B}\\oplus\\cdot$ for some placeholder $\\cdot$,\n",
    "and $\\texttt{B}\\overset{\\tiny\\texttt{right}}{\\rightarrow}\\texttt{C}$ is equivalent to the\n",
    "(concatenation)\n",
    "rule $\\texttt{B}\\oplus\\cdot\\rightarrow\\texttt{B}\\oplus\\texttt{C}$, which is not\n",
    "in Chomsky normal form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4cc70f",
   "metadata": {},
   "source": [
    "Let us now use the [marker](#Marker-states \"Section: Marker states\") states\n",
    "$\\mathcal{S}_\\mathtt{mark}=\\{\\triangleleft,\\triangleright\\}$ to denote the start and end, respectively, of a \n",
    "complete sequence. For simplicity, we assume a single, overall\n",
    "[root](#Root-states \"Section: Root states\") state $\\mathcal{S}_\\mathtt{root}=\\{\\triangleleft\\}$,\n",
    "with probability $P_\\mathtt{root}(\\triangleleft)=1$.\n",
    "Similarly, we assume a non-observable, pseudo-token $\\nu_0=\\square$ corresponding to the \n",
    "[leaf](#Leaf-states \"Section: Leaf states\") end state $\\sigma_0=\\triangleright$,\n",
    "such that $P_\\mathtt{token}(\\square\\mid\\triangleright)=1$ and \n",
    "$P_\\mathtt{token}(\\square\\mid\\sigma_i)=0$ for all $i>0$.\n",
    "\n",
    "Under this modified grammar, the derivation of our example sentence is now shown in the figure below.\n",
    "<img src=\"dependency_generated_sentence.png\" \n",
    "     title=\"Hierarchical parsing model of a sentence with sequential dependencies\" \n",
    "     width=\"40%\">\n",
    "\n",
    "Note that the horizontal, dashed edges correspond to sequential dependencies, and the slanted, dotted edges\n",
    "correspond to context-sensitive dependencies, which would be dropped for the simplified, context-free grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96ab1c",
   "metadata": {},
   "source": [
    "The joint probability of this derivation is now\n",
    "\\begin{eqnarray}\n",
    "P(\\mathbf{S}=\\mathbf{s},\\mathbf{Y}=\\mathbf{y}) & = &\n",
    "P_\\mathtt{root}(\\triangleleft)\\,\n",
    "P_\\mathtt{left}(S\\mid\\triangleleft)\\,P_\\mathtt{right}(\\triangleright\\mid\\texttt{S})\\,\n",
    "P_\\mathtt{left}(\\texttt{NP}\\mid\\texttt{S})\\,P_\\mathtt{right}(\\texttt{VP}\\mid\\texttt{NP})\n",
    "\\\\&&{}\\times\n",
    "P_\\mathtt{left}(\\texttt{D}\\mid\\texttt{NP})\\,P_\\mathtt{right}(\\texttt{N}\\mid\\texttt{D})\\,\n",
    "P_\\mathtt{token}(\\texttt{The}\\mid\\texttt{D})\\,P_\\mathtt{token}(\\texttt{cat}\\mid\\texttt{N})\n",
    "\\\\&&{}\\times\n",
    "P_\\mathtt{left}(\\texttt{V}\\mid\\texttt{VP})\\,P_\\mathtt{right}(\\texttt{PP}\\mid\\text{V})\\,\n",
    "P_\\mathtt{token}(\\texttt{sat}\\mid\\texttt{V})\n",
    "\\\\&&{}\\times\n",
    "P_\\mathtt{left}(\\texttt{P}\\mid\\texttt{PP})\\,P_\\mathtt{right}(\\texttt{NP}\\mid\\text{P})\\,\n",
    "P_\\mathtt{token}(\\texttt{on}\\mid\\texttt{P})\n",
    "\\\\&&{}\\times\n",
    "P_\\mathtt{left}(\\texttt{D}\\mid\\texttt{NP})\\,P_\\mathtt{right}(\\texttt{N}\\mid\\text{D})\\,\n",
    "P_\\mathtt{token}(\\texttt{the}\\mid\\texttt{D})\\,P_\\mathtt{token}(\\texttt{mat}\\mid\\texttt{N})\\,\n",
    "P_\\mathtt{token}(\\square\\mid\\triangleright)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a4db4",
   "metadata": {},
   "source": [
    "Note that the only effect this binary rule backoff has on the inside-outside algorithm is to replace\n",
    "all occurrences of the probability $a_{ijk}$ by $d_{ij}\\,e_{jk}$, say, where\n",
    "$P_\\mathtt{left}(S_{s:r}=\\sigma_j\\mid S_{s:t}=\\sigma_i)\\doteq d_{ij}$, and\n",
    "$P_\\mathtt{right}(S_{r+1:t}=\\sigma_k\\mid S_{s:r}=\\sigma_j)\\doteq e_{jk}$.\n",
    "\n",
    "The estimates for these probability tables are\n",
    "\\begin{eqnarray}\n",
    "\\hat{d}_{ij}~=~\\frac{\\hat{c}_{ij\\cdot}}{\\hat{c}_{i\\cdot\\cdot}}\\,, &&\n",
    "\\hat{e}_{jk}~=~\\frac{\\hat{c}_{\\cdot jk}}{\\hat{c}_{\\cdot j\\cdot}}\\,,\n",
    "\\end{eqnarray}\n",
    "respectively. Note that it might be of interest to determine if there are simpler formulae."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617cd61",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09c501",
   "metadata": {},
   "source": [
    "*Chunking* is the process of partitioning a sequence into contiguous subsequences, called *chunks*, such that each chunk has self-consistent semantics with respect to the grammar $\\mathcal{G}$.\n",
    "For example, an English sentence could be chunked into noun phrases and verb phrases, et cetera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9a350",
   "metadata": {},
   "source": [
    "As an example, consider the sentence \"*The black cat purred.*\", with *unchunked* ground-truth leaf states\n",
    "$\\mathbf{s}^\\texttt{leaf}_{1:4}=\\langle\\texttt{D},\\texttt{J},\\texttt{N},\\texttt{V}\\rangle$,\n",
    "where '$\\langle$' denotes the start of a sequence, and '$\\rangle$' denotes the end of a sequence.\n",
    "It is apparent that the phrase \n",
    "\"*The black cat*\" is a noun phrase (i.e. $\\texttt{NP}$), corresponding to the chunk \n",
    "$\\mathbf{s}^\\texttt{leaf}_{1:3}=(\\texttt{D},\\texttt{J},\\texttt{N})$. Consequently, the remaining chunk must be\n",
    "$\\mathbf{s}^\\texttt{leaf}_{4:4}=(\\texttt{V})$.\n",
    "At the higher level, the chunking gives parent (or intermediate) states\n",
    "$\\mathbf{s}^\\texttt{int}_{1:4}=(\\texttt{NP},\\texttt{VP})$. Finally, at the highest level,\n",
    "chunking gives the grandparent (or root) state(s) $\\mathbf{s}^\\texttt{root}_{1:4}=(\\texttt{S})$.\n",
    "\n",
    "The context-sensistive, top-down derivation of this nested chunking is shown in the figure below.\n",
    "<img src=\"context_sensitive.png\" \n",
    "     title=\"Context-sensitive chunking model of a sentence with sequential dependencies\" \n",
    "     width=\"40%\">\n",
    "\n",
    "Observe that the current context (i.e. the parent state) is carried along downward transitions from parent level to child level, \n",
    "then used across horizontal transitions at the same level, and lastly\n",
    "restored along upward transitions from child level to parent level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96397224",
   "metadata": {},
   "source": [
    "Note that to obtain a context-free approximation to this context-sensitive derivation, the parent context is dropped from each transition, and each node must only depend on its head node. Furthermore, since context is now missing, for every transition from parent to child, the parent can no longer be revisited.\n",
    "For the (mostly) top-down derivation, this causes only a slight change, as shown in the figure below.\n",
    "\n",
    "<img src=\"context_free_top_down.png\" \n",
    "     title=\"Context-free, top-down chunking model of a sentence with sequential dependencies\" \n",
    "     width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bd272",
   "metadata": {},
   "source": [
    "However, the chunking process described above follows an alternative, (mostly) bottom-up algorithm\n",
    "that is more suitable for sequence parsing, although it can also be used for sequence generation.\n",
    "Under this model, each parent state is only created once its child chunk has been closed.\n",
    "The derivation for our example sentence is shown in the figure below.\n",
    "\n",
    "<img src=\"context_free_bottom_up.png\" \n",
    "     title=\"Context-free, bottom-up chunking model of a sentence with sequential dependencies\" \n",
    "     width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c368b4a",
   "metadata": {},
   "source": [
    "The notion of a chunking grammar is pursued further in\n",
    "[another](chunking_grammar.ipynb \"Notebook: Chunking Grammar\") notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499bee1",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6b8f2",
   "metadata": {},
   "source": [
    "[1] J. Baker (1979): \"*Trainable grammars for speech recognition*\",\n",
    "Speech communication papers presented at the 97th meeting of the Acoustical Society of America.\n",
    "\n",
    "[2] Karim Lari and Steve J. Young (1990): \"*The estimation of stochastic context-free grammars using the inside–outside algorithm*\", Computer Speech and Language, 4:35–56.\n",
    "[(PDF)](http://courses.cs.washington.edu/courses/cse599d1/16sp/lari-young-90.pdf \"washington.edu\")\n",
    "\n",
    "[3] K. Lari and S. J. Young (1991): \"*Applications of stochastic context-free grammars using the Inside-Outside algorithm*\", Computer Speech and Language, 5:237-257."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
