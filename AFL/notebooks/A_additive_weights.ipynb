{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e68c05",
   "metadata": {},
   "source": [
    "# Appendix A: Additively Weighted Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d134dfe",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969fe18",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69aef6",
   "metadata": {},
   "source": [
    "The task is to predict the class $c\\in\\mathcal{C}$ that best matches a sequence of observations $\\vec{\\mathbf{x}}\\doteq (\\mathbf{x}_1,\\mathbf{x}_2,\\ldots,\\mathbf{x}_K)$.\n",
    "In some areas, the sequence $\\vec{\\mathbf{x}}$ is known as the *context*, and the class label $c$ is known as the *target*.  \n",
    "\n",
    "We assume the predictor takes the form of some probabilistic model $P(c\\mid\\vec{\\mathbf{x}},\\Theta)$ with unknown parameters $\\Theta$.\n",
    "The parameters are to be estimated from *supervised* training data, with known class labels $\\mathbf{C}\\doteq\\left[c^{(d)}\\right]_{d=1}^N$,\n",
    "and known sequences $\\mathbf{X}\\doteq\\left[\\vec{\\mathbf{x}}^{(d)}\\right]_{d=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e8029",
   "metadata": {},
   "source": [
    "### Additive model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c3439",
   "metadata": {},
   "source": [
    "For a single sequence $\\vec{\\mathbf{x}}=(\\mathbf{x}_k)_{k=1}^K$ of observations, we assume *a priori* that some particular observation in the sequence, say $\\mathbf{x}_{k^*}$, is the best predictor of the target class, $c$.\n",
    "In other words, we suppose that a hypothetical generative process first samples the component $k^*$ from some\n",
    "distribution, say $P(k\\mid\\Phi)$, and then samples the class $c$ from another distribution,\n",
    "say $P(c\\mid k^*,\\vec{\\mathbf{x}},\\Psi)$. Thus, the generative process\n",
    "is described in general by the joint distribution\n",
    "\\begin{eqnarray}\n",
    "P(k,c\\mid\\vec{\\mathbf{x}},\\Theta) & \\doteq & P(k\\mid\\Phi)\\,P(c\\mid k, \\vec{\\mathbf{x}},\\Psi)\\,,\n",
    "\\end{eqnarray}\n",
    "with parameters $\\Theta=(\\Phi,\\Psi)$. Observe that summing both sides over $k$ exposes the\n",
    "underlying modelling assumption, namely that\n",
    "\\begin{eqnarray}\n",
    "P(k,\\mid\\vec{\\mathbf{x}},\\Theta) & \\doteq & P(k\\mid\\Phi)\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we are unable to model $k$ without knowledge of both $c$ and $\\vec{\\mathbf{x}}$ (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117874d",
   "metadata": {},
   "source": [
    "To specify the predictive model, we first define, for convenience, that\n",
    "$\\phi_k\\doteq P(k\\mid\\Phi)$, and $\\boldsymbol{\\phi}\\doteq (\\phi_k)_{k=1}^{K}$.\n",
    "Consequently, we may consider the model to be (partly) parameterised either by $\\Phi$ or by $\\boldsymbol{\\phi}$,\n",
    "interchangeably.\n",
    "\n",
    "Secondly, we consider the sub-model $P(c\\mid k,\\vec{\\mathbf{x}},\\Psi)$. In general, we \n",
    "impose no restrictions on the sub-models, other than to assume that parameters $\\Phi$ and $\\Psi$ are independent.\n",
    "However, since we are supposing that $k$ selects a single observation $\\mathbf{x}_k$ from the sequence\n",
    "$\\vec{\\mathbf{x}}$, then it does makes sense to assume that\n",
    "\\begin{eqnarray}\n",
    "P(c\\mid k,\\vec{\\mathbf{x}},\\Psi) & \\doteq & P(c\\mid\\mathbf{x}_{k},\\Psi_k)\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\Psi\\doteq(\\Psi_k)_{k=1}^{K}$. That is, we consider $K$ arbitrary but independent sub-models.\n",
    "\n",
    "In practice, we must consider issues such as model overfitting and data scarcity. To combat model overfitting, \n",
    "we might assume that the sub-models all share the same parametric form, but with different parameters,\n",
    "e.g. $\\Psi_k$ for $k=1,\\ldots,K$. To overcome data scarcity, we might further assume that all sub-models share the same parameters, i.e. $\\Psi_k=\\bar{\\Psi}$. However, we use no such assumptions here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca02d54",
   "metadata": {},
   "source": [
    "Putting aside such issues, the desired predictive model is now given by\n",
    "\\begin{eqnarray}\n",
    "P(c\\mid\\vec{\\mathbf{x}},\\Theta) & = & \n",
    "\\sum_{k=1}^K P(k,c\\mid\\vec{\\mathbf{x}},\\Theta)\n",
    "~\\doteq~\\sum_{k=1}^K \\phi_k\\,P(c\\mid\\mathbf{x}_k,\\Psi_k)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, the predictive model takes the form of an additively weighted *mixture of experts*,\n",
    "with mixture component weights $\\boldsymbol{\\phi}$ satisfying $\\phi_k\\ge 0$ and $\\sum_{k=1}^{K}\\phi_k=1$.\n",
    "\n",
    "For the remainder of this document, we shall be concerned primarily with the estimation of the mixture\n",
    "weights $\\boldsymbol{\\phi}$ from the training data $\\mathbf{C}$ and $\\mathbf{X}$. For simplicity, we henceforth assume that the parameters $\\Psi$ have already been estimated in some unspecified fashion, and thus remain\n",
    "fixed. In other words, we assume that the experts, or sub-models, have already been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94986ec2",
   "metadata": {},
   "source": [
    "As the final step in our modelling, we may invert the predictive model to obtain the posterior component weights, namely\n",
    "\\begin{eqnarray}\n",
    "P(k\\mid c,\\vec{\\mathbf{x}},\\Theta) & = &\n",
    "\\frac{P(k, c\\mid\\vec{\\mathbf{x}},\\Theta)}{P(c\\mid\\vec{\\mathbf{x}},\\Theta)}\n",
    "~ \\doteq ~\n",
    "\\frac{\\phi_k\\,P(c\\mid\\mathbf{x}_k,\\Psi_k)}\n",
    "{\\sum_{\\tilde{k}=1}^K \\phi_\\tilde{k}\\,P(c\\mid\\mathbf{x}_\\tilde{k},\\Psi_\\tilde{k})}\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318025b",
   "metadata": {},
   "source": [
    "## Expectation-Maxmisation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645fe86",
   "metadata": {},
   "source": [
    "### Known, hidden and complete information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5051a808",
   "metadata": {},
   "source": [
    "The *expectation-maximisation* (EM) approach to parameter estimation differentiates between known data and hidden, or latent, information. If the hidden information were to become known, then we would have\n",
    "*complete* data. Hence, the approach focuses first on modelling the complete data.\n",
    "\n",
    "For the [additive model](#Additive-model \"Introduction: Additive model\"), \n",
    "our information would be complete if we knew the 'optimal' mixture component indices \n",
    "$\\mathbf{K}\\doteq [k^{(d)}]_{d=1}^{N}$ corresponding to the known sequences $\\mathbf{X}$.\n",
    "In that case, an appropriate log-likelihood of the complete data might be\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta) & \\doteq & \\ln P(\\mathbf{K},\\mathbf{C}\\mid\\mathbf{X},\\Theta)\n",
    "~ = ~ \\sum_{d=1}^{N}\\ln P\\left(k^{(d)}, c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590a3ed",
   "metadata": {},
   "source": [
    "### Expected log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1991f",
   "metadata": {},
   "source": [
    "In practice, for any arbitrary sequence $\\vec{\\mathbf{x}}$, the optimal index $k^*$ remains unknown,\n",
    "and thus $\\mathbf{K}$ represents hidden information.\n",
    "We defer this problem somewhat by now introducing notional binary indicators, namely $z_k\\doteq\\delta(k=k^*)$, \n",
    "such that $\\mathbf{z}\\doteq (z_k)_{k=1}^{K}$ is the hidden indicator vector for sequence $\\vec{\\mathbf{x}}$.\n",
    "Clearly, if we knew $k^*$ we would know $\\mathbf{z}$, and vice versa.\n",
    "\n",
    "The point of this alternative parameterisation is that the \n",
    "[joint model](#Additive-model \"Introduction: Additive model\") now takes the form\n",
    "\\begin{eqnarray}\n",
    "P(k^*,c\\mid\\vec{\\mathbf{x}},\\Theta) & = & \n",
    "\\prod_{k=1}^K\\left[\\phi_k\\,P(c\\mid\\mathbf{x}_{k},\\Psi_k)\\right]^{\\,z_k}\\,,\n",
    "\\end{eqnarray}\n",
    "with log-likelihood\n",
    "\\begin{eqnarray}\n",
    "\\ln P(k^*,c\\mid\\vec{\\mathbf{x}},\\Theta) & = & \\sum_{k=1}^K z_k\\ln\\left[\\phi_k\\,P(c\\mid\\mathbf{x}_{k},\\Psi_k)\n",
    "\\right]\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ae453",
   "metadata": {},
   "source": [
    "Now, for training sequence $\\vec{\\mathbf{x}}^{(d)}$, the corresponding indicator vector is $\\mathbf{z}^{(d)}$,\n",
    "such that the hidden information $\\mathbf{K}$ may be represented by $\\mathbf{Z}\\doteq\\left[\\mathbf{z}^{(d)}\\right]_{d=1}^{N}$.\n",
    "Consequently, given known data $\\mathbf{C}$ and $\\mathbf{X}$, the predictive log-likelihood \n",
    "takes the form\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta;\\mathbf{Z}) & = &\n",
    "\\sum_{d=1}^N\\sum_{k=1}^K z_k^{(d)}\\,\n",
    "\\ln\\left[\\phi_k\\,P\\left(c^{(d)}\\mid\\mathbf{x}^{(d)}_k,\\Psi_k\\right)\\right]\\,,\n",
    "\\end{eqnarray}\n",
    "where the explicit dependence upon $\\mathbf{Z}$ indicates that the likelihood still relies on hidden information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5478e4",
   "metadata": {},
   "source": [
    "In order to eliminate the unknown $\\mathbf{Z}$, the EM approach is to take the expectation of the log-likelihood\n",
    "with respect to the hidden information given the known data.\n",
    "We thus consider expectations $\\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{C},\\mathbf{X},\\Theta}[\\cdot]$ over $\\mathbf{Z}$, given the known data $\\mathbf{C}$ and $\\mathbf{X}$, dependent upon the parameter $\\Theta$\n",
    "to be estimated.\n",
    "\n",
    "Since we do not know the model parameters $\\Theta=(\\Phi,\\Psi)$, we start the estimation process with\n",
    "some known approximate values, say $\\Theta'=(\\Phi',\\Psi')$. However, recall for our\n",
    "[additive model](#Additive-model \"Introduction: Additive model\") that\n",
    "we have assumed for convenience that $\\Psi$ has been estimated separately, and is considered fixed, i.e. $\\Psi'=\\Psi$.\n",
    "\n",
    "Consequently, the expected log-likelihood is given by\n",
    "\\begin{eqnarray}\n",
    "Q(\\Theta,\\Theta') & \\doteq &\n",
    "\\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{C},\\mathbf{X},\\Theta'}[L(\\Theta;\\mathbf{Z})]\n",
    "~=~ \n",
    "\\sum_{d=1}^N\\sum_{k=1}^K \\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{C},\\mathbf{X},\\Theta'}\\left[z_k^{(d)}\\right]\\,\n",
    "\\ln\\left[\\phi_k\\,P(c^{(d)}\\mid\\mathbf{x}^{(d)}_k,\\Theta_k)\\right]\\,.\n",
    "\\end{eqnarray}\n",
    "However, since $z_k^{(d)}\\doteq\\delta(k^{(d)}=k)$, we observe that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{C},\\mathbf{X},\\Theta'}\\left[z_k^{(d)}\\right] & = & \n",
    "P(k^{(d)}=k\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta') ~\\doteq~ \\bar{w}_k^{(d)}\\,,\n",
    "\\end{eqnarray}\n",
    "such that $\\bar{w}_k^{(d)}$ is just the posterior mixture weight \n",
    "for the [additive model](#Additive-model \"Introduction: Additive model\").\n",
    "Hence, the expected log-likelihood takes the form\n",
    "\\begin{eqnarray}\n",
    "Q(\\Theta,\\Theta') & = & \n",
    "\\sum_{d=1}^N\\sum_{k=1}^K \\bar{w}_k^{(d)}\\,\n",
    "\\ln\\left[\\phi_k\\,P(c^{(d)}\\mid\\mathbf{x}^{(d)}_k,\\Psi_k)\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4c430",
   "metadata": {},
   "source": [
    "### Maximising the expected log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47464d8d",
   "metadata": {},
   "source": [
    "Before we maximise the \n",
    "[expected log-likelihood](#Expected-log-likelihood \"Expectation-Maximisation Approach: Expected log-likelihood\"),\n",
    "we [recall](#Additive-model \"Introduction: Additive model\") \n",
    "that we may define the parameters $\\Phi$ in terms of the mixture weights $\\boldsymbol{\\phi}$.\n",
    "Furthermore, since these weights sum to unity, this constraint may be included via the use of a Lagrange multiplier. Hence, the appropriate objective function is\n",
    "\\begin{eqnarray}\n",
    "F(\\Phi;\\Phi',\\Psi) & \\doteq & Q(\\Theta,\\Theta')-\\lambda (\\mathbf{1}^T\\boldsymbol{\\phi}-1)\\,.\n",
    "\\end{eqnarray}\n",
    "We now choose the weights to maximise this objective function.\n",
    "\n",
    "The required gradient with respect to the $k$-th component is given by\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial \\phi_k} & = &\n",
    "\\sum_{d=1}^{N}\\frac{\\bar{w}_k^{(d)}}{\\phi_k}-\\lambda\\,,\n",
    "\\end{eqnarray}\n",
    "which vanishes (i.e. becomes zero) exactly for the estimate\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\phi}_k & = & \\frac{1}{\\lambda}\\sum_{d=1}^{N}\\bar{w}_k^{(d)}\\,.\n",
    "\\end{eqnarray}\n",
    "Observe that summing both sides over $k$ results in the identity $\\lambda=N$,\n",
    "since $\\hat{\\phi}_k\\doteq P(k\\mid\\hat{\\Phi})$ and\n",
    "$\\bar{w}_k^{(d)}\\doteq P(k\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989b87c",
   "metadata": {},
   "source": [
    "From the [additive model](#Additive-model \"Introduction: Additive model\"), the update equation\n",
    "for the estimate of the mixture weights $\\boldsymbol{\\phi}$ is therefore\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\phi}_k & = & \n",
    "\\frac{1}{N}\\sum_{d=1}^{N}P\\left(k\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta'\\right) \n",
    "~=~\n",
    "\\frac{1}{N}\\sum_{d=1}^{N}\n",
    "\\frac{\\phi_k'\\,P\\left(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)}\n",
    "{\\sum_{\\tilde{k}=1}^K \\phi_\\tilde{k}'\\,P\\left(c^{(d)}\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Psi_k\\right)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "The EM algorithm now proceeds by iteratively updating the \n",
    "previous parameter estimate $\\Theta'$ with the new estimate $\\hat{\\Theta}$, i.e.\n",
    "$\\Theta'\\leftarrow\\hat{\\Theta}$, or in this case $\\Phi'\\leftarrow\\hat{\\Phi}$.\n",
    "This iteration continues until the parameter estimates converge (subject to mild conditions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c1e95",
   "metadata": {},
   "source": [
    "## Direct Optimsation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51330ca0",
   "metadata": {},
   "source": [
    "### Marginal model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfb847",
   "metadata": {},
   "source": [
    "Recall that the [EM approach](#Expectation-Maximisation-Approach \"Section: Expectation-Maximisation Approach\") explicitly models the hidden information and then takes expectations over its conditional distribution.\n",
    "In contrast, in the direct approach we simply marginalise over the missing information, \n",
    "i.e. the optimal index $k^*$ for sequence $\\vec{\\mathbf{x}}$,\n",
    "and consider only the known data, namely the training labels $\\mathbf{C}$ and sequences $\\mathbf{X}$.\n",
    "\n",
    "The [marginal model](#Additive-model \"Introduction: Additive model\") is therefore given by\n",
    "\\begin{eqnarray}\n",
    "P(c\\mid\\vec{\\mathbf{x}},\\Theta) & = & \\sum_{k=1}^{K}P(k,c\\mid\\vec{\\mathbf{x}},\\Theta)\n",
    "~\\doteq~ \\sum_{k=1}^K \\phi_k\\,P(c\\mid\\mathbf{x}_k,\\Psi_k)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with mixture weights $\\phi_k\\ge 0$ that satisfy $\\sum_{k=1}^{K}\\phi_k=1$.\n",
    "Note that in the direct approach there is no need to interpret these weights as component probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6acf8b",
   "metadata": {},
   "source": [
    "### Discriminative log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de095c24",
   "metadata": {},
   "source": [
    "The discriminative log-likelihood of the training data $\\mathbf{C}$ and $\\mathbf{X}$ is now given by\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta) & \\doteq & \\ln P(\\mathbf{C}\\mid\\mathbf{X},\\Theta)\n",
    "\\nonumber\\\\& = &\n",
    "\\sum_{d=1}^{N}\\ln P\\left(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\nonumber\\\\& = &\n",
    "\\sum_{d=1}^{N}\\ln \\sum_{k=1}^{K}\\phi_k\\,P\\left(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that other forms of likelihood could also be used, such as the joint likelihood\n",
    "$P(\\mathbf{C},\\mathbf{X}\\mid\\Theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cde4eb",
   "metadata": {},
   "source": [
    "### Maximising the discriminative log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c643e",
   "metadata": {},
   "source": [
    "Given the log-likelihood and the constraints on the mixture weights, we obtain the objective function\n",
    "\\begin{eqnarray}\n",
    "F(\\Phi;\\Psi) & \\doteq & L(\\Theta)-\\lambda (\\mathbf{1}^T\\boldsymbol{\\phi}-1)\\,,\n",
    "\\end{eqnarray}\n",
    "which is to be maximised. The gradient with respect to the $k$-th mixture component is then\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial \\phi_k} & = & \n",
    "\\sum_{d=1}^{N}\\frac{P\\left(c^{(d)}\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)}\n",
    "{\\sum_{\\tilde{k}=1}^K \\phi_\\tilde{k}\\,P\\left(c^{(d)}\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Psi_\\tilde{k}\\right)}\n",
    "-\\lambda\n",
    "\\nonumber\\\\\n",
    "& = & \\frac{1}{\\phi_k}\\sum_{d=1}^{N}\n",
    "P\\left(k\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)-\\lambda\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which vanishes exactly at the estimate\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\phi}_k & = & \\frac{1}{N}\\sum_{d=1}^{N}\n",
    "P\\left(k\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\hat{\\Theta}\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "for $\\lambda=N$.\n",
    "\n",
    "Observe that this is a nonlinear optimisation, due to the presence of $\\hat{\\Theta}$ on the right-hand side.\n",
    "Hence, we could use a gradient-ascent approach to maximise $F$, which would likely be the fastest approach.\n",
    "Alternatively, we could take an iterative approach, and repeatedly compute\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\phi}_k & = & \\frac{1}{N}\\sum_{d=1}^{N}\n",
    "P\\left(k\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta'\\right)\n",
    "~\\doteq~\\frac{1}{N}\\sum_{d=1}^{N}\\bar{w}_k^{(d)}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "in conjunction with the update $\\Theta'\\leftarrow\\hat{\\Theta}$, i.e. $\\Phi'\\leftarrow\\hat{\\Phi}$.\n",
    "Observe that this iterative approach is exactly the \n",
    "[EM solution](#Maximising-the-expected-log-likelihood \n",
    "\"Expectation-Maximisation Approach: Maximising the expected log-likelihood\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde7ffd",
   "metadata": {},
   "source": [
    "## Unsupervised Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d03f0",
   "metadata": {},
   "source": [
    "In the previous sections, we assumed that the training labels $\\mathbf{C}$ were known for all training cases\n",
    "$\\mathbf{X}$, and hence used *supervised* learning approaches. In contrast, we now assume instead that **no** class labels are known for any training case. Hence, we must use an *unsupervised* learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d814a",
   "metadata": {},
   "source": [
    "### Unsupervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81feb9b0",
   "metadata": {},
   "source": [
    "As in the [previous](#Direct-Optimisation \"Section: Direct Optimisation\") section,\n",
    "we assume a mixture model of the form\n",
    "\\begin{eqnarray}\n",
    "P(c\\mid\\vec{\\mathbf{x}},\\Theta) & \\doteq & \\sum_{k=1}^K \\phi_k\\,P(c\\mid\\mathbf{x}_k,\\Psi_k)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with mixture weights $w_k\\ge 0$ that satisfy $\\sum_{k=1}^{K}w_k=1$.\n",
    "\n",
    "However, the true class label $c^{(d)}$ is no longer assumed known for the $d$-th sequence $\\vec{\\mathbf{x}}^{(d)}$. \n",
    "Hence, we [borrow](#Expected-log-likelihood \"Expectation-Maximisation Approach: Expected log-likelihood\")\n",
    "the idea of using binary indicators to represent our ignorance.\n",
    "In particular, we introduce the notional class indicator variable $z_c^{(d)}\\doteq\\delta(c^{(d)}=c)$.\n",
    "Note that this is **not** the mixture component indicator $z^{(d)}_k$ used previously.\n",
    "Thus, we (re)define $\\mathbf{z}^{(d)}\\doteq (z_c^{(d)})_{k=1}^K$ and\n",
    "$\\mathbf{Z}\\doteq\\left[\\mathbf{z}^{(d)}\\right]_{d=1}^N$.\n",
    "\n",
    "As a consequence, the unsupervised model now takes the form\n",
    "\\begin{eqnarray}\n",
    "P(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta) & \\doteq & \n",
    "\\prod_{c=1}^{C}\\left[P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\right]^{\\,z_c^{(d)}}\n",
    "\\\\\n",
    "\\Rightarrow \\ln P(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta) & \\doteq &\n",
    "\\sum_{c=1}^{C}z_c^{(d)}\\ln\\sum_{k=1}^K \\phi_k\\,P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d24e7",
   "metadata": {},
   "source": [
    "### Expected class log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69025e44",
   "metadata": {},
   "source": [
    "As [before](#Discriminative-log-likelihood \"Direct optimisation: Discriminative log-likelihood\"),\n",
    "the discriminative log-likelihood, given supervised training data $\\mathbf{C}$ and $\\mathbf{X}$, is taken to be\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta) & \\doteq & \\ln P(\\mathbf{C}\\mid\\mathbf{X},\\Theta)\n",
    "~ = ~ \\sum_{d=1}^{N}\\ln P\\left(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "However, since we no longer know $\\mathbf{C}$ (nor $\\mathbf{Z}$), then we explicitly represent this uncertainty\n",
    "via the log-likelihood\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta;\\mathbf{Z}) & = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}z_c^{(d)}\\ln\\sum_{k=1}^{K}\\phi_k\\,P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522aa163",
   "metadata": {},
   "source": [
    "Once [again](#Expected-log-likelihood \"Expectation-Maximisation Approach: Expected log-likelihood\"),\n",
    "we eliminate the hidden information by taking expectations over $\\mathbf{Z}$, this time given the known data\n",
    "$\\mathbf{X}$, and assumed parameter values $\\Theta'$.\n",
    "We observe that\n",
    "\\begin{eqnarray}\n",
    "q_c^{(d)} & \\doteq & \\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\Theta'}\\left[z_c^{(d)}\\right] ~=~\n",
    "P(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta')\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "It therefore follows that the expected log-likelihood is given by\n",
    "\\begin{eqnarray}\n",
    "Q(\\Theta,\\Theta') & \\doteq & \\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\Theta'}[L(\\Theta;\\mathbf{Z})] ~=~\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}q_c^{(d)}\\ln\\sum_{k=1}^{K}\\phi_k\\,P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c166d",
   "metadata": {},
   "source": [
    "### Maximising the unsupervised log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a5b89",
   "metadata": {},
   "source": [
    "As [usual](#Maximising-the-expected-log-likelihood \n",
    "\"Expectation-Maximisation Approach: Maximising the expected log-likelihood\"), we\n",
    "seek the mixture weights $\\boldsymbol{\\phi}$ that maximise the objective function\n",
    "\\begin{eqnarray}\n",
    "F(\\Phi;\\Phi',\\Psi) & \\doteq & Q(\\Theta,\\Theta')-\\lambda (\\mathbf{1}^T\\boldsymbol{\\phi}-1)\\,.\n",
    "\\end{eqnarray}\n",
    "The gradient with respect to the $k$-th component is given by\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial \\phi_k} & = & \n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}q_c^{(d)}\\,\n",
    "\\frac{P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)}\n",
    "{\\sum_{\\tilde{k}=1}^{K}\\phi_\\tilde{k}\\,P\\left(c\\mid\\mathbf{x}_\\tilde{k}^{(d)},\\Psi_\\tilde{k}\\right)}\n",
    "-\\lambda\n",
    "\\nonumber\\\\\n",
    "& = & \\frac{1}{\\phi_k}\\sum_{d=1}^{N}\\sum_{c=1}^{C}q_c^{(d)}\\,\n",
    "P(k\\mid c,\\vec{\\mathbf{x}}^{(d)},\\Theta)-\\lambda\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which vanishes exactly at the estimate\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\phi}_k & = & \\frac{1}{N}\\sum_{d=1}^{N}\\sum_{c=1}^{C}q_c^{(d)}\\,\n",
    "P\\left(k\\mid c,\\vec{\\mathbf{x}}^{(d)},\\hat{\\Theta}\\right)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "with $\\lambda=N$.\n",
    "\n",
    "Observe that this is a nonlinear optimisation. Even worse, the standard EM approach would be to iteratively\n",
    "update $\\hat{\\Theta}$ (until convergence) keeping $\\Theta'$ fixed, and only then update $\\Theta'\\leftarrow\\hat{\\Theta}$. It is tempting to short-cut this procedure by utilising only a single\n",
    "iteration. However, this immediately causes problems if we replace $\\hat{\\Theta}$ on the right-hand side\n",
    "by $\\Theta'$, since then\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\phi}_k & = & \\frac{1}{N}\\sum_{d=1}^{N}\\sum_{c=1}^{C} \n",
    "P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta'\\right)\\,\n",
    "P\\left(k\\mid c,\\vec{\\mathbf{x}}^{(d)},\\Theta'\\right)\n",
    "\\nonumber\\\\\n",
    "& = & \n",
    "\\frac{1}{N}\\sum_{d=1}^{N}\\sum_{c=1}^{C}P\\left(k,c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta'\\right)\n",
    "\\nonumber\\\\\n",
    "& = &\n",
    "\\frac{1}{N}\\sum_{d=1}^{N}\\sum_{c=1}^{C} \\phi_k'\\,\n",
    "P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\n",
    "~=~\\phi_k'\n",
    "\\,.\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71bc66b",
   "metadata": {},
   "source": [
    "### Direct unsupervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc110ebc",
   "metadata": {},
   "source": [
    "As an alternative to the usual EM iteration of $\\Theta'$ and $\\hat{\\Theta}$,\n",
    "we might instead directly take the expectation over $\\mathbf{Z}$ with respect to the true (but unknown) parameters\n",
    "$\\Theta$. This gives rise to the expected log-likelihood\n",
    "\\begin{eqnarray}\n",
    "Q(\\Theta) & \\doteq & \\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\Theta}\\left[L(\\Theta;\\mathbf{Z})\\right]\n",
    "\\nonumber\\\\ & = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C} P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\,\n",
    "\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\nonumber\\\\ & = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}\\left\\{\n",
    "\\sum_{k=1}^{K}\\phi_k\\,P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\n",
    "\\right\\}\n",
    "\\ln\\sum_{k=1}^{K}\\phi_k\\,P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45792a3e",
   "metadata": {},
   "source": [
    "Taking the objective function to be\n",
    "\\begin{eqnarray}\n",
    "F(\\Phi;\\Psi) & \\doteq & Q(\\Theta)-\\lambda (\\mathbf{1}^T\\boldsymbol{\\phi}-1)\\,,\n",
    "\\end{eqnarray}\n",
    "the gradient with respect to the $k$-th component is therefore\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial\\phi_k} & = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}\\left\\{\n",
    "P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\\,\n",
    "\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "+P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\n",
    "\\right\\}-\\lambda\n",
    "\\nonumber\\\\\n",
    "& = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}\n",
    "P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\\,\n",
    "\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "+N-\\lambda\n",
    "\\nonumber\\\\\n",
    "& = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}\n",
    "P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\\,\n",
    "\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for $\\lambda=N$. Hence, we may use gradient ascent to obtain the optimal parameter estimate, $\\hat{\\Theta}$.\n",
    "Also note that for $\\lambda=N$ we have\n",
    "\\begin{eqnarray}\n",
    "\\sum_{k=1}^K \\phi_k\\,\\frac{\\partial F}{\\partial\\phi_k} & = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}\\left\\{\n",
    "\\sum_{k=1}^K \\phi_k\\,\n",
    "P\\left(c\\mid\\mathbf{x}_k^{(d)},\\Psi_k\\right)\\right\\}\\,\n",
    "\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\nonumber\\\\& = &\n",
    "\\sum_{d=1}^{N}\\sum_{c=1}^{C}P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\,\n",
    "\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "~=~Q(\\Theta)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749db1a6",
   "metadata": {},
   "source": [
    "## Quasi-Supervised Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a934863",
   "metadata": {},
   "source": [
    "### Supervised, unsupervised and semi-supervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0480b9d",
   "metadata": {},
   "source": [
    "In *supervised* learning, the aim is to retrospectively predict the outcome of a single event with known\n",
    "result. Hence, the class label $c^{(d)}$ of the $d$-th training sequence $\\vec{\\mathbf{x}^{(d)}}$ is always known. An appropriate measure is therefore the \n",
    "[discriminative log-likelihood](#Discriminative-log-likelihood \n",
    "\"Direct Optimisation: Discriminative log-likelihood\"), namely \n",
    "$\\ln P\\left(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)$.\n",
    "Alternatively, we may specify the class $c^{(d)}$ via the \n",
    "[binary indicator](#Unsupervised-model \"Unsupervised Training: Unsupervised model\")\n",
    "vector $\\mathbf{z}^{(d)}\\doteq\\left(z_c^{(d)}\\right)_{c=1}^{C}$,\n",
    "where $z_c^{(d)}\\doteq\\delta(c^{(d)}=c)$. The log-likelihood therefore becomes\n",
    "\\begin{eqnarray}\n",
    "L^{(d)}(\\Theta) & \\doteq &\n",
    "\\ln P\\left(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right) ~=~ \n",
    "\\sum_{c=1}^{C}z_c^{(d)}\\,\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "Note that $\\mathbf{z}^{(d)}$ is also known as a 1-of-$C$ vector (in the statistics literature), or a *one-hot* vector (in the engineering literature).\n",
    "\n",
    "Conversely, in *unsupervised* learning, the aim is to predict the outcome of a single event with unknown outcome. Hence, the class label $c^{(d)}$ is never known. Since the indicator $z_c^{(d)}$ is\n",
    "also unknown, it is replaced by its \n",
    "[expectation](#Expected-class-log-likelihood \"Unsupervised Training: Expected class log-likelihood\"), namely\n",
    "\\begin{eqnarray}\n",
    "q_c^{(d)} & \\doteq & \\mathbb{E}_{\\mathbf{Z}\\mid\\mathbf{X},\\Theta}\\left[z_c^{(d)}\\right] ~=~\n",
    "P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that we are evaluating these class probabilities at $\\Theta$ instead of $\\Theta'$.\n",
    "Hence, the appropriate measure is now\n",
    "\\begin{eqnarray}\n",
    "L^{(d)}(\\Theta) & \\doteq &\n",
    "\\mathbb{E}_{\\mathbf{X},\\Theta}\\left[\n",
    "\\ln P\\left(c^{(d)}\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\right]\n",
    "~=~\n",
    "\\sum_{c=1}^{C}q_c^{(d)}\\,\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\,.\n",
    "\\end{eqnarray}\n",
    "\n",
    "In *semi-supervised* learning, some but not all of the class labels $\\mathbf{C}$ are known, and some are unknown. Note that when $c^{(d)}$ is known, we may define\n",
    "\\begin{eqnarray}\n",
    "P\\left(c\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta\\right) & \\doteq & \n",
    "P\\left(c\\mid c^{(d)}\\right)\n",
    "~=~\\delta\\left(c=c^{(d)}\\right)\n",
    "~\\doteq~z_c^{(d)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Conversely, when $c^{(d)}$ is missing, we observe that\n",
    "\\begin{eqnarray}\n",
    "P\\left(c\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta\\right) & \\doteq & \n",
    "P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)~\\doteq~q_c^{(d)}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, we may combine the supervised and unsupervised apparoaches into the common framework\n",
    "\\begin{eqnarray}\n",
    "L^{(d)}(\\Theta) & \\doteq & \n",
    "\\sum_{c=1}^{C}P\\left(c\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\n",
    "\\,\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "P\\left(c\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Theta\\right) & \\doteq &\n",
    "\\left\\{\n",
    "\\begin{array}{lr}\n",
    "\\delta\\left(c=c^{(d)}\\right) & \\mbox{if $c^{(d)}$ is known}\n",
    "\\\\\n",
    "P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right) & \\mbox{if $c^{(d)}$ is unknown}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a677d",
   "metadata": {},
   "source": [
    "### Quasi-supervised log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84806b",
   "metadata": {},
   "source": [
    "[Recall](#Supervised,-unsupervised-and-semi-supervised-training\n",
    "\"Quasi-Supervised Training: Supervised, unsupervised and semi-supervised training\")\n",
    "that for supervised learning we know the class label, and for unsupervised learning we do not.\n",
    "Here we consider an in-between case, for which I have coined the term *quasi-supervised* learning.\n",
    "Rather than having either complete certainty or complete ignorance of the class label,\n",
    "instead we know only the expected proportions of each class.\n",
    "\n",
    "As an example, suppose that instead of modelling a single event, we model a collection of events.\n",
    "Thus, we might amalgamate the class labels of the collected events by computing the proportion of events in each class.\n",
    "Further generalising the [semi-supervised](#Supervised,-unsupervised-and-semi-supervised-training\n",
    "\"Quasi-Supervised Training: Supervised, unsupervised and semi-supervised training\")\n",
    "log-likelihood, the appropriate measure for quasi-supervised learning is therefore the negative cross-entropy\n",
    "\\begin{eqnarray}\n",
    "L^{(d)}(\\Theta) & \\doteq & \n",
    "\\sum_{c=1}^{C}P\\left(c\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Gamma\\right)\n",
    "\\,\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\,,\n",
    "\\end{eqnarray}\n",
    "where the use of $\\Gamma$ indicates a different family of models than our \n",
    "[additive model](#Additive-model \"Introduction: Additive model\") \n",
    "using $\\Theta$. Here we take $\\Gamma$ to be fixed, such that each class proportion\n",
    "\\begin{eqnarray}\n",
    "\\gamma_c^{(d)} & \\doteq & P\\left(c\\mid c^{(d)},\\vec{\\mathbf{x}}^{(d)},\\Gamma\\right)\n",
    "~\\doteq~P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Gamma\\right)\n",
    "\\end{eqnarray}\n",
    "is also constant and known. For convenience, we define\n",
    "$\\boldsymbol{\\gamma}^{(d)}\\doteq\\left(\\gamma_c^{(d)}\\right)_{c=1}^{C}$\n",
    "and $\\boldsymbol{\\Gamma}\\doteq\\left[\\boldsymbol{\\gamma}^{(d)}\\right]_{d=1}^{N}$, such that\n",
    "$\\boldsymbol{\\Gamma}$ now replaces $\\mathbf{C}$ as part of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab860733",
   "metadata": {},
   "source": [
    "Consequently, the overall log-likelihood is now given by\n",
    "\\begin{eqnarray}\n",
    "L(\\Theta) & \\doteq & \\sum_{d=1}^N\\sum_{c=1}^C \\gamma_c^{(d)}\\,\n",
    "\\ln P\\left(c\\mid\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ad5e08",
   "metadata": {},
   "source": [
    "### Maximising the quasi-supervised log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0b69a",
   "metadata": {},
   "source": [
    "The objective function is taken to be\n",
    "\\begin{eqnarray}\n",
    "F(\\Phi;\\Psi) & \\doteq & L(\\Theta)-\\lambda (\\mathbf{1}^T\\boldsymbol{\\phi}-1)\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, the gradient with respect to the $k$-th component is therefore\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial\\phi_k} & = &\n",
    "\\frac{1}{\\phi_k}\\sum_{d=1}^N\\sum_{c=1}^C \\gamma_c^{(d)}\\,\n",
    "P\\left(k\\mid c,\\vec{\\mathbf{x}}^{(d)},\\Theta\\right)-\\lambda\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "which vanishes exactly when\n",
    "\\begin{eqnarray}\n",
    "\\hat{\\phi}_k & = & \\frac{1}{N}\n",
    "\\sum_{d=1}^N\\sum_{c=1}^C \\gamma_c^{(d)}\\,\n",
    "P\\left(k\\mid c,\\vec{\\mathbf{x}}^{(d)},\\hat{\\Theta}\\right)\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "for $\\lambda=N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37e511",
   "metadata": {},
   "source": [
    "This nonlinear equation may be solved via iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
