{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c600f50",
   "metadata": {},
   "source": [
    "# Discrete Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6413d",
   "metadata": {},
   "source": [
    "### Discrete exponential distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5167a14",
   "metadata": {},
   "source": [
    "Consider a $D$-dimensional space ${\\cal V}$ of values. We notionally consider that each point ${\\bf v}=(v_1,v_2,\\ldots,v_D)\\in{\\cal V}$ is an instance of stochastic variables\n",
    "$(V_1,V_2,\\ldots,V_D)$.\n",
    "A Boltzmann machine (BM) now takes the form of a completely connected, undirected graph, with a distinct node for each variable, as shown in the figure below.\n",
    "\n",
    "![General Boltzmann Machine](BM_general.png \"General Boltzmann Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccb208",
   "metadata": {},
   "source": [
    "For convenience, we now restrict ourselves to the case where ${\\cal V}$ is a space of discrete values. Note that for continuous variables, (some of) the corresponding summations below would be replaced by integrations, although the resulting derivations will be of similar form.\n",
    "\n",
    "A discrete Boltzmann machine now has an energy function \n",
    "$E:{\\cal V}\\rightarrow\\mathbb{R}$ that induces the probability distribution \n",
    "\\begin{eqnarray}\n",
    "p({\\bf v}) & = & \n",
    "\\frac{e^{-E({\\bf v})}}\n",
    "{\\sum_{{\\bf v'}\\in{\\cal V}}e^{-E({\\bf v'})}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In practice, it is more convenient to let $E=-f$ for some complementary function\n",
    "$f:{\\cal V}\\rightarrow\\mathbb{R}$, which forms the exponential family\n",
    "\\begin{eqnarray}\n",
    "p({\\bf v}) & = & \n",
    "\\frac{e^{f({\\bf v})}}\n",
    "{\\sum_{{\\bf v'}\\in{\\cal V}}e^{f({\\bf v'})}}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d0260",
   "metadata": {},
   "source": [
    "### Parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b21e4c",
   "metadata": {},
   "source": [
    "We now suppose that $f({\\bf v})$ is implicitly parameterised by some collection of parameters, denoted by $\\Theta$. Our task is therefore to estimate $\\Theta$ from a data-set of known training points. An obvious choice is to jointly maximise the likelihood\n",
    "$p({\\bf v})$ of each training point ${\\bf v}$, under the assumption that training cases are independent. This is equivalent to maximising\n",
    "the log-likelihood, given by\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf v}) & = & \n",
    "f({\\bf v})-\\ln\\sum_{{\\bf v'}\\in{\\cal V}}e^{f({\\bf v'})}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489737ff",
   "metadata": {},
   "source": [
    "The usual choice for maximisation is gradient ascent, in one of its many variants.\n",
    "Hence, for some arbitrary parameter $\\theta$, the gradient of the log-likelihood is\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf v}) & = & \n",
    "\\nabla f({\\bf v})-\\nabla\\ln\\sum_{{\\bf v'}\\in{\\cal V}}e^{f({\\bf v})}\n",
    "\\\\& = &\n",
    "\\nabla f({\\bf v})-\n",
    "\\frac{\n",
    " \\sum_{{\\bf v'}\\in{\\cal V}}e^{f({\\bf v'})}\\,\\nabla f({\\bf v'})\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf v'}\\in{\\cal V}}e^{f({\\bf v'})}\n",
    "}\n",
    "\\\\& = &\n",
    "\\nabla f({\\bf v})-\\sum_{{\\bf v'}\\in{\\cal V}} p({\\bf v'})\\,\\nabla f({\\bf v'})\n",
    "\\\\& = &\n",
    "\\nabla f({\\bf v})-\\mathbb{E}_{\\cal V}\\left[\\nabla f({\\bf v'})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db06a58",
   "metadata": {},
   "source": [
    "The biggest problem in practice with Boltzmann machines is that $p({\\bf v})$ is\n",
    "intractable to compute in general, due largely to the *curse of dimensionality*.\n",
    "Hence, the unconditional expectation $\\mathbb{E}_{\\cal V}[\\cdot]$ is also intractable.\n",
    "In the following sections, we discuss approximation techniques for handling this\n",
    "intractability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91eb59",
   "metadata": {},
   "source": [
    "### Partitioned Boltzmann machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644ea3b",
   "metadata": {},
   "source": [
    "The usual rationale for constructing a Boltzmann machine, or indeed for assuming any probability distribution, is for the purpose of prediction. \n",
    "Let us therefore suppose that the point ${\\bf v}\\in{\\cal V}$ may be partitioned into\n",
    "two sub-points, ${\\bf x}\\in{\\cal X}$ and ${\\bf y}\\in{\\cal Y}$. We also suppose that\n",
    "${\\bf x}$ and ${\\bf y}$ may be *stitched* back together to obtain\n",
    "${\\bf v}=\\breve{\\bf v}({\\bf x},{\\bf y})$. This *partitioned* Boltzmann machine is shown in the figure below.\n",
    "\n",
    "![Partitioned Boltzmann Machine](BM_partitioned.png \"Partitioned Boltzmann Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35352b26",
   "metadata": {},
   "source": [
    "For convenience, let us define \n",
    "$\\breve{f}({\\bf x},{\\bf y})\\doteq f(\\breve{\\bf v}({\\bf x},{\\bf y})) = f({\\bf v})$.\n",
    "Then the joint distribution becomes\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \n",
    "\\frac{e^{\\breve{f}({\\bf x},{\\bf y})}}\n",
    "{\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In addition,\n",
    "due to the non-directionality of edges, we may also predict in either direction, namely\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}) & = & \n",
    "\\frac{e^{\\breve{f}({\\bf x},{\\bf y})}}\n",
    "{\\sum_{{\\bf y'}\\in{\\cal Y}}e^{\\breve{f}({\\bf x},{\\bf y'})}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "or\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}\\mid{\\bf y}) & = & \n",
    "\\frac{e^{\\breve{f}({\\bf x},{\\bf y})}}\n",
    "{\\sum_{{\\bf x'}\\in{\\cal X}}e^{\\breve{f}({\\bf x'},{\\bf y})}}\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "The corresponding marginal distributions are\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}) & = & \n",
    "\\frac{\\sum_{{\\bf x'}\\in{\\cal X}}e^{\\breve{f}({\\bf x'},{\\bf y})}}\n",
    "{\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf x'}\\in{\\cal X}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & \n",
    "\\frac{\\sum_{{\\bf y}\\in{\\cal Y}}e^{\\breve{f}({\\bf x},{\\bf y})}}\n",
    "{\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}}\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad23b3",
   "metadata": {},
   "source": [
    "For convenience, we from now on suppose that the partitioned BM is treated as a predictive model with \n",
    "*input* ${\\bf x}$ and *output* ${\\bf y}$, although it will always be able to operate in\n",
    "reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23617fca",
   "metadata": {},
   "source": [
    "Before we proceed to the examination of parameter estimation for these various models, we first digress to the additional technique that we will utilise for handling the intractability of BMs, dicussed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56e6b3",
   "metadata": {},
   "source": [
    "### Mean field approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6235f11",
   "metadata": {},
   "source": [
    "The mean field approximation states that the mean value of a function averaged over a number of points is approximately equal to the function evaluated at the average of those points. To demonstrate this, first let \n",
    "$\\bar{\\bf v}=\\mathbb{E}_{\\cal V}[{\\bf v}]$ be the average of all the points in \n",
    "${\\cal V}$. Next, consider the first-order Taylor series approximation of some\n",
    "$g({\\bf v})$ about $\\bar{\\bf v}$, namely\n",
    "\\begin{eqnarray}\n",
    "g({\\bf v}) & \\approx & g(\\bar{\\bf v})\n",
    "+\\left({\\bf v}-\\bar{\\bf v}\\right)^T \\nabla g(\\bar{\\bf v})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, taking the expectation over ${\\cal V}$, it follows that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{\\cal V}[g({\\bf v})] & \\approx & \n",
    "g(\\bar{\\bf v})\n",
    "+\\left(\\mathbb{E}_{\\cal V}[{\\bf v}]-\\bar{\\bf v}\\right)^T \\nabla g(\\bar{\\bf v})\n",
    "\\\\& = &\n",
    "g(\\bar{\\bf v}) = g(\\mathbb{E}_{\\cal V}[{\\bf v}])\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "This is the mean field approximation (MFA). \n",
    "\n",
    "If we proceed further to the second term in the Taylor series expansion (not shown here), then it becomes apparent that the accuracy of the approximation depends on both the smoothness of the function (especially its second derivative) and the variance of the points in ${\\cal V}$.\n",
    "However, my experience is that MFA works very well in practice, especially for computing BM gradients that are otherwise intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575c52e",
   "metadata": {},
   "source": [
    "### Joint likelihood optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d3a95",
   "metadata": {},
   "source": [
    "We suppose that the training data-set specifies both ${\\bf x}$ and ${\\bf y}$.\n",
    "Thus, we utilise the joint model $p({\\bf x},{\\bf y})$, defined in an earlier section.\n",
    "Observe that\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x},{\\bf y}) & = & \n",
    "\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\ln\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & = & \n",
    "\\nabla \\breve{f}({\\bf x},{\\bf y})\n",
    "-\\frac{\n",
    " \\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})\\,\\nabla \\breve{f}({\\bf x'},{\\bf y'})}\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}\n",
    "}\n",
    "\\\\& = &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " p({\\bf x'},{\\bf y'})\\,\\nabla\\breve{f}({\\bf x'},{\\bf y'})\n",
    "\\\\& = &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X,Y}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "For convenience, we now let ${\\cal X}'\\equiv{\\cal X}$ (and similarly for ${\\cal Y}'$),\n",
    "where the prime distinguishes expectation over ${\\bf x'}\\in{\\cal X}'$ from\n",
    "expectation over ${\\bf x}\\in{\\cal X}$.\n",
    "Thus, we write\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & = & \n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X',Y'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\\\& = &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X'}\\left[\n",
    " \\mathbb{E}_{\\cal Y'\\mid X'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where we have made use of the BM partitioning.\n",
    "\n",
    "Note that by $\\mathbb{E}_{\\cal Y'\\mid X'}$ I really mean\n",
    "$\\mathbb{E}_{\\cal Y'\\mid {\\bf x'}}$, and thus\n",
    "$\\mathbb{E}_{\\cal X'}[\\mathbb{E}_{\\cal Y'\\mid X'}]$ really means\n",
    "$\\mathbb{E}_{{\\bf x'}\\in\\cal X'}[\\mathbb{E}_{\\cal Y'\\mid {\\bf x'}}]$.\n",
    "However, I didn't feel like mixing sets and points. Alternatively, I could have just\n",
    "written $\\mathbb{E}_{\\cal {\\bf y'}\\mid {\\bf x'}}$, as I have done in other notebooks, although this loses explicit mention of the domain. In my experience, all expectation notation suffers from exposing some explicit dependencies whilst hiding other implicit dependencies, and is thus never entirely unambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ecc88",
   "metadata": {},
   "source": [
    "Now, computing $p({\\bf x'})$ is still intractable in general. However, we do know \n",
    "${\\bf x}$ and ${\\bf y}$. Hence, we make further use of the partitioning by taking the\n",
    "approximation\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & \\approx & \n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X'\\mid Y}\\left[\n",
    " \\mathbb{E}_{\\cal Y'\\mid X'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In other words, we keep alternating between use of the predictive models\n",
    "$p({\\bf x}\\mid{\\bf y})$ and $p({\\bf y}\\mid{\\bf x})$ until we reach known values\n",
    "of the conditional.\n",
    "We shall call this the *conditional expectation approximation* (CEA), although another appropriate name would be\n",
    "*conditional expectation alternation* - take your pick.\n",
    "\n",
    "Note that we assume that these predictive models are tractable to compute!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d07f2d",
   "metadata": {},
   "source": [
    "Next, we define the expectation functions\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\bf x}({\\bf y}) \\doteq \\mathbb{E}_{\\cal X\\mid Y}[{\\bf x}]\\,,&\\,\\,&\n",
    "\\bar{\\bf y}({\\bf x}) \\doteq \\mathbb{E}_{\\cal Y\\mid X}[{\\bf y}]\\,.\n",
    "\\end{eqnarray}\n",
    "These convenience functions allow us to more easily apply MFA, giving\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & \\approx & \n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X'\\mid Y}\\left[\n",
    " \\mathbb{E}_{\\cal Y'\\mid X'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X'\\mid Y}\\left[\n",
    " \\nabla\\breve{f}({\\bf x'},\\bar{\\bf y}({\\bf x'}))\n",
    "\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\nabla\\breve{f}(\\bar{\\bf x}({\\bf y}),\\bar{\\bf y}(\\bar{\\bf x}({\\bf y})))\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1830fd",
   "metadata": {},
   "source": [
    "Alternatively, we may rewrite the gradient as\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & = & \n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal Y'}\\left[\n",
    " \\mathbb{E}_{\\cal X'\\mid Y'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal Y'\\mid X}\\left[\n",
    " \\mathbb{E}_{\\cal X'\\mid Y'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal Y'\\mid X}\\left[\n",
    " \\nabla\\breve{f}(\\bar{\\bf x}({\\bf y'}),{\\bf y'})\n",
    "\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\nabla\\breve{f}(\\bar{\\bf x}(\\bar{\\bf y}({\\bf x})),\\bar{\\bf y}({\\bf x}))\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "It is not clear which alternative is preferable. However, if we notionally think of ${\\bf x}$ as the input, then this perhaps suggests the latter approxmation might be favoured. I have not implemented the former variant, but the latter variant appears to work well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c294e1",
   "metadata": {},
   "source": [
    "In either case, we still cannot tractably compute the joint training score, \n",
    "$\\ln p({\\bf x},{\\bf y})$. However, one possible approach is to observe that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & p({\\bf y}\\mid{\\bf x})\\,p({\\bf x})\n",
    "\\\\& = &\n",
    "p({\\bf y}\\mid{\\bf x})\\,\\sum_{{\\bf y'}\\in{\\cal Y}'} p({\\bf y'})\\,p({\\bf x}\\mid{\\bf y'})\n",
    "\\\\& = &\n",
    "p({\\bf y}\\mid{\\bf x})\\,\\mathbb{E}_{{\\cal Y}'}\\left[p({\\bf x}\\mid{\\bf y'})\\right]\n",
    "\\\\\n",
    "\\Rightarrow p({\\bf x},{\\bf y}) & \\approx &\n",
    "p({\\bf y}\\mid{\\bf x})\\,\\mathbb{E}_{{\\cal Y}'\\mid{\\cal X}}\\left[p({\\bf x}\\mid{\\bf y'})\\right]\n",
    "\\\\& \\approx &\n",
    "p({\\bf y}\\mid{\\bf x})\\,p({\\bf x}\\mid\\bar{\\bf y}({\\bf x}))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "via CEA and MFA. Once again, this has been implemented and tested, and works well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab096d1",
   "metadata": {},
   "source": [
    "### Marginal likelihood optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03faa5a3",
   "metadata": {},
   "source": [
    "We now suppose that the training data-set only specifies ${\\bf x}$ but not ${\\bf y}$.\n",
    "Thus, we might utilise the marginal likelihood $p({\\bf x})$, defined in an earlier section.\n",
    "Observe that\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x}) & = & \n",
    "\\ln\\sum_{{\\bf y}\\in{\\cal Y}}e^{\\breve{f}({\\bf x},{\\bf y})}\n",
    "-\\ln\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla\\ln p({\\bf x}) & = &\n",
    "\\frac{\n",
    " \\sum_{{\\bf y}\\in{\\cal Y}}e^{\\breve{f}({\\bf x},{\\bf y})}\n",
    " \\,\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf y'}\\in{\\cal Y}}e^{\\breve{f}({\\bf x},{\\bf y'})}\n",
    "}\n",
    "-\\frac{\n",
    " \\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}\n",
    "  \\,\\nabla\\breve{f}({\\bf x'},{\\bf y'})\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'})}\n",
    "}\n",
    "\\\\& = &\n",
    "\\sum_{{\\bf y}\\in{\\cal Y}} p({\\bf y}\\mid{\\bf x})\\,\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    "p({\\bf x'},{\\bf y'})\\,\\nabla\\breve{f}({\\bf x'},{\\bf y'})\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{\\cal X', Y'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4253d4b",
   "metadata": {},
   "source": [
    "We observe that a general pattern has emerged here, namely that summation over a collection of variables in the log-likelihood corresponds, in the gradient, to the expectation over those same variables conditional on the remaining variables.\n",
    "Thus, $\\sum_{\\cal Y}$ in the first term on the right-hand side\n",
    "becomes $\\mathbb{E}_{\\cal Y\\mid X}$, and $\\sum_{\\cal X',Y'}$ in the second term\n",
    "becomes $\\mathbb{E}_{\\cal X',Y'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de80bc0",
   "metadata": {},
   "source": [
    "Now, applying CEA gives\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x}) & \\approx &\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{\\cal Y\\mid X}\\left[\n",
    "  \\mathbb{E}_{\\cal X'\\mid Y}\\left[\n",
    "   \\mathbb{E}_{\\cal Y'\\mid X'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "  \\right]\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4a44a",
   "metadata": {},
   "source": [
    "Finally, applying MFA gives\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x}) & \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},\\bar{\\bf y})\n",
    "-\\nabla\\breve{f}(\\bar{\\bf x}',\\bar{\\bf y}')\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\bar{\\bf y}=\\bar{\\bf y}({\\bf x})$, \n",
    "$\\bar{\\bf x}'=\\bar{\\bf x}(\\bar{\\bf y})=\\bar{\\bf x}(\\bar{\\bf y}({\\bf x}))$,\n",
    "and $\\bar{\\bf y}'=\\bar{\\bf y}(\\bar{\\bf x}')=\n",
    "\\bar{\\bf y}(\\bar{\\bf x}(\\bar{\\bf y}({\\bf x})))$.\n",
    "Note that the ordering of these (shorthand) computations corresponds to computing the expectations from left (outside) to right (inside). This is another general pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa621b",
   "metadata": {},
   "source": [
    "This version of the gradient has been tested with a Bernoulli Restricted BM, and works well. The alternative version, namely\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x}) & \\approx &\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{\\cal Y'\\mid X}\\left[\n",
    "   \\mathbb{E}_{\\cal X'\\mid Y'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    " \\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},\\bar{\\bf y}({\\bf x}))\n",
    "-\\nabla\\breve{f}(\\bar{\\bf x}(\\bar{\\bf y}({\\bf x})),\\bar{\\bf y}({\\bf x}))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "has not been tested. However, the presence of the same term $\\bar{\\bf y}({\\bf x})$ on both sides of the difference suggests that the reconstruction of ${\\bf y'}$ would be poor!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af478b2",
   "metadata": {},
   "source": [
    "Lastly, we observe that we cannot tractably compute \n",
    "the marginal score, $\\ln p({\\bf x})$, of training case ${\\bf x}$, for the\n",
    "same reason that we cannot in general compute the unconditional probability \n",
    "$p({\\bf x})$. However, we recall from above that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & \\sum_{{\\bf y}\\in{\\cal Y}} p({\\bf x},{\\bf y})\n",
    "\\\\& = & \\sum_{{\\bf y}\\in{\\cal Y}} p({\\bf x}\\mid{\\bf y})\\,p({\\bf y})\n",
    "\\\\& = & \\mathbb{E}_{\\cal Y}\\left[p({\\bf x}\\mid{\\bf y})\\right]\n",
    "\\\\& \\approx & \\mathbb{E}_{\\cal Y\\mid X}\\left[p({\\bf x}\\mid{\\bf y})\\right]\n",
    "\\\\& \\approx & p({\\bf x}\\mid\\bar{\\bf y}({\\bf x}))\\,,\n",
    "\\end{eqnarray}\n",
    "via CEA then MFA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ae428",
   "metadata": {},
   "source": [
    "### Conditional likelihood optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adffc37",
   "metadata": {},
   "source": [
    "Lastly, we look at the case where we wish to directly optimise the predictive model $p({\\bf y}\\mid{\\bf x})$ instead of the\n",
    "joint likelihood $p({\\bf x},{\\bf y})$.\n",
    "Assuming we know both ${\\bf x}$ and ${\\bf y}$, then\n",
    "(from our earlier derivation) we have\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf y}\\mid{\\bf x}) & = &\n",
    "\\breve{f}({\\bf x},{\\bf y})-\\ln\\sum_{{\\bf y'}\\in{\\cal Y}}e^{\\breve{f}({\\bf x},{\\bf y'})}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla\\ln p({\\bf y}\\mid{\\bf x}) & = &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal Y'\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y'})\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y}) - \\nabla\\breve{f}({\\bf x},\\bar{\\bf y}({\\bf x}))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "via MFA. The log-likelihood score, $\\ln p({\\bf y}\\mid{\\bf x})$, can now be computed\n",
    "directly from the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c7530",
   "metadata": {},
   "source": [
    "I should add, as a note of caution obtained from testing various other discriminative models, that directly optimising the conditional predictive model can often exacerbate the effect of over-training, although this\n",
    "depends strongly on the training data.\n",
    "\n",
    "Also note that there\n",
    "is no explicit modelling of the distribution of ${\\bf x}$, and hence any related model\n",
    "parameters required for computing $p({\\bf x}\\mid{\\bf y})$ would need to be estimated in some other fashion. We could, for example, alternate between the gradient updates\n",
    "$\\nabla\\ln p({\\bf y}\\mid{\\bf x})$ and $\\nabla\\ln p({\\bf x}\\mid{\\bf y})$, where\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x}\\mid{\\bf y}) & = &\n",
    "\\breve{f}({\\bf x},{\\bf y})-\\ln\\sum_{{\\bf x'}\\in{\\cal X}}e^{\\breve{f}({\\bf x'},{\\bf y})}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla\\ln p({\\bf x}\\mid{\\bf y}) & = &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X'\\mid Y}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y})\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y}) - \\nabla\\breve{f}(\\bar{\\bf x}({\\bf y}),{\\bf y})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "I have not tested this latter gradient scheme for BMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdf6a5",
   "metadata": {},
   "source": [
    "### Expected likelihood optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86dbdc",
   "metadata": {},
   "source": [
    "What happens if we want to use discriminative training, but do not know ${\\bf y}$?\n",
    "\n",
    "We recall from a previous section that the traditional approach for unsupervised learning, when ${\\bf y}$ is always unknown, is to maximise the marginal likelihood, $p({\\bf x})$. \n",
    "Conversely, for supervised learning, when ${\\bf y}$ is always known, we may\n",
    "optimise either $p({\\bf x},{\\bf y})$ or $p({\\bf y}\\mid{\\bf x})$, as we see fit\n",
    "(again, refer to the previous sections above).\n",
    "Now, for semi-supervised learning, where ${\\bf y}$ is known for some cases but unknown for others, it is traditional to use either $p({\\bf x},{\\bf y})$ or $p({\\bf y}\\mid{\\bf x})$ for the cases where ${\\bf y}$ is known, but instead to use $p({\\bf x})$ for the cases where ${\\bf y}$ is unknown.\n",
    "\n",
    "What is wrong with this traditional approach? The answer is that, for discrete distributions,\n",
    "we have $p({\\bf x})\\approx\\frac{1}{|{\\cal X}|}$, $p({\\bf y}\\mid{\\bf x})\\approx\\frac{1}{|{\\cal Y}|}$,\n",
    "and $p({\\bf x},{\\bf y})\\approx\\frac{1}{|{\\cal X}|\\,|{\\cal Y}|}$,\n",
    "in terms of approximate magnitudes.\n",
    "Thus, for cases where we have computed $p({\\bf x})$ in place of $p({\\bf x},{\\bf y})$, we have overestimated\n",
    "the joint likelihood by a factor of $|{\\cal Y}|$. Likewise, for cases where we have computed $p({\\bf x})$ in place of $p({\\bf y}\\mid{\\bf x})$, we have overestimated\n",
    "the discriminative likelihood by a factor of $\\frac{|{\\cal X}|}{|{\\cal Y}|}$.\n",
    "In practice, this means that traditional semi-supervised learning gives more (possibly much more) weight to unknown cases than to known cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b1e91",
   "metadata": {},
   "source": [
    "The solution is to either correct the magnitude of the likelihood approximation, or\n",
    "else to use expected likelihoods (or, rather, expected log-likelihoods), for unsupervised and \n",
    "especially semi-supervised learning. Thus, if we wish to optimise the joint log-likelihood, \n",
    "$\\ln p({\\bf x},{\\bf y})$, when ${\\bf y}$ is unknown, then instead of the traditional approximation\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x},{\\bf y}) & \\approx & \n",
    "\\ln\\sum_{{\\bf y'}\\in{\\cal Y}'}p({\\bf x},{\\bf y'})\n",
    "=\\ln p({\\bf x})\\,,\n",
    "\\end{eqnarray}\n",
    "we could use the corrected version\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x},{\\bf y}) & \\approx & \n",
    "\\frac{1}{|{\\cal Y}|}\n",
    "\\sum_{{\\bf y'}\\in{\\cal Y}'}\\ln p({\\bf x},{\\bf y'})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "This has the correct magnitude, but essentially assumes that each ${\\bf y}\\in{\\cal Y}$ is of equal importance.\n",
    "\n",
    "Alternatively,  we could instead use the expected value\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x},{\\bf y}) & \\approx & \n",
    "\\sum_{{\\bf y'}\\in{\\cal Y}'}p({\\bf y'}\\mid{\\bf x})\\,\\ln p({\\bf x},{\\bf y'})\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{{\\cal Y}'\\mid{\\cal X}}\\left[\\ln p({\\bf x},{\\bf y'})\\right]\n",
    "\\doteq L_{J}({\\bf x})\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "on the supposition that some values of ${\\bf y}\\in{\\cal Y}$ are conditonally more likely than\n",
    "others.\n",
    "Interestingly, this means that $\\ln p({\\bf x},{\\bf y}) \\approx \\ln p({\\bf x},\\bar{\\bf y}({\\bf x}))$,\n",
    "via MFA, even though both terms remain intractable to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e45f0c",
   "metadata": {},
   "source": [
    "The equivalent approximation to the discriminative log-likelihood is thus\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf y}\\mid{\\bf x}) & \\approx & \n",
    "\\sum_{{\\bf y'}\\in{\\cal Y}'}p({\\bf y'}\\mid{\\bf x})\\,\\ln p({\\bf y'}\\mid{\\bf x})\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{{\\cal Y}'\\mid{\\cal X}}\\left[\\ln p({\\bf y'}\\mid{\\bf x})\\right]\n",
    "\\doteq L_{D}({\\bf x})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "In this case, since $p({\\bf y}\\mid{\\bf x})$ is assumed to be tractable to compute \n",
    "(for small enough $|{\\cal Y}|$), then the expectation is also tractable.\n",
    "Also, via MFA, we have $\\ln p({\\bf y}\\mid{\\bf x})\\approx \\ln p(\\bar{\\bf y}({\\bf x})\\mid{\\bf x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a6a8d",
   "metadata": {},
   "source": [
    "To help with the gradient calculations, observe that, in general,\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\mathbb{E}_{\\cal V}\\left[g({\\bf v})\\right]\n",
    "& = & \\nabla\\sum_{{\\bf v}\\in{\\cal V}}p({\\bf v})\\,g({\\bf v})\n",
    "\\\\& = &\n",
    "\\sum_{{\\bf v}\\in{\\cal V}}\\left\\{\n",
    "p({\\bf v})\\,\\nabla g({\\bf v})+\\nabla p({\\bf v})\\,g({\\bf v})\n",
    "\\right\\}\n",
    "\\\\& = &\n",
    "\\sum_{{\\bf v}\\in{\\cal V}}\\left\\{\n",
    "p({\\bf v})\\,\\nabla g({\\bf v})+\n",
    "g({\\bf v})\\,p({\\bf v})\\,\\nabla\\ln p({\\bf v})\n",
    "\\right\\}\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{\\cal V}\\left[\\nabla g({\\bf v})\\right]\n",
    "+\\mathbb{E}_{\\cal V}\\left[g({\\bf v})\\,\\nabla\\ln p({\\bf v})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Hence, for the expected discriminative log-likelihood we have\n",
    "\\begin{eqnarray}\n",
    "\\nabla L_{D}({\\bf x}) & = & \n",
    "\\nabla\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\n",
    " \\ln p({\\bf y}\\mid{\\bf x})\n",
    "\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\\nabla \\ln p({\\bf y}\\mid{\\bf x})\\right]\n",
    "+\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\n",
    " \\ln p({\\bf y}\\mid{\\bf x})\\,\\nabla \\ln p({\\bf y}\\mid{\\bf x})\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Now, from the previous section we have\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf y}\\mid{\\bf x}) & = &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal Y'\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y'})\\right]\n",
    "\\\\\n",
    "\\Rightarrow \n",
    "\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\\nabla \\ln p({\\bf y}\\mid{\\bf x})\\right]\n",
    "& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{\\cal Y'\\mid\\cal X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y'})\\right]\n",
    "\\equiv 0\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this means we cannot simply take the expectation of the gradient, but must go further\n",
    "and take the gradient of the expectation, giving\n",
    "\\begin{eqnarray}\n",
    "\\nabla L_{D}({\\bf x}) & = &\n",
    "\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\n",
    " \\ln p({\\bf y}\\mid{\\bf x})\\,\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "\\right]\n",
    "-\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\n",
    " \\ln p({\\bf y}\\mid{\\bf x})\n",
    "\\right]\\,\n",
    "\\mathbb{E}_{\\cal Y'\\mid\\cal X}\\left[\n",
    " \\nabla\\breve{f}({\\bf x},{\\bf y'})\n",
    "\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\n",
    " \\ln p({\\bf y}\\mid{\\bf x})\\,\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "\\right]\n",
    "- L_{D}({\\bf x})\\,\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\n",
    " \\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "However, note that MFA cannot help us much here, since it results in this last difference being approximated by zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645fba7",
   "metadata": {},
   "source": [
    "Now turning back to the expected joint log-likelihood, we have\n",
    "\\begin{eqnarray}\n",
    "\\nabla L_{J}({\\bf x}) & = & \\nabla\\mathbb{E}_{{\\cal Y}\\mid{\\cal X}}\\left[\\ln p({\\bf x},{\\bf y})\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\\nabla \\ln p({\\bf x},{\\bf y})\\right]\n",
    "+\\mathbb{E}_{\\cal Y\\mid\\cal X}\\left[\n",
    " \\ln p({\\bf x},{\\bf y})\\,\\nabla \\ln p({\\bf y}\\mid{\\bf x})\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Now, from the previous section on joint likelihood optimisation we have\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & \\approx & \n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y})\n",
    "-\\mathbb{E}_{\\cal X'\\mid Y}\\left[\n",
    " \\mathbb{E}_{\\cal Y'\\mid X'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\nabla\\ln p({\\bf x},{\\bf y})\\right]\n",
    "& \\approx & \n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y})\\right]\n",
    "-\\mathbb{E}_{\\cal Y\\mid X}\\left[\n",
    " \\mathbb{E}_{\\cal X'\\mid Y}\\left[\n",
    " \\mathbb{E}_{\\cal Y'\\mid X'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'})\\right]\n",
    "\\right]\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Note that this is exactly the approximation for $\\nabla\\ln p({\\bf x})$, from the previous section\n",
    "on marginal likelihood optimisation! Hence, we could, if we wished, stop with the expectation of the gradient,\n",
    "namely\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\nabla\\ln p({\\bf x},{\\bf y})\\right]\n",
    "& \\approx & \n",
    "\\nabla\\ln p({\\bf x})\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "However, the approxpriate log-likelihood score is not $\\ln p({\\bf x})$, \n",
    "for the reasons outlined at the start of this section, but instead\n",
    "\\begin{eqnarray}\n",
    "L_{J}({\\bf x}) & = & \\mathbb{E}_{\\cal Y\\mid X}\\left[\\ln p({\\bf x},{\\bf y})\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\ln p({\\bf x})+\\ln p({\\bf y}\\mid{\\bf x})\\right]\n",
    "\\\\& = &\n",
    "\\ln p({\\bf x})+L_{D}({\\bf x})\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, we also see that gradient of the expectation of the joint log-likelihood is\n",
    "\\begin{eqnarray}\n",
    "\\nabla L_{J}({\\bf x}) & = & \\nabla\\ln p({\\bf x}) + \\nabla L_{D}({\\bf x})\\,. \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac302416",
   "metadata": {},
   "source": [
    "### Hidden models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1e981",
   "metadata": {},
   "source": [
    "In the above derivations, we partitioned ${\\bf v}$ into ${\\bf x}$ and ${\\bf y}$, where we assumed for training that ${\\bf x}$ is always known, and ${\\bf y}$ might or might not be known.\n",
    "\n",
    "We now turn to the related case where ${\\bf v}\\in{\\cal V}$ is partitioned into\n",
    "${\\bf x}\\in{\\cal X}$, ${\\bf y}\\in{\\cal Y}$ and ${\\bf z}\\in{\\cal Z}$, where ${\\bf z}$\n",
    "is never observed, i.e. it is latent or hidden.\n",
    "For convenience, we redefine \n",
    "$f({\\bf v})=f(\\breve{\\bf v}({\\bf x},{\\bf y},{\\bf z}))\n",
    "\\doteq\\breve{f}({\\bf x},{\\bf y},{\\bf z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6e873",
   "metadata": {},
   "source": [
    "The relvant unconditional distributions are now given by\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y},{\\bf z}) & = & \n",
    "\\frac{e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}}\n",
    "{\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})}}\n",
    "\\,,\n",
    "\\\\\n",
    "p({\\bf x},{\\bf y}) & = & \n",
    "\\frac{\\sum_{{\\bf z}\\in{\\cal Z}}e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}}\n",
    "{\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})}}\n",
    "\\,,\n",
    "\\\\\n",
    "p({\\bf x}) & = & \n",
    "\\frac{\\sum_{{\\bf y}\\in{\\cal Y}}\\sum_{{\\bf z}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}}\n",
    "{\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})}}\n",
    "%\\,,\n",
    "%\\\\\n",
    "%p({\\bf y}) & = & \n",
    "%\\frac{\\sum_{{\\bf x}\\in{\\cal X}}\\sum_{{\\bf z}\\in{\\cal Z}}\n",
    "% e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}}\n",
    "%{\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    "% e^{\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})}}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521773b",
   "metadata": {},
   "source": [
    "Similarly, the relevant conditional distributions are\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}\\mid{\\bf z}) & = & \n",
    "\\frac{\n",
    " e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'},{\\bf z)}}\n",
    "}\n",
    "\\,,\n",
    "\\\\\n",
    "p({\\bf z}\\mid{\\bf x},{\\bf y}) & = & \n",
    "\\frac{\n",
    " e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z'})}\n",
    "}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbef22",
   "metadata": {},
   "source": [
    "The forward and backward predictive distributions are\n",
    "\\begin{eqnarray}\n",
    "p({\\bf y}\\mid{\\bf x}) & = & \n",
    "\\frac{\n",
    " \\sum_{{\\bf z}\\in{\\cal Z}}e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x},{\\bf y'},{\\bf z')}}\n",
    "}\n",
    "\\,,\n",
    "\\\\\n",
    "p({\\bf x}\\mid{\\bf y}) & = & \n",
    "\\frac{\n",
    " \\sum_{{\\bf z}\\in{\\cal Z}}e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}\n",
    "}\n",
    "{\n",
    " \\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y},{\\bf z'})}\n",
    "}\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2433e2b",
   "metadata": {},
   "source": [
    "### Joint likelihood optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bae4c6",
   "metadata": {},
   "source": [
    "From the above definition of $p({\\bf x},{\\bf y})$, we have\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x},{\\bf y}) & = & \n",
    "\\ln\\sum_{{\\bf z}\\in{\\cal Z}}e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}\n",
    "-\\ln\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & = &\n",
    "\\mathbb{E}_{\\cal Z\\mid X,Y}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y},{\\bf z})\\right]\n",
    "-\n",
    "\\mathbb{E}_{\\cal X',Y',Z'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6d8d5",
   "metadata": {},
   "source": [
    "Examination of the various gradient approximations derived in earlier sections suggests\n",
    "yet another pattern, namely that the inner conditional expectation on the right-hand side of the difference should match the conditional expectation on the left-hand side (with added primes).\n",
    "Thus, we choose the CEA expansion\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & = &\n",
    "\\mathbb{E}_{\\cal Z\\mid X,Y}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y},{\\bf z})\\right]\n",
    "-\n",
    "\\mathbb{E}_{\\cal Z}\\left[\n",
    " \\mathbb{E}_{\\cal X',Y'\\mid Z}\\left[\n",
    "  \\mathbb{E}_{\\cal Z'\\mid X',Y'}\\left[\n",
    "   \\nabla\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})\n",
    "  \\right]\n",
    " \\right]\n",
    "\\right]\n",
    "\\\\& \\approx &\n",
    "\\mathbb{E}_{\\cal Z\\mid X,Y}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y},{\\bf z})\\right]\n",
    "-\n",
    "\\mathbb{E}_{\\cal Z\\mid X,Y}\\left[\n",
    " \\mathbb{E}_{\\cal X',Y'\\mid Z}\\left[\n",
    "  \\mathbb{E}_{\\cal Z'\\mid X',Y'}\\left[\n",
    "   \\nabla\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})\n",
    "  \\right]\n",
    " \\right]\n",
    "\\right]\n",
    "\\,.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5b13c",
   "metadata": {},
   "source": [
    "We lack sufficient information about the specific model to be able to approximate the\n",
    "middle expectation $\\mathbb{E}_{\\cal X',Y'\\mid Z}$ via MFA. However, for the inner and outer expectations, it is clear that we need to define\n",
    "\\begin{eqnarray}\n",
    "\\bar{\\bf z}({\\bf x},{\\bf y}) & \\doteq & \n",
    "\\mathbb{E}_{\\cal Z\\mid X,Y}[{\\bf z}]\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Consequently, we at least know that\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x},{\\bf y}) & \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y},\\bar{\\bf z})\n",
    "-\\nabla\\breve{f}(\\bar{\\bf x}',\\bar{\\bf y}',\\bar{\\bf z}')\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\bar{\\bf z}=\\bar{\\bf z}({\\bf x},{\\bf y})$ and \n",
    "$\\bar{\\bf z}'=\\bar{\\bf z}({\\bf x}',{\\bf y}')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8563d",
   "metadata": {},
   "source": [
    "In order to compute the log-likelihood score, observe that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x},{\\bf y}) & = & \\sum_{{\\bf z}\\in{\\cal Z}} p({\\bf x},{\\bf y},{\\bf z})\n",
    "\\\\\n",
    "& = & \\sum_{{\\bf z}\\in{\\cal Z}} p({\\bf x},{\\bf y}\\mid{\\bf z})\\,p({\\bf z})\n",
    "\\\\\n",
    "& = & \\mathbb{E}_{\\cal Z}[p({\\bf x},{\\bf y}\\mid{\\bf z})]\n",
    "\\\\\n",
    "& \\approx & \\mathbb{E}_{\\cal Z\\mid X, Y}[p({\\bf x},{\\bf y}\\mid{\\bf z})]\n",
    "\\\\\n",
    "& \\approx & p({\\bf x},{\\bf y}\\mid\\bar{z}({\\bf x},{\\bf y}))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "via CEA and MFA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda1fde",
   "metadata": {},
   "source": [
    "### Marginal likelihood optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4f23d",
   "metadata": {},
   "source": [
    "From the definition of $p({\\bf x})$ in a previous section, we have\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf x}) & = & \n",
    "\\ln\\sum_{{\\bf y}\\in{\\cal Y}}\\sum_{{\\bf z}\\in{\\cal Z}}\n",
    "e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}\n",
    "-\\ln\\sum_{{\\bf x'}\\in{\\cal X}}\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla\\ln p({\\bf x}) & = &\n",
    "\\mathbb{E}_{\\cal Y,Z\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y},{\\bf z})\\right]\n",
    "-\n",
    "\\mathbb{E}_{\\cal X',Y',Z'}\\left[\\nabla\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})\\right]\n",
    "\\\\& \\approx &\n",
    "\\mathbb{E}_{\\cal Y,Z\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y},{\\bf z})\\right]\n",
    "-\\mathbb{E}_{\\cal Y,Z\\mid X}\\left[\n",
    "  \\mathbb{E}_{\\cal X'\\mid Y,Z}\\left[\n",
    "   \\mathbb{E}_{\\cal Y',Z'\\mid X'}\\left[\n",
    "    \\nabla\\breve{f}({\\bf x'},{\\bf y'},{\\bf z'})\n",
    "   \\right]\n",
    "  \\right]\n",
    "\\right]\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "via CEA. We further note that\n",
    "\\begin{eqnarray}\n",
    "\\mathbb{E}_{\\cal Y,Z\\mid X}[\\cdot]\n",
    "& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\\mathbb{E}_{\\cal Z\\mid X,Y}[\\cdot]\\right]\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\mathbb{E}_{\\cal Y,Z\\mid X}[{\\bf y}] & = & \n",
    "\\mathbb{E}_{\\cal Y\\mid X}[{\\bf y}] = \\bar{\\bf y}({\\bf x})\\,,\n",
    "\\\\\n",
    "\\mathbb{E}_{\\cal Y,Z\\mid X}[{\\bf z}] & = & \n",
    "\\mathbb{E}_{\\cal Y\\mid X}[\\bar{\\bf z}({\\bf x},{\\bf y})]\n",
    "\\approx\n",
    "\\bar{\\bf z}({\\bf x},\\bar{\\bf y}({\\bf x}))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "via MFA.\n",
    "Consequently, we have\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf x}) & \\approx & \n",
    "\\nabla\\breve{f}({\\bf x},\\bar{\\bf y},\\bar{\\bf z})\n",
    "-\n",
    "\\nabla\\breve{f}(\\bar{\\bf x}',\\bar{\\bf y}',\\bar{\\bf z}')\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "where $\\bar{\\bf y}=\\bar{\\bf y}({\\bf x})$,\n",
    "$\\bar{\\bf z}=\\bar{\\bf z}({\\bf x},\\bar{\\bf y})$,\n",
    "$\\bar{\\bf y}'=\\bar{\\bf y}(\\bar{\\bf x}')$,\n",
    "and\n",
    "$\\bar{\\bf z}'=\\bar{\\bf z}(\\bar{\\bf x}',\\bar{\\bf y}')$.\n",
    "The form that $\\bar{\\bf x}'$ takes depends upon $\\mathbb{E}_{\\cal X'\\mid Y,Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d83068",
   "metadata": {},
   "source": [
    "In order to compute the log-likelihood score, observe that\n",
    "\\begin{eqnarray}\n",
    "p({\\bf x}) & = & \n",
    "\\sum_{{\\bf y}\\in{\\cal Y}}\\sum_{{\\bf z}\\in{\\cal Z}} p({\\bf x},{\\bf y},{\\bf z})\n",
    "\\\\\n",
    "& = & \n",
    "\\sum_{{\\bf y}\\in{\\cal Y}}\\sum_{{\\bf z}\\in{\\cal Z}} \n",
    "p({\\bf x}\\mid{\\bf y},{\\bf z})\\,p({\\bf y},{\\bf z})\n",
    "\\\\\n",
    "& = & \\mathbb{E}_{\\cal Y,Z}[p({\\bf x}\\mid{\\bf y},{\\bf z})]\n",
    "\\\\\n",
    "& \\approx & \\mathbb{E}_{\\cal Y,Z\\mid X}[p({\\bf x}\\mid{\\bf y},{\\bf z})]\n",
    "\\\\\n",
    "& = &\n",
    "\\mathbb{E}_{\\cal Y\\mid X}\\left[\n",
    " \\mathbb{E}_{\\cal Z\\mid Y,X}[p({\\bf x}\\mid{\\bf y},{\\bf z})]\n",
    "\\right]\n",
    "\\\\\n",
    "& \\approx & p({\\bf x}\\mid\\bar{\\bf y}({\\bf x}),\\bar{z}({\\bf x},\\bar{\\bf y}({\\bf x})))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "via CEA and MFA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e0d29",
   "metadata": {},
   "source": [
    "### Conditional likelihood optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47132e97",
   "metadata": {},
   "source": [
    "From the definition of $p({\\bf y}\\mid{\\bf x})$ in an earlier section, we have\n",
    "\\begin{eqnarray}\n",
    "\\ln p({\\bf y}\\mid{\\bf x}) & = & \n",
    "\\ln\\sum_{{\\bf z}\\in{\\cal Z}}\n",
    "e^{\\breve{f}({\\bf x},{\\bf y},{\\bf z})}\n",
    "-\\ln\\sum_{{\\bf y'}\\in{\\cal Y}}\\sum_{{\\bf z'}\\in{\\cal Z}}\n",
    " e^{\\breve{f}({\\bf x},{\\bf y'},{\\bf z'})}\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\nabla\\ln p({\\bf y}\\mid{\\bf x}) & = &\n",
    "\\mathbb{E}_{\\cal Z\\mid X,Y}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y},{\\bf z})\\right]\n",
    "-\n",
    "\\mathbb{E}_{\\cal Y',Z'\\mid X}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y'},{\\bf z'})\\right]\n",
    "\\\\& = &\n",
    "\\mathbb{E}_{\\cal Z\\mid X,Y}\\left[\\nabla\\breve{f}({\\bf x},{\\bf y},{\\bf z})\\right]\n",
    "-\\mathbb{E}_{\\cal Y'\\mid X}\\left[\n",
    "   \\mathbb{E}_{\\cal Z'\\mid X,Y'}\\left[\n",
    "    \\nabla\\breve{f}({\\bf x},{\\bf y'},{\\bf z'})\n",
    "   \\right]\n",
    "\\right]\n",
    "\\\\& \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y},\\bar{\\bf z}({\\bf x},{\\bf y}))\n",
    "-\n",
    "\\nabla\\breve{f}({\\bf x},\\bar{\\bf y}({\\bf x}),\\bar{\\bf z}({\\bf x},\\bar{\\bf y}({\\bf x})))\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "via MFA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea8aa4",
   "metadata": {},
   "source": [
    "We can therefore write this as\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({\\bf y}\\mid{\\bf x}) & \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y},\\bar{\\bf z})\n",
    "-\\nabla\\breve{f}({\\bf x},\\bar{\\bf y}',\\bar{\\bf z}')\n",
    "\\,,\n",
    "\\end{eqnarray}\n",
    "in contrast to the above gradient of the joint log-likelihood, namely\n",
    "\\begin{eqnarray}\n",
    "\\nabla\\ln p({{\\bf x}, \\bf y}) & \\approx &\n",
    "\\nabla\\breve{f}({\\bf x},{\\bf y},\\bar{\\bf z})\n",
    "-\\nabla\\breve{f}(\\bar{\\bf x}',\\bar{\\bf y}',\\bar{\\bf z}')\n",
    "\\,.\n",
    "\\end{eqnarray}\n",
    "Thus, when directly optimising the conditional (or discriminative) log-likelhood,\n",
    "we do not reconstruct the input ${\\bf x}$ via $\\bar{\\bf x}'$. In practice, this means\n",
    "that there are some parameters (i.e. those linked entirely to ${\\bf x}$) that cannot be directly estimated (but might be indirectly estimated via some hybrid gradient scheme)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
